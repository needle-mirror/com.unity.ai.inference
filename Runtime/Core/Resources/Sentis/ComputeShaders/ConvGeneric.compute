// Note: CACHE_DEPTH is the cache for an inner product chunk
// Defaults when not specified:
//  OUT_OUTCHAN_USES_HIBITS
//  MAIN_CONFIG_NUMBER == 0
//
#pragma kernel Conv3D_Generic MainName=Conv3D_Generic CONV3D NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16
#pragma kernel Conv2D_Generic MainName=Conv2D_Generic CONV2D NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16
#pragma kernel Conv1D_Generic MainName=Conv1D_Generic CONV1D NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16

#pragma kernel Conv3D_1x1_Generic MainName=Conv3D_1x1_Generic CONV3D K1x1 NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16
#pragma kernel Conv2D_1x1_Generic MainName=Conv2D_1x1_Generic CONV2D K1x1 NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16
#pragma kernel Conv1D_1x1_Generic MainName=Conv1D_1x1_Generic CONV1D K1x1 NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16

#pragma kernel ConvTranspose3D_Generic MainName=ConvTranspose3D_Generic CONV_TRANSPOSE CONV3D NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16
#pragma kernel ConvTranspose2D_Generic MainName=ConvTranspose2D_Generic CONV_TRANSPOSE CONV2D NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16
#pragma kernel ConvTranspose1D_Generic MainName=ConvTranspose1D_Generic CONV_TRANSPOSE CONV1D NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16

#pragma kernel ConvTranspose3D_1x1_Generic MainName=ConvTranspose3D_1x1_Generic CONV_TRANSPOSE CONV3D K1x1 NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16
#pragma kernel ConvTranspose2D_1x1_Generic MainName=ConvTranspose2D_1x1_Generic CONV_TRANSPOSE CONV2D K1x1 NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16
#pragma kernel ConvTranspose1D_1x1_Generic MainName=ConvTranspose1D_1x1_Generic CONV_TRANSPOSE CONV1D K1x1 NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16

// For STFT:
//#pragma kernel Conv1D_OutTransposed_4eoc_8esp MainName=Conv1D_OutTransposed_4eoc_8esp CONV1D OUT_OUTCHAN_USES_LOBITS TRANSPOSE_OUTPUT NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16

#pragma kernel Conv1D_OutTransposed_4eoc_4esp MainName=Conv1D_OutTransposed_4eoc_4esp CONV1D OUT_OUTCHAN_USES_LOBITS TRANSPOSE_OUTPUT                                                     NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=4 CACHE_DEPTH=16
#pragma kernel Conv1DComplex_OutTransposed_4eoc_4esp MainName=Conv1DComplex_OutTransposed_4eoc_4esp DATA_TYPE_SPEC=DATA_TYPE_SPEC_COMPLEX CONV1D OUT_OUTCHAN_USES_LOBITS TRANSPOSE_OUTPUT NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=4 CACHE_DEPTH=16

// To pass the normalizing constant for IDFT:
#pragma kernel Conv1D_Scaled_OutTransposed_4eoc_4esp MainName=Conv1D_Scaled_OutTransposed_4eoc_4esp FINAL_SCALAR_MUL CONV1D OUT_OUTCHAN_USES_LOBITS TRANSPOSE_OUTPUT                                                      NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=4 CACHE_DEPTH=16
#pragma kernel Conv1D_Scaled_Complex_OutTransposed_4eoc_4esp MainName=Conv1D_Scaled_Complex_OutTransposed_4eoc_4esp FINAL_SCALAR_MUL DATA_TYPE_SPEC=DATA_TYPE_SPEC_COMPLEX CONV1D OUT_OUTCHAN_USES_LOBITS TRANSPOSE_OUTPUT NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=4 CACHE_DEPTH=16

#pragma kernel Conv1D_OutTransposedDouble_4eoc_4esp MainName=Conv1D_OutTransposedDouble_4eoc_4esp CONV1D OUT_OUTCHAN_USES_LOBITS TRANSPOSE_OUTPUT STFT_DOUBLE_OUTPUT                      NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=4 CACHE_DEPTH=16

//#define DATA_TYPE_SPEC DATA_TYPE_SPEC_COMPLEX
//#define DATA_TYPE_SPEC DATA_TYPE_SPEC_NATIVE
// Variant Conv1DComplex_OutTransposed_4eoc_4esp above uses DATA_TYPE_SPEC_COMPLEX
#include "Packages/com.unity.ai.inference/Runtime/Core/ShaderLibrary/DataTypeSpec.hlsl"

// STFT specifics:

#if defined(STFT_DOUBLE_OUTPUT)
    #if !(  (DATA_TYPE_SPEC==DATA_TYPE_SPEC_NATIVE) && defined(TRANSPOSE_OUTPUT) && defined(OUT_OUTCHAN_USES_LOBITS)  )
        #error "This STFT code path is only implemented for a real signal using TRANSPOSE_OUTPUT + OUT_OUTCHAN_USES_LOBITS"
    #endif
#endif


// Dummy all-variant defines to test performance impact of transpose, obviously output would be wrong.
// test with eg yolov8m_seg_onnx
// #define OUT_OUTCHAN_USES_LOBITS 
// #define TRANSPOSE_OUTPUT


#pragma multi_compile_local _ USEBIAS
#pragma multi_compile_local _ UNIT_STRIDES

// Grouped conv:
//#pragma multi_compile_local _ GROUP_OC_PER_GROUP_LT_ANYVARIANTS GROUP_OC_PER_GROUP_MINDIV_2 GROUP_OC_PER_GROUP_MINDIV_4 GROUP_OC_PER_GROUP_MINDIV_8 GROUP_OC_PER_GROUP_MINDIV_16 GROUP_OC_PER_GROUP_MINDIV_32 GROUP_OC_PER_GROUP_MINDIV_64
// for now we just need those 2 anyway:
#pragma multi_compile_local _ GROUP_OC_PER_GROUP_LT_ANYVARIANTS GROUP_OC_PER_GROUP_MINDIV_64
// Explanation:
//
// You need to enable the variants corresponding to each config having NB_OUTCHAN_PER_TG evenly dividing the number output channels per convolution group
// that the variant refers to.
// This is because only one matters vs each config:
// eg if NB_OUTCHAN_PER_TG == 64 for a particular compiled kernel, only GROUP_OC_PER_GROUP_MINDIV_64 is relevant.
// C# code should drive the keyword knowing NB_OUTCHAN_PER_TG == (ELEM_PER_THREAD_Y*NUMTHREADS_Y) of the particular kernel it selects.
//
// Also, if none of those keywords are enabled, then it is assumed there's no convolution group (# groups == 1) involved.

// #pragma skip_variants GROUP_OC_PER_GROUP_LT_ANYVARIANTS
// #pragma skip_variants GROUP_OC_PER_GROUP_MINDIV_2
// #pragma skip_variants GROUP_OC_PER_GROUP_MINDIV_4
// #pragma skip_variants GROUP_OC_PER_GROUP_MINDIV_8
// #pragma skip_variants GROUP_OC_PER_GROUP_MINDIV_16
// #pragma skip_variants GROUP_OC_PER_GROUP_MINDIV_32
// #pragma skip_variants GROUP_OC_PER_GROUP_MINDIV_64 // all our current variants use NUMTHREADS_Y=8 ELEM_PER_THREAD_Y=8, so NB_OUTCHAN_PER_TG == 64

#if defined(GROUP_OC_PER_GROUP_LT_ANYVARIANTS) || defined(GROUP_OC_PER_GROUP_MINDIV_2) || defined(GROUP_OC_PER_GROUP_MINDIV_4) || defined(GROUP_OC_PER_GROUP_MINDIV_8) || defined(GROUP_OC_PER_GROUP_MINDIV_16) || defined(GROUP_OC_PER_GROUP_MINDIV_32) || defined(GROUP_OC_PER_GROUP_MINDIV_64)

    #define GROUPS_ENABLED

    #if defined(GROUP_OC_PER_GROUP_LT_ANYVARIANTS)
        #define GROUP_NB_OC_CHAN_DIVIDER 1
    #elif defined(GROUP_OC_PER_GROUP_MINDIV_2)
        #define GROUP_NB_OC_CHAN_DIVIDER 2
    #elif defined(GROUP_OC_PER_GROUP_MINDIV_4)
        #define GROUP_NB_OC_CHAN_DIVIDER 4
    #elif defined(GROUP_OC_PER_GROUP_MINDIV_8)
        #define GROUP_NB_OC_CHAN_DIVIDER 8
    #elif defined(GROUP_OC_PER_GROUP_MINDIV_16)
        #define GROUP_NB_OC_CHAN_DIVIDER 16
    #elif defined(GROUP_OC_PER_GROUP_MINDIV_32)
        #define GROUP_NB_OC_CHAN_DIVIDER 32
    #elif defined(GROUP_OC_PER_GROUP_MINDIV_64)
        #define GROUP_NB_OC_CHAN_DIVIDER 64
    #endif

#endif


#define DISABLE_SHADER_WARNING

// Note: this is bogus (actual values will be taken from the pragma definitions above) but prevents the compiler IPC from crashing,
// ie this is to bypass a bug.
#if !defined(NUMTHREADS_X)
#define NUMTHREADS_X 8
#endif
#if !defined(NUMTHREADS_Y)
#define NUMTHREADS_Y 8
#endif
#if !defined(ELEM_PER_THREAD_Y)
#define ELEM_PER_THREAD_Y 1
#endif
#if !defined(ELEM_PER_THREAD_X)
#define ELEM_PER_THREAD_X 1
#endif
#if !defined(CACHE_DEPTH)
#define CACHE_DEPTH 8
#endif

// ---------------------------------------------------------------------------------------------------------------------------------------
// Data types, buffer and uniform config / declarations:
// ---------------------------------------------------------------------------------------------------------------------------------------

#if (defined(SHADER_API_XBOXONE) || defined(SHADER_API_GAMECORE_XBOXSERIES)) && (DATA_TYPE_SPEC==DATA_TYPE_SPEC_COMPLEX)
    // The reason why all platforms work is because there is no such thing as something like D3D11_RESOURCE_MISC_BUFFER_ALLOW_RAW_VIEWS
    // in modern graphics APIs (this is an old *permission* flag) and also the stride for StructuredBuffer is in the view not the resource,
    // which the engine presumably takes from shader introspection, but it seems this path is broken on XBOX.
    //
    // Also ideally we should remove the resource restriction (ComputeBufferType = Default/Structured) at CcmputeBuffer creation time.
    // This might not fix the problem though, as if the stride is incorrectly fetched from the old underlying resource creation data
    // and not the created view, bug would still be there. Requires proper engine side fix.
    // We still want to avoid building different buffer memory pools for different data types while also keeping the indexing simple
    // by using StructuredBuffer<DATA_TYPE> instead of a ByteAddressBuffer.
    #define USE_BYTEADDRESSBUFFER
#endif


#if !defined(USE_BYTEADDRESSBUFFER)
StructuredBuffer<DATA_TYPE> Xptr;
StructuredBuffer<DATA_TYPE> Kptr;
StructuredBuffer<DATA_TYPE> Bptr;
RWStructuredBuffer<DATA_TYPE> Optr;

    #define READ_Xptr(idx) Xptr[idx];
    #define READ_Kptr(idx) Kptr[idx];
    #define READ_Bptr(idx) Bptr[idx];
    #define WRITE_OUTPUT(idx, data) Optr[idx] = data;

#else // #if !defined(USE_BYTEADDRESSBUFFER)
ByteAddressBuffer Xptr;
ByteAddressBuffer Kptr;
ByteAddressBuffer Bptr;
RWByteAddressBuffer Optr;

#if (DATA_TYPE_SPEC==DATA_TYPE_SPEC_COMPLEX)
    #define READ_Xptr(idx) ((DATA_TYPE)(asfloat(Xptr.Load2((idx) << (2 + DATA_TYPE_DWORD_SIZELOG2)))));
    #define READ_Kptr(idx) ((DATA_TYPE)(asfloat(Kptr.Load2((idx) << (2 + DATA_TYPE_DWORD_SIZELOG2)))));
    #define READ_Bptr(idx) ((DATA_TYPE)(asfloat(Bptr.Load2((idx) << (2 + DATA_TYPE_DWORD_SIZELOG2)))));
    #define WRITE_OUTPUT(idx, data) Optr.Store2(((idx) << (2 + DATA_TYPE_DWORD_SIZELOG2)), asuint(float2((data).re, (data).im)));
#else
    #define READ_Xptr(idx) (asfloat(Xptr.Load((idx) << (2 + DATA_TYPE_DWORD_SIZELOG2))));
    #define READ_Kptr(idx) (asfloat(Kptr.Load((idx) << (2 + DATA_TYPE_DWORD_SIZELOG2))));
    #define READ_Bptr(idx) (asfloat(Bptr.Load((idx) << (2 + DATA_TYPE_DWORD_SIZELOG2))));
    #define WRITE_OUTPUT(idx, data) Optr.Store(((idx) << (2 + DATA_TYPE_DWORD_SIZELOG2)), asuint(data));
#endif

#endif // #if !defined(USE_BYTEADDRESSBUFFER)


uint O_batch;
uint O_channels, O_depth, O_height, O_width;
uint X_channels, X_depth, X_height, X_width;
uint K_depth, K_height, K_width;

uint X_channelsPerGroup; // if conv with group > 1, # of input channels per group
uint O_channelsPerGroup; // if conv with group > 1, # of output channels per group

uint4 _Pad;
uint4 _Stride;
uint4 _Dilation;
float _MinValue;

#if defined(FINAL_SCALAR_MUL)
float Scale;
#else
#define Scale (1.0)
#endif

#if defined(STFT_DOUBLE_OUTPUT)
uint NbUniqueDFTFreqTimes2;
#endif


//CBUFFER_START(TestBuffer)
cbuffer TestBuffer
{
    //float4 UnsizedLastMemberTest[] = {0,0,0,0};
    //float4 CBTestA;
    //float4 CBTestB;
    //float4 CBTestC;
    float4 CBTestArray[3];
};
//CBUFFER_END




#if defined(UNIT_STRIDES)
#define _StrideParam uint4(1,1,1,1)
#else
#define _StrideParam _Stride
#endif


// ---------------------------------------------------------------------------------------------------------------------------------------
// Main config:
// Check list when adding a variant:
//  -switch on/off NB_THREADIDS_FROM_NUMTHREADS_XY : 
//      -nb threadids and organization to use in the first phase of storing to LDS cache is tied to NUMTHREADS_X and NUMTHREADS_Y
//      -will also define USE_ALTERNATE_LDS_ORG_*
//
//  -force always on/off CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE regardless of K1x1


//#define BIAS_AFTER
//..slightly faster before

// ------------------------------------------------------------------
// Custom number of threads for storing values in LDS cache:
// ------------------------------------------------------------------


// This configures the usage of threadids for the final output phase
// must correspond to the way the dispatch sizes the launched grid (ie nb TG for outchans and spatials)

#if !defined(OUT_OUTCHAN_USES_HIBITS) && !defined(OUT_OUTCHAN_USES_LOBITS)
#define OUT_OUTCHAN_USES_HIBITS            // ie NUMTHREADS_OUTCHAN == NUMTHREADS_Y
#endif

//To debug that mode:
//#define OUT_OUTCHAN_USES_LOBITS
//#define TRANSPOSE_OUTPUT

//#define USE_BANK_CONFLICT_OFFSET // to test when NOT using USE_ALTERNATE_LDS_ORG_*  (later is used when NB_THREADIDS_FROM_NUMTHREADS_XY)

#if !defined(MAIN_CONFIG_NUMBER)
    // Faster to define NB_THREADIDS_FROM_NUMTHREADS_XY:
    #define MAIN_CONFIG_NUMBER 0
#endif

#if (MAIN_CONFIG_NUMBER == 0)
    #define NB_THREADIDS_FROM_NUMTHREADS_XY
    #define USE_ALTERNATE_LDS_ORG_W // TODO investigate, saves 5-6 / 42ms
    #define USE_ALTERNATE_LDS_ORG_X // TODO investigate (hunch: drvopt), saves 2ms / 40ms 
#endif

#if (MAIN_CONFIG_NUMBER == 1)
    // Mimicks NB_THREADIDS_FROM_NUMTHREADS_XY
    // but allows switching USE_ALTERNATE_LDS_ORG_* or not

    // Custom config (cf just defining NB_THREADIDS_FROM_NUMTHREADS_XY)

    //
    //    LDS_STORE_W_BITPRIORITY_OUTCHAN // vs LDS_STORE_W_BITPRIORITY_INNER
    //    LDS_STORE_X_BITPRIORITY_SPATIALS // vs LDS_STORE_X_BITPRIORITY_INNER
    //    
    //    LDS_STORE_W_BITPRIORITY_LOBITS // vs HIBITS currently uses LOBITS
    //    LDS_STORE_X_BITPRIORITY_LOBITS // vs HIBITS currently uses LOBITS
    //
    //    LDS_STORE_W_USES_MAXBITS
    //    // otherwise must specify 
    //    // LDS_STORE_W_USES_THIS_NB_THREADIDS xx
    //    LDS_STORE_X_USES_MAXBITS 
    //    // otherwise must specify 
    //    // LDS_STORE_X_USES_THIS_NB_THREADIDS xx

    // The above defines control how to allocate threadids for reading 
    // in values from memory and storing them to LDS for sharing on reload.
    // The number of threaids after are fixed by the threadgroup (numthreads)
    // dispatch configuration and id usage specifiers OUT_OUTCHAN_USES_HIBITS or OUT_OUTCHAN_USES_LOBITS.
    // BITPRIORITY_ * macros selects which bits are assigned for what usage,
    // and then a 3rd macro specifies how many threadids are assigned
    // (or MAXBITS, to use the max that usage might require).

    #define USE_CUSTOM_THREAD_NB_AND_v1_FOR_LDS_STORE
    #define FORCE_ALLOW_DETECTION_NUMTHREADS_XY

    #define USE_ALTERNATE_LDS_ORG_W
    #define USE_ALTERNATE_LDS_ORG_X

    #define LDS_STORE_W_BITPRIORITY_OUTCHAN
    #define LDS_STORE_W_BITPRIORITY_HIBITS
    #define LDS_STORE_W_USES_THIS_NB_THREADIDS NUMTHREADS_Y

    #define LDS_STORE_X_BITPRIORITY_SPATIALS
    #define LDS_STORE_X_BITPRIORITY_LOBITS
    #define LDS_STORE_X_USES_THIS_NB_THREADIDS NUMTHREADS_X
#endif

#if (MAIN_CONFIG_NUMBER == 2)
    // Custom config (cf just defining NB_THREADIDS_FROM_NUMTHREADS_XY)
    // Every low bits used to enumerate for outchan from kernel
    // then low bits used for spatial starting coord (for kernel overlay) from X input.
    // NOT optimal wrt streaming the kernel from global memory
    #define LDS_STORE_W_BITPRIORITY_OUTCHAN
    #define LDS_STORE_W_BITPRIORITY_LOBITS
    #define LDS_STORE_W_USES_MAXBITS

    #define LDS_STORE_X_BITPRIORITY_SPATIALS
    #define LDS_STORE_X_BITPRIORITY_LOBITS
    #define LDS_STORE_X_USES_MAXBITS
#endif

#if (MAIN_CONFIG_NUMBER == 3)
    // Custom config (cf just defining NB_THREADIDS_FROM_NUMTHREADS_XY)
    //
    // Use _v4 macros: inner chunk == low bits
    //
    #define USE_ALTERNATE_LDS_ORG_W
    #define USE_ALTERNATE_LDS_ORG_X

    #define LDS_STORE_W_BITPRIORITY_INNER
    #define LDS_STORE_W_BITPRIORITY_LOBITS
    //#define LDS_STORE_W_USES_MAXBITS
    // means CACHE_DEPTH in that case since we selected priority to INNER
    #define LDS_STORE_W_USES_THIS_NB_THREADIDS 16

    #define LDS_STORE_X_BITPRIORITY_INNER
    #define LDS_STORE_X_BITPRIORITY_LOBITS
    #define LDS_STORE_X_USES_MAXBITS
    //#define LDS_STORE_X_USES_MAXBITS
    // means CACHE_DEPTH in that case since we selected priority to INNER
    #define LDS_STORE_X_USES_THIS_NB_THREADIDS 8
#endif

#if (MAIN_CONFIG_NUMBER == 4)
    // Custom config (cf just defining NB_THREADIDS_FROM_NUMTHREADS_XY)
    //
    // Almost like NB_THREADIDS_FROM_NUMTHREADS_XY
    // But spatials will use high bits while inner will use low bits (.x)
    // Use _v4 macros: inner chunk == low bits
    //
    #define FORCE_ALLOW_DETECTION_NUMTHREADS_XY
    #define USE_ALTERNATE_LDS_ORG_W
    #define USE_ALTERNATE_LDS_ORG_X

    #define LDS_STORE_W_BITPRIORITY_OUTCHAN
    #define LDS_STORE_W_BITPRIORITY_HIBITS
    #define LDS_STORE_W_USES_THIS_NB_THREADIDS NUMTHREADS_Y

    #define LDS_STORE_X_BITPRIORITY_SPATIALS
    #define LDS_STORE_X_BITPRIORITY_HIBITS
    #define LDS_STORE_X_USES_THIS_NB_THREADIDS NUMTHREADS_X

#endif


//
// IMPORTANT:
//
// Once an above MAIN_CONFIG_NUMBER is chosen, make sure an appropriate
// set of access pattern to/from LDS will be used, search below the section
// defining various CALCULATE_LDS_STORE *
// 


// For defined(NB_THREADIDS_FROM_NUMTHREADS_XY) || defined(USE_CUSTOM_THREAD_NB_AND_v1_FOR_LDS_STORE)
// vs USE_ALTERNATE_LDS_ORG_*
//
//
//
//  Although USE_ALTERNATE_LDS_ORG_X and USE_ALTERNATE_LDS_ORG_W
//  can be defined independently of NB_THREADIDS_FROM_NUMTHREADS_XY,
//  the reloading/broadcast code (ie *from*) LDS need to use the final purpose
//  of threadids, ie where groupThreadID.x == spatial output and groupThreadID.y == output channels,
//  for which we cache for NUMTHREADS_X * ELEM_PER_THREAD_X and NUMTHREADS_Y * ELEM_PER_THREAD_Y
//  different number of indices in the LDS cache respectively
//  (CACHE_DEPTH of inner product data for each different indices).
//
//  It follows that whatever organization we use to fill the LDS, we need to:
//
//      1) access it the same way on reload that we used on store
//      2) access the final number required for each of the final threadids
//         (each thread holds NB_ACCU_REG_PER_THREAD == ELEM_PER_THREAD_X * ELEM_PER_THREAD_Y 
//         - ie this is the total nb of explicit values held per threads -
//         and like we said above because in the final phase threadids in x and y are assigned to output spatial idx or output channel idx,
//         cached values for a total of 
//              NB_SPATIALS_PER_TG = NUMTHREADS_X * ELEM_PER_THREAD_X 
//         and 
//              NB_OUTCHAN_PER_TG = NUMTHREADS_Y * ELEM_PER_THREAD_Y 
//         different indices should be available.)
//
//      (We can add also 
//         3) For biases, make sure that the order (not only their number) in which we select the output 
//         channels match the way they are initialized in the accumulators,
//         see outOutchanThreadBaseOffset + eoc * NUMTHREADS_OUTCHAN
//         when we access Bptr.
//       but this point normally follows naturally from the fact that accumulators are initialized
//       as expected by final threadids purpose.)



// ------------------------------------------------------------------


// debug, to force compilation of path to check for compilation errors:
//#define GROUPS_ENABLED


#if !defined(K1x1)
#define CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE
#endif

// #if defined(K1x1)
// #undef K1x1
// #endif

#if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) && defined(SHADER_API_WEBGPU)
// For SHADER_API_WEBGPU even when K1x1 (ie no spatial window), see ConvTranspose.compute, we still enable the caching 
// path as if there was a spatial window, as this removes the triple for loop for it (but needs in turn to recover the individual window coord from a linear index)
#define CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE
#endif


// ---------------------------------------------------------------------------------------------------------------------------------------

// To quickly test variants

//#define CACHE_DEPTH 16 // 32


// #if 0
// #define NUMTHREADS_X 64
// #define NUMTHREADS_Y 4
// #define ELEM_PER_THREAD_X 1
// #define ELEM_PER_THREAD_Y 1

// #elif !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)

// //#define BIAS_AFTER

// #define NUMTHREADS_X 16
// #define NUMTHREADS_Y 8
// #define ELEM_PER_THREAD_X 4
// #define ELEM_PER_THREAD_Y 8

// #else
// //#define BIAS_AFTER

// #define NUMTHREADS_X 16
// #define NUMTHREADS_Y 8
// #define ELEM_PER_THREAD_X 4
// #define ELEM_PER_THREAD_Y 8
// #endif


// ------------------------------------------------------------------
// Main threadgroup config and final output thread config
// ------------------------------------------------------------------

// WIPWIP
// Final output config.
// (Numthreads should be more precisely read as "number of thread IDs" or log2 number of bits in threadid)
//
#if defined(OUT_OUTCHAN_USES_HIBITS)
    #if defined(TRANSPOSE_OUTPUT)
        #warning "Using OUT_OUTCHAN_USES_HIBITS with TRANSPOSE_OUTPUT is slow and trashes memory bandwidth on output"
    #endif

    #define NUMTHREADS_SPATIALS         NUMTHREADS_X
    #define NUMTHREADS_OUTCHAN          NUMTHREADS_Y
    #define SPATIALS_ELEM_PER_THREAD    ELEM_PER_THREAD_X
    #define OUTCHAN_ELEM_PER_THREAD     ELEM_PER_THREAD_Y

    #define SELECT_FINAL_OUT_SPATIALS_THREADIDS(ti, gtid) (gtid.x)
    #define SELECT_FINAL_OUT_OUTCHAN_THREADIDS(ti, gtid)  (gtid.y)
    #define SELECT_FINAL_OUT_SPATIALS_THREADGROUP_ID(tgid) (tgid.x)
    #define SELECT_FINAL_OUT_OUTCHAN_THREADGROUP_ID(tgid)  (tgid.y)
#else
    #if !defined(OUT_OUTCHAN_USES_LOBITS)
        #error "OUT_OUTCHAN_USES_xxBITS mode: need to specify _HIBITS or _LOBITS"
    #endif

    #if !defined(TRANSPOSE_OUTPUT)
        #warning "Using OUT_OUTCHAN_USES_LOBITS without TRANSPOSE_OUTPUT is slow and trashes memory bandwidth on output"
    #endif

    #define NUMTHREADS_SPATIALS         NUMTHREADS_Y
    #define NUMTHREADS_OUTCHAN          NUMTHREADS_X
    #define SPATIALS_ELEM_PER_THREAD    ELEM_PER_THREAD_Y
    #define OUTCHAN_ELEM_PER_THREAD     ELEM_PER_THREAD_X

    #define SELECT_FINAL_OUT_SPATIALS_THREADIDS(ti, gtid) (gtid.y)
    #define SELECT_FINAL_OUT_OUTCHAN_THREADIDS(ti, gtid)  (gtid.x)
    #define SELECT_FINAL_OUT_SPATIALS_THREADGROUP_ID(tgid) (tgid.y)
    #define SELECT_FINAL_OUT_OUTCHAN_THREADGROUP_ID(tgid)  (tgid.x)

#endif


#define NB_THREADS_PER_TG (NUMTHREADS_X * NUMTHREADS_Y)
#define LOG2_NB_THREADS_PER_TG uint(log2(NB_THREADS_PER_TG))

// Output channels and spatial elements per TG
#define NB_SPATIALS_PER_TG (SPATIALS_ELEM_PER_THREAD * NUMTHREADS_SPATIALS)
#define LOG2_NB_SPATIALS_PER_TG uint(log2(NB_SPATIALS_PER_TG))

#define NB_OUTCHAN_PER_TG (OUTCHAN_ELEM_PER_THREAD * NUMTHREADS_OUTCHAN)
#define LOG2_NB_OUTCHAN_PER_TG uint(log2(NB_OUTCHAN_PER_TG))


#define LOG2_NUMTHREADS_SPATIALS         uint(log2(NUMTHREADS_SPATIALS))
#define LOG2_NUMTHREADS_OUTCHAN          uint(log2(NUMTHREADS_OUTCHAN))

#define LOG2_SPATIALS_ELEM_PER_THREAD         uint(log2(SPATIALS_ELEM_PER_THREAD))
#define LOG2_OUTCHAN_ELEM_PER_THREAD          uint(log2(OUTCHAN_ELEM_PER_THREAD))

// ------------------------------------------------------------------


// Bug somewhere in compiler pipeline: simply this 
//#if (NB_OUTCHAN_PER_TG) / GROUP_NB_OC_CHAN_DIVIDER == 0
// will cause this: 
//Shader error in 'ConvGeneric.compute': Internal error communicating with the shader compiler process.  Please report a bug including this shader and the editor log. Error code 0x80000004 (Not connected).
// Valid arithmetic expressions with a minus ("-") sign will give an "Invalid conditional expression" error...
//#endif
// The following can at least fix what seems to be a division by 0 crash:
//#ifndef GROUP_NB_OC_CHAN_DIVIDER
//#define GROUP_NB_OC_CHAN_DIVIDER 1
//#endif
//
// There's no modulo in the preprocessor ops anyway so for now,
// just add a equality check, each config can be validated manually if needed
#if defined(GROUPS_ENABLED)
#if NB_OUTCHAN_PER_TG != GROUP_NB_OC_CHAN_DIVIDER

    #if !defined(TRANSPOSE_OUTPUT) // not supported in these variants
        #define NON_UNIFORM_CONVGROUP_PER_OC
    #else
        #warning "TRANSPOSE_OUTPUT variants not supported with NON_UNIFORM_CONVGROUP_PER_OC, make sure this variant is not used"
    #endif // #if !defined(TRANSPOSE_OUTPUT)

#endif
#endif


#if !defined(SHADER_API_D3D11)
#define MIN_WHEN_NON_D3D(val, maxidx) (min((val), (maxidx)))
#else
#define MIN_WHEN_NON_D3D(val, maxidx) (val)
#endif

//#if defined(SHADER_API_METAL) || defined(UNITY_PLATFORM_OSX) || defined(UNITY_PLATFORM_IOS)
#if defined(UNITY_PLATFORM_IOS)
#define MIN_PATCHBUG(val, maxidx) (min((val), (maxidx)))
#else
#define MIN_PATCHBUG(val, maxidx) (val)
#endif


#if defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
#define IF_ONLY_INCHAN_ELSE(a, b) (b)
#else
#define IF_ONLY_INCHAN_ELSE(a, b) (a)
#endif

#if defined(CONV_TRANSPOSE)
#define IF_CONV_TRANSPOSE_ELSE(a, b) (a)
#else
#define IF_CONV_TRANSPOSE_ELSE(a, b) (b)
#endif

#if defined(GROUPS_ENABLED)
#define IF_HAVE_GROUPS_ELSE(a, b) (a)
#else
#define IF_HAVE_GROUPS_ELSE(a, b) (b)
#endif

#if defined(NON_UNIFORM_CONVGROUP_PER_OC)
#define IF_NON_UNIFORM_GROUPS_ELSE(a, b) (a)
#else
#define IF_NON_UNIFORM_GROUPS_ELSE(a, b) (b)
#endif


#if defined(GROUPS_ENABLED) && defined(CONV_TRANSPOSE)
#define CONV_TRANSPOSE_WITH_GROUPS
#define IF_CONV_TRANSPOSE_GROUPS(a, b) (a)
#else
#define IF_CONV_TRANSPOSE_GROUPS(a, b) (b)
#endif


#define NB_BANKS 32
#define LOG2_NB_BANKS 5


#define LOG2_CACHE_DEPTH uint(log2(CACHE_DEPTH))


// Main derived config that makes the rest of the code generic:
//
// NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE
// NB_THREADIDS_FOR_INNER_W_LDS_STORE
//
// SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE
// SELECT_FROMTHREADIDS_AS_INNER_FOR_W_LDS_STORE
//
// NB_EXPLICIT_INNER_VAL_NEEDED_W_LDS_STORE
//
// NB_THREADIDS_FOR_INNER_AND_OUTCHAN_W_LDS_STORE
// eg stride when doing multiple load/store in LDS   (using the counter )
//
// IF_THREAD_DOING_W_LDS_STORE
// ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE_IDX
// ->in the original conv scheme just "ti" because all threads index into LDS, high part are for the inner cache chunk,
// low part is for all the output channels
//
// NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE
// NB_THREADIDS_FOR_INNER_X_LDS_STORE
// 
// SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE
// SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE
// 
// NB_EXPLICIT_INNER_VAL_NEEDED_X_LDS_STORE
// 
// NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE
// 
// IF_THREAD_DOING_X_LDS_STORE
// ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE_IDX
//
//
// need also (then not enough threadids left for inner chunk, used in inner product sum)
//    NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE
//    NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE


// ------------------------------------------------------------------
// Handling NB_THREADIDS_FROM_NUMTHREADS_XY

#if defined(NB_THREADIDS_FROM_NUMTHREADS_XY)
    #define LDS_STORE_W_BITPRIORITY_OUTCHAN
    #define LDS_STORE_W_BITPRIORITY_HIBITS
    #define LDS_STORE_W_USES_THIS_NB_THREADIDS NUMTHREADS_Y

    #define LDS_STORE_X_BITPRIORITY_SPATIALS
    #define LDS_STORE_X_BITPRIORITY_LOBITS
    #define LDS_STORE_X_USES_THIS_NB_THREADIDS NUMTHREADS_X
#endif

#if 0
    #if (defined(LDS_STORE_W_BITPRIORITY_OUTCHAN) && defined(LDS_STORE_W_BITPRIORITY_HIBITS) && (LDS_STORE_W_USES_THIS_NB_THREADIDS == NUMTHREADS_Y)   \
        || defined(LDS_STORE_W_BITPRIORITY_INNER) && defined(LDS_STORE_W_BITPRIORITY_LOBITS) && (LDS_STORE_W_USES_THIS_NB_THREADIDS == NUMTHREADS_X) ) \
        /* note how the 2 lines above end up doing the same */                                                                                    \
        && \
        (defined(LDS_STORE_X_BITPRIORITY_SPATIALS) && defined(LDS_STORE_X_BITPRIORITY_LOBITS) && (LDS_STORE_X_USES_THIS_NB_THREADIDS == NUMTHREADS_X)  \
        || defined(LDS_STORE_X_BITPRIORITY_INNER) && defined(LDS_STORE_X_BITPRIORITY_HIBITS) && (LDS_STORE_X_USES_THIS_NB_THREADIDS == NUMTHREADS_Y) ) \
        /* note how the 2 lines above end up doing the same */

        #if !defined(NB_THREADIDS_FROM_NUMTHREADS_XY)
            #if !defined(DISABLE_SHADER_WARNING)
                #warning "Configuration of threadids matches NB_THREADIDS_FROM_NUMTHREADS_XY, enabling..."
            #endif
        #define NB_THREADIDS_FROM_NUMTHREADS_XY
        #endif
    #endif
#endif
// ------------------------------------------------------------------


// ------------------------------------------------------------------
// LDS rebroadcast and MAD to final output writing threads
// ELEM_PER_THREAD_X ELEM_PER_THREAD_Y

#define NB_ACCU_REG_PER_THREAD (ELEM_PER_THREAD_X * ELEM_PER_THREAD_Y)

// ------------------------------------------------------------------


// kicsp = kernel inchan and possible spatial (ie inside a single output channel slice of the kernel)
// eoc = extra output channel per oc index
// oc = output channel

//------------------------------------------------------------------------
// LDS_STORE_W:
//
//      _MAXBITS or _NB_THREADIDS mode
//      also uses _BITPRIORITY_OUTCHAN or _BITPRIORITY_INNER
//      to finalize LDS_STORE_W_USES_THIS_NB_THREADIDS
//
//      Later we use again _BITPRIORITY_OUTCHAN or _BITPRIORITY_INNER
//      to set all other W_LDS_STORE required macros (access masks, NB threads, IF_THREAD_DOING, etc.)

    #if !defined(LDS_STORE_W_USES_THIS_NB_THREADIDS)
        //
        // nb bits: using a maximum for a specific purpose: either outchan or inner slice
        //
        #if !defined(LDS_STORE_W_USES_MAXBITS)
        #error "LDS_STORE_W mode: need to specify _MAXBITS or _THIS_NB_THREADIDS xx where xx is the nb of bits required"
        #endif

        #if defined(LDS_STORE_W_BITPRIORITY_OUTCHAN)
            // 
            // Use maxbits for outchan
            //
            #define LDS_STORE_W_USES_THIS_NB_THREADIDS NB_OUTCHAN_PER_TG
        #else
            // else for inner product chunk
            #if !defined(LDS_STORE_W_BITPRIORITY_INNER)
            #error "LDS_STORE_W_BITPRIORITY mode: need to specify _OUTCHAN or _INNER"
            #endif
            #define LDS_STORE_W_USES_THIS_NB_THREADIDS CACHE_DEPTH
        #endif
    #else
        // nb bits: using a specific number
        #if !defined(LDS_STORE_W_USES_THIS_NB_THREADIDS)
        #error "LDS_STORE_W mode: need to specify _MAXBITS or _THIS_NB_THREADIDS xx where xx is the nb of bits required"
        #endif
    #endif // if !defined(LDS_STORE_W_USES_THIS_NB_THREADIDS)

    //
    // LDS_STORE_W_USES_THIS_NB_THREADIDS should now be known,
    // need to sanitize it vs actual available 
    //
    #if defined(LDS_STORE_W_BITPRIORITY_OUTCHAN)
        #if LDS_STORE_W_USES_THIS_NB_THREADIDS > NB_OUTCHAN_PER_TG
            #undef LDS_STORE_W_USES_THIS_NB_THREADIDS
            #define LDS_STORE_W_USES_THIS_NB_THREADIDS NB_OUTCHAN_PER_TG
        #endif
    #else
        // else for inner product chunk
        #if LDS_STORE_W_USES_THIS_NB_THREADIDS > CACHE_DEPTH
            #undef LDS_STORE_W_USES_THIS_NB_THREADIDS
            #define LDS_STORE_W_USES_THIS_NB_THREADIDS CACHE_DEPTH
        #endif
    #endif

    #if LDS_STORE_W_USES_THIS_NB_THREADIDS > NB_THREADS_PER_TG
        #warning "CACHE_DEPTH or NB_OUTCHAN_PER_TG is larger than total NB_THREADS_PER_TG available"
        #undef LDS_STORE_W_USES_THIS_NB_THREADIDS
        #define LDS_STORE_W_USES_THIS_NB_THREADIDS NB_THREADS_PER_TG
    #endif

//
// Synthesize these intermediate defines:
//
// LDS_STORE_W_HIBITS_ARE_THREADIDS_Y
// LDS_STORE_W_OUTCHAN_LOBITS or LDS_STORE_W_OUTCHAN_HIBITS

    //
    // From num of threadids and priority, check if later masks are just the x or y thread group id:
    //
    #if !defined(LDS_STORE_W_BITPRIORITY_HIBITS) && !defined(LDS_STORE_W_BITPRIORITY_LOBITS)
        #error "LDS_STORE_W_BITPRIORITY mode: need to specify _HIBITS or LOBITS"
    #endif

    // ONLY if NB_THREADIDS_FROM_NUMTHREADS_XY
    #if defined(NB_THREADIDS_FROM_NUMTHREADS_XY) || defined(FORCE_ALLOW_DETECTION_NUMTHREADS_XY)
        #if ( (defined(LDS_STORE_W_BITPRIORITY_HIBITS) && (LDS_STORE_W_USES_THIS_NB_THREADIDS == NUMTHREADS_Y)) \
            || (defined(LDS_STORE_W_BITPRIORITY_LOBITS) && (LDS_STORE_W_USES_THIS_NB_THREADIDS == NUMTHREADS_X)) )
            #define LDS_STORE_W_HIBITS_ARE_THREADIDS_Y
            // would be the same as a define like LDS_STORE_W_LOBITS_ARE_THREADIDS_X
        #endif
    #endif
    // ...and those high/low bits are for LDS_STORE_W_BITPRIORITY_OUTCHAN or LDS_STORE_W_BITPRIORITY_INNER

    //
    // Combine threadids high/low priority with outchan/inner
    // (we needed those separate to properly select the nb of threadids given various limits like total NB_THREADS_PER_TG, CACHE_DEPTH,
    // but for selection mask definitions, we can combine those:
    // 
    #if !defined(LDS_STORE_W_BITPRIORITY_OUTCHAN) && !defined(LDS_STORE_W_BITPRIORITY_INNER)
    #error "LDS_STORE_W_BITPRIORITY mode: need to specify _OUTCHAN or _INNER"
    #endif
    #if ( (defined(LDS_STORE_W_BITPRIORITY_OUTCHAN) && defined(LDS_STORE_W_BITPRIORITY_HIBITS))  \
        || (defined(LDS_STORE_W_BITPRIORITY_INNER) && defined(LDS_STORE_W_BITPRIORITY_LOBITS)) )
        #define LDS_STORE_W_OUTCHAN_HIBITS
    #elif ( (defined(LDS_STORE_W_BITPRIORITY_OUTCHAN) && defined(LDS_STORE_W_BITPRIORITY_LOBITS))  \
          || (defined(LDS_STORE_W_BITPRIORITY_INNER) && defined(LDS_STORE_W_BITPRIORITY_HIBITS)) )
        #define LDS_STORE_W_OUTCHAN_LOBITS
    #endif



// Defined those main needed values and macros needed:
//
// NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE
// NB_THREADIDS_FOR_INNER_W_LDS_STORE
//
// SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE
// SELECT_FROMTHREADIDS_AS_INNER_FOR_W_LDS_STORE
//
// NB_EXPLICIT_INNER_VAL_NEEDED_W_LDS_STORE
// NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE
//
// NB_THREADIDS_FOR_INNER_AND_OUTCHAN_W_LDS_STORE
//
// IF_THREAD_DOING_W_LDS_STORE
// ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE_IDX
//


        #if defined(LDS_STORE_W_BITPRIORITY_OUTCHAN)
            // Note: in that case, LDS_STORE_OUTCHAN_USES_THIS_NB_THREADIDS has already been clamped by NB_OUTCHAN_PER_TG)
            #define NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE LDS_STORE_W_USES_THIS_NB_THREADIDS

            // assert: no modulo in preproc macro ops :/
            #if !( ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 1) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 2) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 4) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 8) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 16) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 32) )
            #error NB_THREADS_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE should be a power of 2
            #endif

            #define NB_THREADIDS_FOR_INNER_W_LDS_STORE (NB_THREADS_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE)

            // Sanitize to the max required: we've assigned threadids for outchan,
            // either there's no ids (ie just 1 - the same across all TG) left for the inner chunk,
            // or there's more than 1,
            // check if what's left for inner chunk is larger than CACHE_DEPTH and clamp to that,
            // also indicate that condition for later (see IF_THREAD_DOING_W_LDS_STORE)
            //
            #if (NB_THREADIDS_FOR_INNER_W_LDS_STORE > CACHE_DEPTH)
                #warning "Priority to outchan and we're left with NB_THREADIDS_FOR_INNER_W_LDS_STORE > CACHE_DEPTH, clamping"
                #undef NB_THREADIDS_FOR_INNER_W_LDS_STORE
                #define NB_THREADIDS_FOR_INNER_W_LDS_STORE CACHE_DEPTH

                #define NB_THREADIDS_LEFT_FOR_INNER_W_LDS_STORE_WASGT_CACHE_DEPTH
            #endif

        #else
            // else for inner product chunk
            // Note: in that case, LDS_STORE_OUTCHAN_USES_THIS_NB_THREADIDS has already been clamped by CACHE_DEPTH)
            #if !defined(LDS_STORE_W_BITPRIORITY_INNER)
            #error "LDS_STORE_W_BITPRIORITY mode: need to specify _OUTCHAN or _INNER"
            #endif
            #define NB_THREADIDS_FOR_INNER_W_LDS_STORE LDS_STORE_W_USES_THIS_NB_THREADIDS

            // assert: no modulo in preproc macro ops :/ 
            #if !( ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_INNER_W_LDS_STORE) == 1) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_INNER_W_LDS_STORE) == 2) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_INNER_W_LDS_STORE) == 4) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_INNER_W_LDS_STORE) == 8) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_INNER_W_LDS_STORE) == 16) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_INNER_W_LDS_STORE) == 32) )
            #error NB_THREADS_PER_TG/NB_THREADIDS_FOR_INNER_W_LDS_STORE should be a power of 2
            #endif

            #define NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE (NB_THREADS_PER_TG / NB_THREADIDS_FOR_INNER_W_LDS_STORE)

            // Sanitize to the max required: we've assigned threadids for inner,
            // either there's no ids (ie just 1 - the same across all TG) left for the outchans,
            // or there's more than 1,
            // check if what's left for outchan is larger than NB_OUTCHAN_PER_TG and clamp to that,
            // also indicate that condition for later (see IF_THREAD_DOING_W_LDS_STORE)
            //
            #if (NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE > NB_OUTCHAN_PER_TG)
                #warning "Priority to outchan and we're left with NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE > NB_OUTCHAN_PER_TG, clamping"
                #undef NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE
                #define NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE NB_OUTCHAN_PER_TG

                #define NB_THREADIDS_LEFT_FOR_OUTCHAN_W_LDS_STORE_WASGT_NB_OUTCHAN_PER_TG
            #endif

        #endif

// New synthesized intermediate:
//
// NB_THREADIDS_LEFT_FOR_INNER_W_LDS_STORE_WASGT_CACHE_DEPTH
// NB_THREADIDS_LEFT_FOR_OUTCHAN_W_LDS_STORE_WASGT_NB_OUTCHAN_PER_TG

// Explicit values (regs) needed for OUTCHAN:
#if !( ((NB_OUTCHAN_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 1) \
    || ((NB_OUTCHAN_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 2) \
    || ((NB_OUTCHAN_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 4) \
    || ((NB_OUTCHAN_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 8) \
    || ((NB_OUTCHAN_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 16) \
    || ((NB_OUTCHAN_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 32) )
#error NB_OUTCHAN_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE should be a power of 2
#endif

#define LOG2_NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE uint(log2(NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE))

#define NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE (NB_OUTCHAN_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE)

// Similarly for inner:
// Explicit values (regs) needed for INNER:
//
#if !( ((CACHE_DEPTH/NB_THREADIDS_FOR_INNER_W_LDS_STORE) == 1) \
    || ((CACHE_DEPTH/NB_THREADIDS_FOR_INNER_W_LDS_STORE) == 2) \
    || ((CACHE_DEPTH/NB_THREADIDS_FOR_INNER_W_LDS_STORE) == 4) \
    || ((CACHE_DEPTH/NB_THREADIDS_FOR_INNER_W_LDS_STORE) == 8) \
    || ((CACHE_DEPTH/NB_THREADIDS_FOR_INNER_W_LDS_STORE) == 16) \
    || ((CACHE_DEPTH/NB_THREADIDS_FOR_INNER_W_LDS_STORE) == 32) )
#error CACHE_DEPTH/NB_THREADIDS_FOR_INNER_W_LDS_STORE should be a power of 2
#endif

#define LOG2_NB_THREADIDS_FOR_INNER_W_LDS_STORE uint(log2(NB_THREADIDS_FOR_INNER_W_LDS_STORE))

#define NB_EXPLICIT_INNER_VAL_NEEDED_W_LDS_STORE (CACHE_DEPTH / NB_THREADIDS_FOR_INNER_W_LDS_STORE)

//
// Threadids selection masks
//
// Uses
//      LDS_STORE_W_BITPRIORITY_OUTCHAN
//      LDS_STORE_W_BITPRIORITY_HIBITS
//      LDS_STORE_W_USES_THIS_NB_THREADIDS
//
// Uses also these new synthesized intermediate defines:
//
//      LDS_STORE_W_HIBITS_ARE_THREADIDS_Y
//      LDS_STORE_W_OUTCHAN_LOBITS or LDS_STORE_W_OUTCHAN_HIBITS
//
//      NB_THREADIDS_LEFT_FOR_INNER_W_LDS_STORE_WASGT_CACHE_DEPTH
//      NB_THREADIDS_LEFT_FOR_OUTCHAN_W_LDS_STORE_WASGT_NB_OUTCHAN_PER_TG


#if (NB_THREADIDS_LEFT_FOR_INNER_W_LDS_STORE_WASGT_CACHE_DEPTH || NB_THREADIDS_LEFT_FOR_OUTCHAN_W_LDS_STORE_WASGT_NB_OUTCHAN_PER_TG)

    #define NOT_ALL_THREADS_DO_W_LDS_STORE

    #if (NB_THREADIDS_FOR_INNER_W_LDS_STORE < CACHE_DEPTH) || (NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE < NB_OUTCHAN_PER_TG)
        #error "NOT_ALL_THREADS_DO_W_LDS_STORE and NB_THREADIDS_ assigned for inner chunk or outchan aren't maximized!"
    #endif

    #define NB_THREADIDS_FOR_INNER_AND_OUTCHAN_W_LDS_STORE (NB_THREADIDS_FOR_INNER_W_LDS_STORE * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE)

#else

    #define NB_THREADIDS_FOR_INNER_AND_OUTCHAN_W_LDS_STORE (NB_THREADS_PER_TG)

    #if (NB_THREADIDS_FOR_INNER_W_LDS_STORE * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) != NB_THREADS_PER_TG
        #error "(NB_THREADIDS_FOR_INNER_W_LDS_STORE * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) should be == NB_THREADS_PER_TG"
    #endif

    #define IF_THREAD_DOING_W_LDS_STORE(ti, gtid) 
    //...passthrough as all threads participate

#endif


// Threadids selection masks (continued), divided into 
// LDS_STORE_W_OUTCHAN_HIBITS and
// LDS_STORE_W_OUTCHAN_LOBITS sections.

#if defined(LDS_STORE_W_OUTCHAN_HIBITS)
    #if defined(LDS_STORE_W_HIBITS_ARE_THREADIDS_Y)
        #if !defined(DISABLE_SHADER_WARNING)
            #warning "should have (NB_THREADIDS_FROM_NUMTHREADS_XY) - NOTE: path like previous macros"
        #endif

        #define SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) (gtid.y)
        #define SELECT_FROMTHREADIDS_AS_INNER_FOR_W_LDS_STORE(ti, gtid) (gtid.x)

        #if defined(NOT_ALL_THREADS_DO_W_LDS_STORE)
            #if defined(LDS_STORE_W_BITPRIORITY_OUTCHAN) 
                // OUTCHAN was designated/selected as the subject of other options like LOBITS/HIBITS, and here we know OUTCHAN == HIBITS,
                // thus we know LDS_STORE_W_BITPRIORITY_HIBITS
                #if !defined(LDS_STORE_W_BITPRIORITY_HIBITS)
                    #error "Bad config"
                #endif
                // We're also in the context of LDS_STORE_W_HIBITS_ARE_THREADIDS_Y,
                // so unused threadids are in the middle, ie high bits of threadids.x, bad`
                #warning "Suboptimal config detected, see code..."
                #define IF_THREAD_DOING_W_LDS_STORE(ti, gtid) if (gtid.x < CACHE_DEPTH)
                // Since LDS_STORE_W_HIBITS_ARE_THREADIDS_Y should be used when defined(NB_THREADIDS_FROM_NUMTHREADS_XY),
                // this shouldn't be needed:
                //#define ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE_IDX(ti, gtid) ((gtid.y << LOG2_CACHE_DEPTH) | (gtid.x & (CACHE_DEPTH-1)))

            #elif defined(LDS_STORE_W_BITPRIORITY_INNER)
                // INNER was designated/selected as the subject of other options like LOBITS/HIBITS, and here we know OUTCHAN == HIBITS,
                // thus we know LDS_STORE_W_BITPRIORITY_LOBITS
                #if !defined(LDS_STORE_W_BITPRIORITY_LOBITS)
                    #error "Bad config"
                #endif
                // We're also in the context of LDS_STORE_W_HIBITS_ARE_THREADIDS_Y,
                // so unused threadids are for outchan and high bits of threadids.y, we're ok.
                #define IF_THREAD_DOING_W_LDS_STORE(ti, gtid) if (gtid.y < NB_OUTCHAN_PER_TG)
                // this would work too: #define IF_THREAD_DOING_W_LDS_STORE(ti, gtid) if ((ti) < NB_THREADIDS_FOR_INNER_AND_OUTCHAN_W_LDS_STORE)
                //#define ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE_IDX(ti, gtid) (ti)
                // ...we forgo a mask like ((ti) & (NB_THREADIDS_FOR_INNER_AND_OUTCHAN_W_LDS_STORE-1)) since it should only be used
                // in a block of code guarded by IF_THREAD_DOING_W_LDS_STORE()
            #else
                #error "_OUTCHAN or _INNER expected"
            #endif
        #else
            // All threads participate: IF_THREAD_DOING_W_LDS_STORE already defined
            // We shouldnt need this:
            //#define ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE_IDX(ti, gtid) (ti)
        #endif
    #else
        // We don't use gtid.x or .y here

        #if defined(NOT_ALL_THREADS_DO_W_LDS_STORE)
            #if defined(LDS_STORE_W_BITPRIORITY_OUTCHAN)
                // OUTCHAN was designated/selected as the subject of other options like LOBITS/HIBITS, and here we know OUTCHAN == HIBITS,
                // thus we know LDS_STORE_W_BITPRIORITY_HIBITS
                #if !defined(LDS_STORE_W_BITPRIORITY_HIBITS)
                    #error "Bad config"
                #endif
            #elif defined(LDS_STORE_W_BITPRIORITY_INNER)
                // INNER was designated/selected as the subject of other options like LOBITS/HIBITS, and here we know OUTCHAN == HIBITS,
                // thus we know LDS_STORE_W_BITPRIORITY_LOBITS
                #if !defined(LDS_STORE_W_BITPRIORITY_LOBITS)
                    #error "Bad config"
                #endif
            #endif

            // This works for all of the above (outchan or inner priority):
            #define IF_THREAD_DOING_W_LDS_STORE(ti, gtid) if ((ti) < NB_THREADIDS_FOR_INNER_AND_OUTCHAN_W_LDS_STORE)
        #else
            // All threads participate: IF_THREAD_DOING_W_LDS_STORE already defined
        #endif

        #define SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) ((ti) >> LOG2_NB_THREADIDS_FOR_INNER_W_LDS_STORE)
        #define SELECT_FROMTHREADIDS_AS_INNER_FOR_W_LDS_STORE(ti, gtid)  ((ti) & (NB_THREADIDS_FOR_INNER_W_LDS_STORE-1))
        #define ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE_IDX(ti, gtid) (ti)

    #endif

#elif defined(LDS_STORE_W_OUTCHAN_LOBITS)
    #if defined(LDS_STORE_W_HIBITS_ARE_THREADIDS_Y)
        #warning "should have (NB_THREADIDS_FROM_NUMTHREADS_XY)"

        #define SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) (gtid.x) // outchan are low bits here!
        #define SELECT_FROMTHREADIDS_AS_INNER_FOR_W_LDS_STORE(ti, gtid) (gtid.y)

        #if defined(NOT_ALL_THREADS_DO_W_LDS_STORE)
            #if defined(LDS_STORE_W_BITPRIORITY_OUTCHAN) 
                // OUTCHAN was designated/selected as the subject of other options like LOBITS/HIBITS, and here we know OUTCHAN == LOBITS,
                // thus we know LDS_STORE_W_BITPRIORITY_LOBITS
                #if !defined(LDS_STORE_W_BITPRIORITY_LOBITS)
                    #error "Bad config"
                #endif
                // We're also in the context of LDS_STORE_W_HIBITS_ARE_THREADIDS_Y,
                // so unused threadids are in high bits, not used for INNER selection, ie high bits of threadids.y

                #define IF_THREAD_DOING_W_LDS_STORE(ti, gtid) if (gtid.y < CACHE_DEPTH)
                // this would work too: #define IF_THREAD_DOING_W_LDS_STORE(ti, gtid) if ((ti) < NB_THREADIDS_FOR_INNER_AND_OUTCHAN_W_LDS_STORE)

                // Since LDS_STORE_W_HIBITS_ARE_THREADIDS_Y should be used when defined(NB_THREADIDS_FROM_NUMTHREADS_XY),
                // this shouldn't be needed:
                //#define ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE_IDX(ti, gtid) (ti)
                // ...we forgo a mask like ((ti) & (NB_THREADIDS_FOR_INNER_AND_OUTCHAN_W_LDS_STORE-1)) since it should only be used
                // in a block of code guarded by IF_THREAD_DOING_W_LDS_STORE()

            #elif defined(LDS_STORE_W_BITPRIORITY_INNER)
                // INNER was designated/selected as the subject of other options like LOBITS/HIBITS, and here we know OUTCHAN == LOBITS,
                // thus we know LDS_STORE_W_BITPRIORITY_HIBITS
                #if !defined(LDS_STORE_W_BITPRIORITY_HIBITS)
                    #error "Bad config"
                #endif
                // We're also in the context of LDS_STORE_W_HIBITS_ARE_THREADIDS_Y,
                // so unused threadids are in the middle, the ones not used for OUTCHAN selection, ie high bits of threadids.x, bad`
                #warning "Suboptimal config detected, see code..."
                #define IF_THREAD_DOING_W_LDS_STORE(ti, gtid) if (gtid.x < NB_OUTCHAN_PER_TG)
                // Since LDS_STORE_W_HIBITS_ARE_THREADIDS_Y should be used when defined(NB_THREADIDS_FROM_NUMTHREADS_XY),
                // this shouldn't be needed:
                //#define ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE_IDX(ti, gtid) ((gtid.y << LOG2_NB_OUTCHAN_PER_TG) | (gtid.x & (NB_OUTCHAN_PER_TG-1)))
            #else
                #error "_OUTCHAN or _INNER expected"
            #endif
        #else
            // All threads participate: IF_THREAD_DOING_W_LDS_STORE already defined
            // We shouldnt need this:
            //#define ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE_IDX(ti, gtid) (ti)
        #endif
    #else
        // We don't use gtid.x or .y here

        #if defined(NOT_ALL_THREADS_DO_W_LDS_STORE)
            #if defined(LDS_STORE_W_BITPRIORITY_OUTCHAN)
                // OUTCHAN was designated/selected as the subject of other options like LOBITS/HIBITS, and here we know OUTCHAN == LOBITS,
                // thus we know LDS_STORE_W_BITPRIORITY_LOBITS
                #if !defined(LDS_STORE_W_BITPRIORITY_LOBITS)
                    #error "Bad config"
                #endif
            #elif defined(LDS_STORE_W_BITPRIORITY_INNER)
                // INNER was designated/selected as the subject of other options like LOBITS/HIBITS, and here we know OUTCHAN == LOBITS,
                // thus we know LDS_STORE_W_BITPRIORITY_HIBITS
                #if !defined(LDS_STORE_W_BITPRIORITY_HIBITS)
                    #error "Bad config"
                #endif
            #endif

            // This works for all of the above (outchan or inner priority):
            #define IF_THREAD_DOING_W_LDS_STORE(ti, gtid) if ((ti) < NB_THREADIDS_FOR_INNER_AND_OUTCHAN_W_LDS_STORE)
        #else
            // All threads participate: IF_THREAD_DOING_W_LDS_STORE already defined
        #endif

        //#warning "OLD NON NB_THREADIDS_FROM_NUMTHREADS_XY for LDS_STORE_W"
        #define SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) ((ti) & (NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE-1))
        #define SELECT_FROMTHREADIDS_AS_INNER_FOR_W_LDS_STORE(ti, gtid) ((ti) >> LOG2_NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE)
        #define ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE_IDX(ti, gtid) (ti)

    #endif

#else
    #error "LDS_STORE_W: by this point should have LDS_STORE_W_OUTCHAN_LOBITS or LDS_STORE_W_OUTCHAN_HIBITS"
#endif
// ...LDS_STORE_W
//------------------------------------------------------------------------
//------------------------------------------------------------------------
// LDS_STORE_X:
//
//      _MAXBITS or _NB_THREADIDS mode
//      also uses _BITPRIORITY_SPATIALS or _BITPRIORITY_INNER
//      to finalize LDS_STORE_X_USES_THIS_NB_THREADIDS
//
//      Later we use again _BITPRIORITY_SPATIALS or _BITPRIORITY_INNER
//      to set all other X_LDS_STORE required macros (access masks, NB threads, IF_THREAD_DOING, etc.)

    #if !defined(LDS_STORE_X_USES_THIS_NB_THREADIDS)
        //
        // nb bits: using a maximum for a specific purpose: either spatials or inner slice
        //
        #if !defined(LDS_STORE_X_USES_MAXBITS)
        #error "LDS_STORE_X mode: need to specify _MAXBITS or _THIS_NB_THREADIDS xx where xx is the nb of bits required"
        #endif

        #if defined(LDS_STORE_X_BITPRIORITY_SPATIALS)
            // 
            // Use maxbits for spatials
            //
            #define LDS_STORE_X_USES_THIS_NB_THREADIDS NB_SPATIALS_PER_TG
        #else
            // else for inner product chunk
            #if !defined(LDS_STORE_X_BITPRIORITY_INNER)
            #error "LDS_STORE_X_BITPRIORITY mode: need to specify _SPATIALS or _INNER"
            #endif
            #define LDS_STORE_X_USES_THIS_NB_THREADIDS CACHE_DEPTH
        #endif
    #else
        // nb bits: using a specific number
        #if !defined(LDS_STORE_X_USES_THIS_NB_THREADIDS)
        #error "LDS_STORE_X mode: need to specify _MAXBITS or _THIS_NB_THREADIDS xx where xx is the nb of bits required"
        #endif
    #endif // if !defined(LDS_STORE_X_USES_THIS_NB_THREADIDS)

    //
    // LDS_STORE_X_USES_THIS_NB_THREADIDS should now be known,
    // need to sanitize it vs actual available 
    //
    #if defined(LDS_STORE_X_BITPRIORITY_SPATIALS)
        #if LDS_STORE_X_USES_THIS_NB_THREADIDS > NB_SPATIALS_PER_TG
            #undef LDS_STORE_X_USES_THIS_NB_THREADIDS
            #define LDS_STORE_X_USES_THIS_NB_THREADIDS NB_SPATIALS_PER_TG
        #endif
    #else
        // else for inner product chunk
        #if LDS_STORE_X_USES_THIS_NB_THREADIDS > CACHE_DEPTH
            #undef LDS_STORE_X_USES_THIS_NB_THREADIDS
            #define LDS_STORE_X_USES_THIS_NB_THREADIDS CACHE_DEPTH
        #endif
    #endif

    #if LDS_STORE_X_USES_THIS_NB_THREADIDS > NB_THREADS_PER_TG
        #warning "CACHE_DEPTH or NB_SPATIALS_PER_TG is larger than total NB_THREADS_PER_TG available"
        #undef LDS_STORE_X_USES_THIS_NB_THREADIDS
        #define LDS_STORE_X_USES_THIS_NB_THREADIDS NB_THREADS_PER_TG
    #endif


//
// Synthesize these intermediate defines:
//
// LDS_STORE_X_HIBITS_ARE_THREADIDS_Y
// LDS_STORE_X_SPATIALS_LOBITS or LDS_STORE_X_SPATIALS_HIBITS

    //
    // From num of threadids and priority, check if later masks are just the x or y thread group id:
    //
    #if !defined(LDS_STORE_X_BITPRIORITY_HIBITS) && !defined(LDS_STORE_X_BITPRIORITY_LOBITS)
        #error "LDS_STORE_X_BITPRIORITY mode: need to specify _HIBITS or LOBITS"
    #endif

    // ONLY if NB_THREADIDS_FROM_NUMTHREADS_XY
    #if defined(NB_THREADIDS_FROM_NUMTHREADS_XY)
        #if ( (defined(LDS_STORE_X_BITPRIORITY_HIBITS) && (LDS_STORE_X_USES_THIS_NB_THREADIDS == NUMTHREADS_Y)) \
            || (defined(LDS_STORE_X_BITPRIORITY_LOBITS) && (LDS_STORE_X_USES_THIS_NB_THREADIDS == NUMTHREADS_X)) )
            #define LDS_STORE_X_HIBITS_ARE_THREADIDS_Y
            // would be the same as a define like LDS_STORE_X_LOBITS_ARE_THREADIDS_X
        #endif
    #endif
    // ...and those high/low bits are for LDS_STORE_X_BITPRIORITY_SPATIALS or LDS_STORE_X_BITPRIORITY_INNER

    //
    // Combine threadids high/low priority with spatials/inner
    // (we needed those separate to properly select the nb of threadids given various limits like total NB_THREADS_PER_TG, CACHE_DEPTH,
    // but for selection mask definitions, we can combine those:
    // 
    #if !defined(LDS_STORE_X_BITPRIORITY_SPATIALS) && !defined(LDS_STORE_X_BITPRIORITY_INNER)
    #error "LDS_STORE_X_BITPRIORITY mode: need to specify _SPATIALS or _INNER"
    #endif
    #if ( (defined(LDS_STORE_X_BITPRIORITY_SPATIALS) && defined(LDS_STORE_X_BITPRIORITY_HIBITS))  \
        || (defined(LDS_STORE_X_BITPRIORITY_INNER) && defined(LDS_STORE_X_BITPRIORITY_LOBITS)) )
        #define LDS_STORE_X_SPATIALS_HIBITS
    #elif ( (defined(LDS_STORE_X_BITPRIORITY_SPATIALS) && defined(LDS_STORE_X_BITPRIORITY_LOBITS))  \
          || (defined(LDS_STORE_X_BITPRIORITY_INNER) && defined(LDS_STORE_X_BITPRIORITY_HIBITS)) )
        #define LDS_STORE_X_SPATIALS_LOBITS
    #endif



// Defined those main needed values and macros needed:
//
// NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE
// NB_THREADIDS_FOR_INNER_X_LDS_STORE
//
// SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE
// SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE
//
// NB_EXPLICIT_INNER_VAL_NEEDED_X_LDS_STORE
// NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE
//
// NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE
//
// IF_THREAD_DOING_X_LDS_STORE
// ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE_IDX
//


        #if defined(LDS_STORE_X_BITPRIORITY_SPATIALS)
            // Note: in that case, LDS_STORE_SPATIALS_USES_THIS_NB_THREADIDS has already been clamped by NB_SPATIALS_PER_TG)
            #define NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE LDS_STORE_X_USES_THIS_NB_THREADIDS

            // assert: no modulo in preproc macro ops :/
            #if !( ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 1) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 2) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 4) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 8) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 16) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 32) )
            #error NB_THREADS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE should be a power of 2
            #endif

            #define NB_THREADIDS_FOR_INNER_X_LDS_STORE (NB_THREADS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE)

            // Sanitize to the max required: we've assigned threadids for spatials,
            // either there's no ids (ie just 1 - the same across all TG) left for the inner chunk,
            // or there's more than 1,
            // check if what's left for inner chunk is larger than CACHE_DEPTH and clamp to that,
            // also indicate that condition for later (see IF_THREAD_DOING_X_LDS_STORE)
            //
            #if (NB_THREADIDS_FOR_INNER_X_LDS_STORE > CACHE_DEPTH)
                #warning "Priority to spatials and we're left with NB_THREADIDS_FOR_INNER_X_LDS_STORE > CACHE_DEPTH, clamping"
                #undef NB_THREADIDS_FOR_INNER_X_LDS_STORE
                #define NB_THREADIDS_FOR_INNER_X_LDS_STORE CACHE_DEPTH

                #define NB_THREADIDS_LEFT_FOR_INNER_X_LDS_STORE_WASGT_CACHE_DEPTH
            #endif

        #else
            // else for inner product chunk
            // Note: in that case, LDS_STORE_SPATIALS_USES_THIS_NB_THREADIDS has already been clamped by CACHE_DEPTH)
            #if !defined(LDS_STORE_X_BITPRIORITY_INNER)
            #error "LDS_STORE_X_BITPRIORITY mode: need to specify _SPATIALS or _INNER"
            #endif
            #define NB_THREADIDS_FOR_INNER_X_LDS_STORE LDS_STORE_X_USES_THIS_NB_THREADIDS

            // assert: no modulo in preproc macro ops :/ 
            #if !( ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_INNER_X_LDS_STORE) == 1) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_INNER_X_LDS_STORE) == 2) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_INNER_X_LDS_STORE) == 4) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_INNER_X_LDS_STORE) == 8) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_INNER_X_LDS_STORE) == 16) \
                || ((NB_THREADS_PER_TG/NB_THREADIDS_FOR_INNER_X_LDS_STORE) == 32) )
            #error NB_THREADS_PER_TG/NB_THREADIDS_FOR_INNER_X_LDS_STORE should be a power of 2
            #endif

            #define NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE (NB_THREADS_PER_TG / NB_THREADIDS_FOR_INNER_X_LDS_STORE)

            // Sanitize to the max required: we've assigned threadids for inner,
            // either there's no ids (ie just 1 - the same across all TG) left for the spatialss,
            // or there's more than 1,
            // check if what's left for spatials is larger than NB_SPATIALS_PER_TG and clamp to that,
            // also indicate that condition for later (see IF_THREAD_DOING_X_LDS_STORE)
            //
            #if (NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE > NB_SPATIALS_PER_TG)
                #warning "Priority to spatials and we're left with NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE > NB_SPATIALS_PER_TG, clamping"
                #undef NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE
                #define NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE NB_SPATIALS_PER_TG

                #define NB_THREADIDS_LEFT_FOR_SPATIALS_X_LDS_STORE_WASGT_NB_SPATIALS_PER_TG
            #endif

        #endif

// New synthesized intermediate:
//
// NB_THREADIDS_LEFT_FOR_INNER_X_LDS_STORE_WASGT_CACHE_DEPTH
// NB_THREADIDS_LEFT_FOR_SPATIALS_X_LDS_STORE_WASGT_NB_SPATIALS_PER_TG

// Explicit values (regs) needed for SPATIALS:
#if !( ((NB_SPATIALS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 1) \
    || ((NB_SPATIALS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 2) \
    || ((NB_SPATIALS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 4) \
    || ((NB_SPATIALS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 8) \
    || ((NB_SPATIALS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 16) \
    || ((NB_SPATIALS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 32) )
#error NB_SPATIALS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE should be a power of 2
#endif

#define LOG2_NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE uint(log2(NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE))

#define NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE (NB_SPATIALS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE)

// Similarly for inner:
// Explicit values (regs) needed for INNER:
//
#if !( ((CACHE_DEPTH/NB_THREADIDS_FOR_INNER_X_LDS_STORE) == 1) \
    || ((CACHE_DEPTH/NB_THREADIDS_FOR_INNER_X_LDS_STORE) == 2) \
    || ((CACHE_DEPTH/NB_THREADIDS_FOR_INNER_X_LDS_STORE) == 4) \
    || ((CACHE_DEPTH/NB_THREADIDS_FOR_INNER_X_LDS_STORE) == 8) \
    || ((CACHE_DEPTH/NB_THREADIDS_FOR_INNER_X_LDS_STORE) == 16) \
    || ((CACHE_DEPTH/NB_THREADIDS_FOR_INNER_X_LDS_STORE) == 32) )
#error CACHE_DEPTH/NB_THREADIDS_FOR_INNER_X_LDS_STORE should be a power of 2
#endif

#define LOG2_NB_THREADIDS_FOR_INNER_X_LDS_STORE uint(log2(NB_THREADIDS_FOR_INNER_X_LDS_STORE))

#define NB_EXPLICIT_INNER_VAL_NEEDED_X_LDS_STORE (CACHE_DEPTH / NB_THREADIDS_FOR_INNER_X_LDS_STORE)

//
// Threadids selection masks
//
// Uses
//      LDS_STORE_X_BITPRIORITY_SPATIALS
//      LDS_STORE_X_BITPRIORITY_HIBITS
//      LDS_STORE_X_USES_THIS_NB_THREADIDS
//
// Uses also these new synthesized intermediate defines:
//
//      LDS_STORE_X_HIBITS_ARE_THREADIDS_Y
//      LDS_STORE_X_SPATIALS_LOBITS or LDS_STORE_X_SPATIALS_HIBITS
//
//      NB_THREADIDS_LEFT_FOR_INNER_X_LDS_STORE_WASGT_CACHE_DEPTH
//      NB_THREADIDS_LEFT_FOR_SPATIALS_X_LDS_STORE_WASGT_NB_SPATIALS_PER_TG


#if (NB_THREADIDS_LEFT_FOR_INNER_X_LDS_STORE_WASGT_CACHE_DEPTH || NB_THREADIDS_LEFT_FOR_SPATIALS_X_LDS_STORE_WASGT_NB_SPATIALS_PER_TG)

    #define NOT_ALL_THREADS_DO_X_LDS_STORE

    #if (NB_THREADIDS_FOR_INNER_X_LDS_STORE < CACHE_DEPTH) || (NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE < NB_SPATIALS_PER_TG)
        #error "NOT_ALL_THREADS_DO_X_LDS_STORE and NB_THREADIDS_ assigned for inner chunk or spatials aren't maximized!"
    #endif

    #define NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE (NB_THREADIDS_FOR_INNER_X_LDS_STORE * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE)

#else

    #define NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE (NB_THREADS_PER_TG)

    #if (NB_THREADIDS_FOR_INNER_X_LDS_STORE * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) != NB_THREADS_PER_TG
        #error "(NB_THREADIDS_FOR_INNER_X_LDS_STORE * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) should be == NB_THREADS_PER_TG"
    #endif

    #define IF_THREAD_DOING_X_LDS_STORE(ti, gtid) 
    //...passthrough as all threads participate

#endif


// Threadids selection masks (continued), divided into 
// LDS_STORE_X_SPATIALS_HIBITS and
// LDS_STORE_X_SPATIALS_LOBITS sections.

#if defined(LDS_STORE_X_SPATIALS_HIBITS)
    #if defined(LDS_STORE_X_HIBITS_ARE_THREADIDS_Y)
        #warning "should have (NB_THREADIDS_FROM_NUMTHREADS_XY)"

        #define SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid) (gtid.y)
        #define SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) (gtid.x)

        #if defined(NOT_ALL_THREADS_DO_X_LDS_STORE)
            #if defined(LDS_STORE_X_BITPRIORITY_SPATIALS) 
                // SPATIALS was designated/selected as the subject of other options like LOBITS/HIBITS, and here we know SPATIALS == HIBITS,
                // thus we know LDS_STORE_X_BITPRIORITY_HIBITS
                #if !defined(LDS_STORE_X_BITPRIORITY_HIBITS)
                    #error "Bad config"
                #endif
                // We're also in the context of LDS_STORE_X_HIBITS_ARE_THREADIDS_Y,
                // so unused threadids are in the middle, ie high bits of threadids.x, bad`
                #warning "Suboptimal config detected, see code..."
                #define IF_THREAD_DOING_X_LDS_STORE(ti, gtid) if (gtid.x < CACHE_DEPTH)
                // Since LDS_STORE_X_HIBITS_ARE_THREADIDS_Y should be used when defined(NB_THREADIDS_FROM_NUMTHREADS_XY),
                // this shouldn't be needed:
                //#define ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE_IDX(ti, gtid) ((gtid.y << LOG2_CACHE_DEPTH) | (gtid.x & (CACHE_DEPTH-1)))

            #elif defined(LDS_STORE_X_BITPRIORITY_INNER)
                // INNER was designated/selected as the subject of other options like LOBITS/HIBITS, and here we know SPATIALS == HIBITS,
                // thus we know LDS_STORE_X_BITPRIORITY_LOBITS
                #if !defined(LDS_STORE_X_BITPRIORITY_LOBITS)
                    #error "Bad config"
                #endif
                // We're also in the context of LDS_STORE_X_HIBITS_ARE_THREADIDS_Y,
                // so unused threadids are for spatials and high bits of threadids.y, we're ok.
                #define IF_THREAD_DOING_X_LDS_STORE(ti, gtid) if (gtid.y < NB_SPATIALS_PER_TG)
                // this would work too: #define IF_THREAD_DOING_X_LDS_STORE(ti, gtid) if ((ti) < NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE)
                //#define ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE_IDX(ti, gtid) (ti)
                // ...we forgo a mask like ((ti) & (NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE-1)) since it should only be used
                // in a block of code guarded by IF_THREAD_DOING_X_LDS_STORE()
            #else
                #error "_SPATIALS or _INNER expected"
            #endif
        #else
            // All threads participate: IF_THREAD_DOING_X_LDS_STORE already defined
            // We shouldnt need this:
            //#define ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE_IDX(ti, gtid) (ti)
        #endif
    #else
        // We don't use gtid.x or .y here

        #if defined(NOT_ALL_THREADS_DO_X_LDS_STORE)
            #if defined(LDS_STORE_X_BITPRIORITY_SPATIALS)
                // SPATIALS was designated/selected as the subject of other options like LOBITS/HIBITS, and here we know SPATIALS == HIBITS,
                // thus we know LDS_STORE_X_BITPRIORITY_HIBITS
                #if !defined(LDS_STORE_X_BITPRIORITY_HIBITS)
                    #error "Bad config"
                #endif
            #elif defined(LDS_STORE_X_BITPRIORITY_INNER)
                // INNER was designated/selected as the subject of other options like LOBITS/HIBITS, and here we know SPATIALS == HIBITS,
                // thus we know LDS_STORE_X_BITPRIORITY_LOBITS
                #if !defined(LDS_STORE_X_BITPRIORITY_LOBITS)
                    #error "Bad config"
                #endif
            #endif

            // This works for all of the above (spatials or inner priority):
            #define IF_THREAD_DOING_X_LDS_STORE(ti, gtid) if ((ti) < NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE)
        #else
            // All threads participate: IF_THREAD_DOING_X_LDS_STORE already defined
        #endif

        #define SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid) ((ti) >> LOG2_NB_THREADIDS_FOR_INNER_X_LDS_STORE)
        #define SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid)  ((ti) & (NB_THREADIDS_FOR_INNER_X_LDS_STORE-1))
        #define ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE_IDX(ti, gtid) (ti)

    #endif

#elif defined(LDS_STORE_X_SPATIALS_LOBITS)
    #if defined(LDS_STORE_X_HIBITS_ARE_THREADIDS_Y)
        #if !defined(DISABLE_SHADER_WARNING)
            #warning "should have (NB_THREADIDS_FROM_NUMTHREADS_XY) - NOTE: path like previous macros"
        #endif

        #define SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid) (gtid.x) // spatials are low bits here!
        #define SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) (gtid.y)

        #if defined(NOT_ALL_THREADS_DO_X_LDS_STORE)
            #if defined(LDS_STORE_X_BITPRIORITY_SPATIALS) 
                // SPATIALS was designated/selected as the subject of other options like LOBITS/HIBITS, and here we know SPATIALS == LOBITS,
                // thus we know LDS_STORE_X_BITPRIORITY_LOBITS
                #if !defined(LDS_STORE_X_BITPRIORITY_LOBITS)
                    #error "Bad config"
                #endif
                // We're also in the context of LDS_STORE_X_HIBITS_ARE_THREADIDS_Y,
                // so unused threadids are in high bits, not used for INNER selection, ie high bits of threadids.y

                #define IF_THREAD_DOING_X_LDS_STORE(ti, gtid) if (gtid.y < CACHE_DEPTH)
                // this would work too: #define IF_THREAD_DOING_X_LDS_STORE(ti, gtid) if ((ti) < NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE)

                // Since LDS_STORE_X_HIBITS_ARE_THREADIDS_Y should be used when defined(NB_THREADIDS_FROM_NUMTHREADS_XY),
                // this shouldn't be needed:
                //#define ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE_IDX(ti, gtid) (ti)
                // ...we forgo a mask like ((ti) & (NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE-1)) since it should only be used
                // in a block of code guarded by IF_THREAD_DOING_X_LDS_STORE()

            #elif defined(LDS_STORE_X_BITPRIORITY_INNER)
                // INNER was designated/selected as the subject of other options like LOBITS/HIBITS, and here we know SPATIALS == LOBITS,
                // thus we know LDS_STORE_X_BITPRIORITY_HIBITS
                #if !defined(LDS_STORE_X_BITPRIORITY_HIBITS)
                    #error "Bad config"
                #endif
                // We're also in the context of LDS_STORE_X_HIBITS_ARE_THREADIDS_Y,
                // so unused threadids are in the middle, the ones not used for SPATIALS selection, ie high bits of threadids.x, bad`
                #warning "Suboptimal config detected, see code..."
                #define IF_THREAD_DOING_X_LDS_STORE(ti, gtid) if (gtid.x < NB_SPATIALS_PER_TG)
                // Since LDS_STORE_X_HIBITS_ARE_THREADIDS_Y should be used when defined(NB_THREADIDS_FROM_NUMTHREADS_XY),
                // this shouldn't be needed:
                //#define ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE_IDX(ti, gtid) ((gtid.y << LOG2_NB_SPATIALS_PER_TG) | (gtid.x & (NB_SPATIALS_PER_TG-1)))
            #else
                #error "_SPATIALS or _INNER expected"
            #endif
        #else
            // All threads participate: IF_THREAD_DOING_X_LDS_STORE already defined
            // We shouldnt need this:
            //#define ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE_IDX(ti, gtid) (ti)
        #endif
    #else
        // We don't use gtid.x or .y here

        #if defined(NOT_ALL_THREADS_DO_X_LDS_STORE)
            #if defined(LDS_STORE_X_BITPRIORITY_SPATIALS)
                // SPATIALS was designated/selected as the subject of other options like LOBITS/HIBITS, and here we know SPATIALS == LOBITS,
                // thus we know LDS_STORE_X_BITPRIORITY_LOBITS
                #if !defined(LDS_STORE_X_BITPRIORITY_LOBITS)
                    #error "Bad config"
                #endif
            #elif defined(LDS_STORE_X_BITPRIORITY_INNER)
                // INNER was designated/selected as the subject of other options like LOBITS/HIBITS, and here we know SPATIALS == LOBITS,
                // thus we know LDS_STORE_X_BITPRIORITY_HIBITS
                #if !defined(LDS_STORE_X_BITPRIORITY_HIBITS)
                    #error "Bad config"
                #endif
            #endif

            // This works for all of the above (spatials or inner priority):
            #define IF_THREAD_DOING_X_LDS_STORE(ti, gtid) if ((ti) < NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE)
        #else
            // All threads participate: IF_THREAD_DOING_X_LDS_STORE already defined
        #endif

        //#warning "OLD NON NB_THREADIDS_FROM_NUMTHREADS_XY for LDS_STORE_X"
        #define SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid) ((ti) & (NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE-1))
        #define SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) ((ti) >> LOG2_NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE)
        #define ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE_IDX(ti, gtid) (ti)

    #endif

#else
    #error "LDS_STORE_X: by this point should have LDS_STORE_X_SPATIALS_LOBITS or LDS_STORE_X_SPATIALS_HIBITS"
#endif
// ...LDS_STORE_X
//------------------------------------------------------------------------






// ----------------------------------------------------------------------------------------------------------------
// LDS (SM/CU shared memory) declaration and access config:
// ----------------------------------------------------------------------------------------------------------------

#define LDS_W_STRICT_SIZE (CACHE_DEPTH * NB_OUTCHAN_PER_TG)
#define LDS_X_STRICT_SIZE (CACHE_DEPTH * NB_SPATIALS_PER_TG)
#define LDS_W_FINAL_SIZE (LDS_W_STRICT_SIZE + (LDS_W_STRICT_SIZE >> LOG2_NB_BANKS))
#define LDS_X_FINAL_SIZE (LDS_X_STRICT_SIZE + (LDS_X_STRICT_SIZE >> LOG2_NB_BANKS))
#define LDS_BCAO(off) (off + (off >> LOG2_NB_BANKS)) // bank conflict avoidance offset



// See those defines for LDS org:

// #define CALCULATE_LDS_STORE_IDX_W
// #define CALCULATE_LDS_LOAD_IDX_W
// #define LDS_ACCESS_STORE_W
// #define LDS_ACCESS_LOAD_W

// #define CALCULATE_LDS_STORE_IDX_X
// #define CALCULATE_LDS_LOAD_IDX_X
// #define LDS_ACCESS_STORE_X
// #define LDS_ACCESS_LOAD_X


//
// groupshared LDS_W declaration 
//
#ifdef USE_ALTERNATE_LDS_ORG_W

    #define W_CACHE_EXTRA 1
    groupshared DATA_TYPE LDS_W[NB_OUTCHAN_PER_TG * (CACHE_DEPTH + W_CACHE_EXTRA)];
    // ITE = inner (kernel inputchannel and possibly flattened spatial) index, threadid for output indices, explicit output channel
    //      #define LDS_ACCESS_W_ITE(ik, ytid, eoc) LDS_W[(ik) * (NUMTHREADS_Y * ELEM_PER_THREAD_Y + 1) + (ytid) * ELEM_PER_THREAD_Y + (eoc)]
    //
    // each multiplier above is obviously the stride, and in the general config, ie not necessarily "NB_THREADIDS_FROM_NUMTHREADS_XY" config,
    // the equivalent of the above is:
    //
    //      eoc => explicit output channel ids, we have NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE of those,
    //      ytid => threadids reserved for output channel indexing, we have NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE of those
    //
    #define LDS_ACCESS_W_ITE(ik, ytid, eoc) LDS_W[(ik) * (NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE * NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE + W_CACHE_EXTRA) + (ytid) * NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE + (eoc)]


#else // #ifdef USE_ALTERNATE_LDS_ORG_W

    groupshared DATA_TYPE LDS_W[LDS_W_FINAL_SIZE];
    #define LDS_ACCESS_W_BCA(offset) LDS_W[LDS_BCAO(offset)]
    // slower?
    //#define LDS_ACCESS_W(offset) LDS_ACCESS_W_BCA(offset) // LDS_W[offset]
    #define LDS_ACCESS_W(offset) LDS_W[offset]

#endif // #ifdef USE_ALTERNATE_LDS_ORG_W


//
// Various "calculate index" macros available for LDS_W access
//
//
// REMEMBER to guard code with IF_THREAD_DOING_W_LDS_STORE(ti, gtid)
// when in the store to LDS phase
//
// LDS_STORE macros can use the xy threadids macros, 
// but the reload macros must use eg NUMTHREADS_SPATIALS not NUMTHREADS_X directly
//
#ifdef USE_ALTERNATE_LDS_ORG_W // these need W_CACHE_EXTRA

    // ---------------------------------------------------------------------------------------------
    // v1 : splits threadids, eg .y for OC in mid part, .x for inner kicsp slice, highest bits = eic
    //
    // fastest ? (this is used with NB_THREADIDS_FROM_NUMTHREADS_XY and is like LDS_ACCESS_W_ITE)
    //
    // Can only work if NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE == NUMTHREADS_SPATIALS
    // (ie same number of threads on store and reload)
    // unless the LOAD phase macro used is __v1_Repurpose
    #define CALCULATE_LDS_STORE_IDX_W__v1(ti, gtid, eoc, eic) \
        ( /*high part*/ ((SELECT_FROMTHREADIDS_AS_INNER_FOR_W_LDS_STORE(ti, gtid) + (NB_THREADIDS_FOR_INNER_W_LDS_STORE * (eic))) * (NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE * NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE + W_CACHE_EXTRA)) \
          /*mid part*/ + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) * NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE  \
            + (eoc) \
        )\

    //LDS_ACCESS_W_ITE(di, groupThreadID.y, eoc);
    #define CALCULATE_LDS_LOAD_IDX_W__v1(gtid, eoc, ik) \
        ( ((ik) * (NUMTHREADS_OUTCHAN * OUTCHAN_ELEM_PER_THREAD + W_CACHE_EXTRA)) \
            + SELECT_FINAL_OUT_OUTCHAN_THREADIDS(ti, gtid)/*(gtid.y)*/ * OUTCHAN_ELEM_PER_THREAD  \
            + (eoc) \
        )\

    //
    // These allow the same CALCULATE_LDS_STORE_IDX_W__v1 WITH a thread repurpose:
    //
        #if (NUMTHREADS_OUTCHAN > NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE)

            #define CALCULATE_LDS_LOAD_IDX_W__v1_Repurpose(gtid, eoc, ik) \
            (     ((ik) * (NUMTHREADS_OUTCHAN * OUTCHAN_ELEM_PER_THREAD + W_CACHE_EXTRA))                                                                   \
                + ((SELECT_FINAL_OUT_OUTCHAN_THREADIDS(ti, gtid) & (NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE-1)) * NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE)  \
                + ( ((eoc) << (LOG2_NUMTHREADS_OUTCHAN - LOG2_NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE))                  \
                     | (SELECT_FINAL_OUT_OUTCHAN_THREADIDS(ti, gtid) >> LOG2_NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) )  \
            )

        #elif (NUMTHREADS_OUTCHAN < NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE)

            #define CALCULATE_LDS_LOAD_IDX_W__v1_Repurpose(gtid, eoc, ik) \
                (     ((ik) * (NUMTHREADS_OUTCHAN * OUTCHAN_ELEM_PER_THREAD + W_CACHE_EXTRA))                                  \
                    + ( (  ( ((eoc) & ((NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE/NUMTHREADS_OUTCHAN) - 1)) * NUMTHREADS_OUTCHAN )  \
                         + SELECT_FINAL_OUT_OUTCHAN_THREADIDS(ti, gtid)    )                                                   \
                       * NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE                                                            \
                      )                                                                                                        \
                    + ((eoc) >> (LOG2_NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE - LOG2_NUMTHREADS_OUTCHAN))                         \
                )

        #else
            #define CALCULATE_LDS_LOAD_IDX_W__v1_Repurpose(gtid, eoc, ik) CALCULATE_LDS_LOAD_IDX_W__v1(gtid, eoc, ik)
        #endif


    // These work even if NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE != NUMTHREADS_SPATIALS
    #define CALCULATE_LDS_STORE_IDX_W__v1b(ti, gtid, eoc, eic) \
        ( /*high part*/ ((SELECT_FROMTHREADIDS_AS_INNER_FOR_W_LDS_STORE(ti, gtid) + (NB_THREADIDS_FOR_INNER_W_LDS_STORE * (eic))) * (NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE * NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE + W_CACHE_EXTRA)) \
          /*mid part*/ + (eoc) * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE  \
            + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) \
        )\

    #define CALCULATE_LDS_LOAD_IDX_W__v1b(gtid, eoc, ik) \
        ( ((ik) * (NUMTHREADS_OUTCHAN * OUTCHAN_ELEM_PER_THREAD + W_CACHE_EXTRA)) \
            + (eoc) * NUMTHREADS_OUTCHAN  \
            + SELECT_FINAL_OUT_OUTCHAN_THREADIDS(ti, gtid) \
        )\
    // ---------------------------------------------------------------------------------------------

#endif

    // ---------------------------------------------------------------------------------------------
    // v2 : splits threadids, eg .y for OC in mid part + highest bits eoc, .x for inner kicsp slice, lowest bits + next bits = eic
    //
    // can be used with NB_THREADIDS_FROM_NUMTHREADS_XY but seems slower even though should avoid bank conflicts
    //
    #define CALCULATE_LDS_STORE_IDX_W__v2(ti, gtid, eoc, eic) \
        ( /*high part*/ ((eoc) * CACHE_DEPTH * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) \
          /*mid part*/ + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) * CACHE_DEPTH /* groupThreadID.y * CACHE_DEPTH */ \
            + ((eic) * NB_THREADIDS_FOR_INNER_W_LDS_STORE) \
            + SELECT_FROMTHREADIDS_AS_INNER_FOR_W_LDS_STORE(ti, gtid) /* groupThreadID.x */ \
        )\

    //
    //uint index = eoc * CACHE_DEPTH * NUMTHREADS_Y + ty * CACHE_DEPTH + di;
    //
    // Note: we dont use the eg SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid)
    // macros here instead of ty (groupThreadID.y) and NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE instead of NUMTHREADS_Y etc. 
    // because whatever way we arrange the LDS cache, ty (groupThreadID.y) MUST select an output channel index base
    // when threads load for a final time here before doing calculations and final output.
    // ie it is required by the dispatch convention of having output channel dispatch (and ceil(oc nb / ELEM_PER_THREAD_Y)) flattened
    // in gridY and similarly for all output spatial dimensions in gridX.
    #define CALCULATE_LDS_LOAD_IDX_W__v2(gtid, eoc, ik) \
        ( ((eoc) * CACHE_DEPTH * NUMTHREADS_OUTCHAN) \
            + SELECT_FINAL_OUT_OUTCHAN_THREADIDS(ti, gtid) * CACHE_DEPTH /* groupThreadID.y * CACHE_DEPTH */ \
            + (ik) \
        )\
    // ---------------------------------------------------------------------------------------------


    // ---------------------------------------------------------------------------------------------
    // v3 : use combined threadids for OC (low threads) and if there are some left, use them for inner slice
    //
    // can be used with NON NB_THREADIDS_FROM_NUMTHREADS_XY config, wip, as slower
    //
    // (Note that eoc is 0 if only 1 threadid is left available for part of the inner indexing,
    // so mid part disappears at compile-time)
    #define CALCULATE_LDS_STORE_IDX_W__v3(ti, gtid, eoc, eic) \
        ( /*high part*/ ((eic) * NB_THREADIDS_FOR_INNER_AND_OUTCHAN_W_LDS_STORE * NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE) \
          /*mid part*/ + ((eoc) * NB_THREADIDS_FOR_INNER_AND_OUTCHAN_W_LDS_STORE) \
            + ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE_IDX(ti, gtid) \
        )\

    //
    // index = ((di << LOG2_NB_OUTCHAN_PER_TG) | (eoc * NUMTHREADS_Y | ty));
    //
    #define CALCULATE_LDS_LOAD_IDX_W__v3(gtid, eoc, ik) \
        /* ( (((ik) << LOG2_NB_OUTCHAN_PER_TG) | ((eoc) * NUMTHREADS_Y | gtid.y)) */ \
        ( (((ik) << LOG2_NB_OUTCHAN_PER_TG) | ((eoc) * NUMTHREADS_OUTCHAN | SELECT_FINAL_OUT_OUTCHAN_THREADIDS(ti, gtid)))      \
        )\
    // ---------------------------------------------------------------------------------------------

    // ---------------------------------------------------------------------------------------------
    // v4 : use combined threadids for inner product chunk (low threads) 
    //      and if there are some left, use them for outchan
    //
    // Uses USE_ALTERNATE_LDS_ORG_W
    //
    // (Note that eoc is 0 if only 1 threadid is left available for part of the inner indexing,
    // so mid part disappears at compile-time)
    #ifndef W_CACHE_EXTRA // in case used without USE_ALTERNATE_LDS_ORG_W
        #define W_CACHE_EXTRA 0
    #endif

    #define CALCULATE_LDS_STORE_IDX_W__v4(ti, gtid, eoc, eic) \
        ( /*high part*/ ((SELECT_FROMTHREADIDS_AS_INNER_FOR_W_LDS_STORE(ti, gtid) + (NB_THREADIDS_FOR_INNER_W_LDS_STORE * (eic))) * (NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE * NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE + W_CACHE_EXTRA)) \
          /*mid part*/ + (eoc) * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE  \
            + (SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid)) \
        )\

    #define CALCULATE_LDS_LOAD_IDX_W__v4(gtid, eoc, ik) \
        ( ((ik) * (NUMTHREADS_OUTCHAN * OUTCHAN_ELEM_PER_THREAD + W_CACHE_EXTRA)) \
            + (eoc) * NUMTHREADS_OUTCHAN  \
            + SELECT_FINAL_OUT_OUTCHAN_THREADIDS(ti, gtid) \
        )\
    // ---------------------------------------------------------------------------------------------


//
// groupshared LDS_X declaration 
//
#if !defined(NON_UNIFORM_CONVGROUP_PER_OC)
    //
    // If defined(NON_UNIFORM_CONVGROUP_PER_OC), for now we don't cache for multiple different groups
    // as they can't uniformly be broadcasted the same weights.
    //

    #ifdef USE_ALTERNATE_LDS_ORG_X

        #define X_CACHE_EXTRA 0
        groupshared DATA_TYPE LDS_X[NB_SPATIALS_PER_TG * (CACHE_DEPTH + X_CACHE_EXTRA)];
        // ITE = inner (kernel inputchannel and possibly flattened spatial) index, threadid for output spatial indices, explicit spatial indices
        //
        //      #define LDS_ACCESS_X_ITE(ik, xtid, esp) LDS_X[(ik) * (NUMTHREADS_X * ELEM_PER_THREAD_X) + (xtid) * ELEM_PER_THREAD_X + (esp)]
        //
        // each multiplier above is obviously the stride, and in the general config, ie not necessarily "NB_THREADIDS_FROM_NUMTHREADS_XY" config,
        // the equivalent of the above is:
        //
        //      esp => explicit spatial ids, we have NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE of those,
        //      xtid => threadids reserved for spatial indexing, we have NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE of those
        //
        #define LDS_ACCESS_X_ITE(ik, xtid, esp) LDS_X[(ik) * (NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE + X_CACHE_EXTRA) + (xtid) * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE + (esp)]

    #else // #ifdef USE_ALTERNATE_LDS_ORG_X

        groupshared DATA_TYPE LDS_X[LDS_X_FINAL_SIZE];
        #define LDS_ACCESS_X_BCA(offset) LDS_X[LDS_BCAO(offset)]
        // slower?
        // #define LDS_ACCESS_X(offset) LDS_ACCESS_X_BCA(offset) // LDS_X[offset]
        #define LDS_ACCESS_X(offset) LDS_X[offset]

    #endif // #ifdef USE_ALTERNATE_LDS_ORG_X
#endif // #if !defined(NON_UNIFORM_CONVGROUP_PER_OC)


//
// Various "calculate index" macros available for LDS_X access
//
// REMEMBER to guard code with IF_THREAD_DOING_W_LDS_STORE(ti, gtid)
// when in the store to LDS phase
//
#ifdef USE_ALTERNATE_LDS_ORG_X // these need X_CACHE_EXTRA

    // ---------------------------------------------------------------------------------------------
    // v1 : splits threadids, eg .x for spatial in mid part, high bits .y for inner kicsp slice, highest bits = eic
    //
    // fastest ? (this is used with NB_THREADIDS_FROM_NUMTHREADS_XY and is like LDS_ACCESS_X_ITE)
    //
    // Can only work if NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE == NUMTHREADS_SPATIALS
    // (ie same number of threads on store and reload)
    // unless the LOAD phase macro used is __v1_Repurpose
    #define CALCULATE_LDS_STORE_IDX_X__v1(ti, gtid, esp, eic) \
        ( /*high part*/ ((SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) + (NB_THREADIDS_FOR_INNER_X_LDS_STORE * (eic))) * (NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE + X_CACHE_EXTRA)) \
          /*mid part*/ + SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid) * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE  \
            + (esp) \
        )\

    //#define LDS_ACCESS_X_ITE(ik, xtid, esp)
    #define CALCULATE_LDS_LOAD_IDX_X__v1(gtid, esp, ik) \
        ( ((ik) * (NUMTHREADS_SPATIALS * SPATIALS_ELEM_PER_THREAD + X_CACHE_EXTRA)) \
            + SELECT_FINAL_OUT_SPATIALS_THREADIDS(ti, gtid) /*(gtid.x)*/ * SPATIALS_ELEM_PER_THREAD  \
            + (esp) \
        )\

    //
    // These allow the same CALCULATE_LDS_STORE_IDX_X__v1 WITH a thread repurpose:
    // 

        #if (NUMTHREADS_SPATIALS > NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE)

            #define CALCULATE_LDS_LOAD_IDX_X__v1_Repurpose(gtid, esp, ik) \
            (     ((ik) * (NUMTHREADS_SPATIALS * SPATIALS_ELEM_PER_THREAD + X_CACHE_EXTRA))                                                                    \
                + ((SELECT_FINAL_OUT_SPATIALS_THREADIDS(ti, gtid) & (NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE-1)) * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE)  \
                + ( ((esp) << (LOG2_NUMTHREADS_SPATIALS - LOG2_NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE))                  \
                     | (SELECT_FINAL_OUT_SPATIALS_THREADIDS(ti, gtid) >> LOG2_NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) )  \
            )

        #elif (NUMTHREADS_SPATIALS < NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE)

            #define CALCULATE_LDS_LOAD_IDX_X__v1_Repurpose(gtid, esp, ik) \
                (     ((ik) * (NUMTHREADS_SPATIALS * SPATIALS_ELEM_PER_THREAD + X_CACHE_EXTRA))                                   \
                    + ( (  ( ((esp) & ((NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE/NUMTHREADS_SPATIALS) - 1)) * NUMTHREADS_SPATIALS )  \
                         + SELECT_FINAL_OUT_SPATIALS_THREADIDS(ti, gtid)    )                                                     \
                       * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE                                                              \
                      )                                                                                                           \
                    + ((esp) >> (LOG2_NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE - LOG2_NUMTHREADS_SPATIALS))                          \
                )

        #else
            #define CALCULATE_LDS_LOAD_IDX_X__v1_Repurpose(gtid, esp, ik) CALCULATE_LDS_LOAD_IDX_X__v1(gtid, esp, ik)
        #endif

    // These work even if NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE != NUMTHREADS_SPATIALS
    #define CALCULATE_LDS_STORE_IDX_X__v1b(ti, gtid, esp, eic) \
        ( /*high part*/ ((SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) + (NB_THREADIDS_FOR_INNER_X_LDS_STORE * (eic))) * (NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE + X_CACHE_EXTRA)) \
          /*mid part*/ + (esp) * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE \
            + SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid) \
        )\

    #define CALCULATE_LDS_LOAD_IDX_X__v1b(gtid, esp, ik) \
        ( ((ik) * (NUMTHREADS_SPATIALS * SPATIALS_ELEM_PER_THREAD + X_CACHE_EXTRA)) \
            + (esp) * NUMTHREADS_SPATIALS  \
            + SELECT_FINAL_OUT_SPATIALS_THREADIDS(ti, gtid) \
        )\
    // ---------------------------------------------------------------------------------------------

#endif

    // ---------------------------------------------------------------------------------------------
    // v2 : splits threadids, eg .x for spatial in low part + next bits esp, .y for inner kicsp slice + highest bits = eic
    //
    // can be used with NB_THREADIDS_FROM_NUMTHREADS_XY but seems slower even though should avoid bank conflicts
    //
    #define CALCULATE_LDS_STORE_IDX_X__v2(ti, gtid, esp, eic) \
        ( /*high part*/ (eic * NB_THREADIDS_FOR_INNER_X_LDS_STORE * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) \
          /*mid part*/ + SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE /* groupThreadID.y * ... */ \
            + ((esp) * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) \
            + SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid) /* groupThreadID.x */ \
        )\

    //
    // srcX[esp] = LDS_ACCESS_X( ((di << LOG2_NB_SPATIALS_PER_TG) | (esp * NUMTHREADS_X | tx)) );
    //
    #define CALCULATE_LDS_LOAD_IDX_X__v2(gtid, esp, ik) \
        ( ((ik) << LOG2_NB_SPATIALS_PER_TG)   \
            /* | ((esp) * NUMTHREADS_X | gtid.x) */ \
               | ((esp) * NUMTHREADS_SPATIALS | SELECT_FINAL_OUT_SPATIALS_THREADIDS(ti, gtid))   \
        )\
    // ---------------------------------------------------------------------------------------------


    // ---------------------------------------------------------------------------------------------
    // v3 : use combined threadids for input spatial start indices (low threads) and if there are some left, use them for inner slice
    //
    // can be used with NON NB_THREADIDS_FROM_NUMTHREADS_XY config, wip, as slower
    //
    // (Note that esp is 0 if only 1 threadid is left available for part of the inner indexing,
    // so mid part disappears at compile-time)
    #define CALCULATE_LDS_STORE_IDX_X__v3(ti, gtid, esp, eic) \
        ( /*high part*/ ((eic) * NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE) \
          /*mid part*/ + ((esp) * NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE) \
            + ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE_IDX(ti, gtid) \
        )\

    //
    // srcX[esp] = LDS_ACCESS_X( ((di << LOG2_NB_SPATIALS_PER_TG) | (esp * NUMTHREADS_X | tx)) );
    //
    // This is actually the same as CALCULATE_LDS_LOAD_IDX_X__v2
    #define CALCULATE_LDS_LOAD_IDX_X__v3(gtid, eoc, ik) CALCULATE_LDS_LOAD_IDX_X__v2(gtid, esp, ik)
    // ---------------------------------------------------------------------------------------------

    // ---------------------------------------------------------------------------------------------
    // v4 : use combined threadids for inner product chunk (low threads) 
    //      and if there are some left, use them for outchan
    //
    // Uses USE_ALTERNATE_LDS_ORG_X
    //
    // (Note that esp is 0 if only 1 threadid is left available for part of the inner indexing,
    // so mid part disappears at compile-time)
    #ifndef X_CACHE_EXTRA // in case used without USE_ALTERNATE_LDS_ORG_X
        #define X_CACHE_EXTRA 0
    #endif
    #define CALCULATE_LDS_STORE_IDX_X__v4(ti, gtid, esp, eic) \
        ( /*high part*/ ((SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) + (NB_THREADIDS_FOR_INNER_X_LDS_STORE * (eic))) * (NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE + X_CACHE_EXTRA)) \
          /*mid part*/ + (esp) * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE  \
            + (SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid)) \
        )\

    #define CALCULATE_LDS_LOAD_IDX_X__v4(gtid, esp, ik) \
        ( ((ik) * (NUMTHREADS_SPATIALS * SPATIALS_ELEM_PER_THREAD + X_CACHE_EXTRA)) \
            + (esp) * NUMTHREADS_SPATIALS  \
            + SELECT_FINAL_OUT_SPATIALS_THREADIDS(ti, gtid) \
        )\

    // ---------------------------------------------------------------------------------------------


//
// Final selection of LDS access macros from the various versions declared before
//

#if defined(NB_THREADIDS_FROM_NUMTHREADS_XY) || defined(USE_CUSTOM_THREAD_NB_AND_v1_FOR_LDS_STORE)

    #ifdef USE_ALTERNATE_LDS_ORG_W // these need W_CACHE_EXTRA
        // #define CALCULATE_LDS_STORE_IDX_W(ti, gtid, eoc, eic) CALCULATE_LDS_STORE_IDX_W__v1(ti, gtid, eoc, eic)
        // #define CALCULATE_LDS_LOAD_IDX_W(gtid, eoc, ik)  CALCULATE_LDS_LOAD_IDX_W__v1(gtid, eoc, ik)
        // #define CALCULATE_LDS_STORE_IDX_W(ti, gtid, eoc, eic) CALCULATE_LDS_STORE_IDX_W__v1b(ti, gtid, eoc, eic)
        // #define CALCULATE_LDS_LOAD_IDX_W(gtid, eoc, ik)  CALCULATE_LDS_LOAD_IDX_W__v1b(gtid, eoc, ik)
        #define CALCULATE_LDS_STORE_IDX_W(ti, gtid, eoc, eic) CALCULATE_LDS_STORE_IDX_W__v1(ti, gtid, eoc, eic)
        #define CALCULATE_LDS_LOAD_IDX_W(gtid, eoc, ik)  CALCULATE_LDS_LOAD_IDX_W__v1_Repurpose(gtid, eoc, ik)
    #else
        #define CALCULATE_LDS_STORE_IDX_W(ti, gtid, eoc, eic) CALCULATE_LDS_STORE_IDX_W__v2(ti, gtid, eoc, eic)
        #define CALCULATE_LDS_LOAD_IDX_W(gtid, eoc, ik)  CALCULATE_LDS_LOAD_IDX_W__v2(gtid, eoc, ik)
    #endif

    #ifdef USE_ALTERNATE_LDS_ORG_X // these need X_CACHE_EXTRA
        // #define CALCULATE_LDS_STORE_IDX_X(ti, gtid, esp, eic) CALCULATE_LDS_STORE_IDX_X__v1(ti, gtid, esp, eic)
        // #define CALCULATE_LDS_LOAD_IDX_X(gtid, esp, ik)  CALCULATE_LDS_LOAD_IDX_X__v1(gtid, esp, ik)
        // #define CALCULATE_LDS_STORE_IDX_X(ti, gtid, esp, eic) CALCULATE_LDS_STORE_IDX_X__v1b(ti, gtid, esp, eic)
        // #define CALCULATE_LDS_LOAD_IDX_X(gtid, esp, ik)  CALCULATE_LDS_LOAD_IDX_X__v1b(gtid, esp, ik)
        #define CALCULATE_LDS_STORE_IDX_X(ti, gtid, esp, eic) CALCULATE_LDS_STORE_IDX_X__v1(ti, gtid, esp, eic)
        #define CALCULATE_LDS_LOAD_IDX_X(gtid, esp, ik)  CALCULATE_LDS_LOAD_IDX_X__v1_Repurpose(gtid, esp, ik)
    #else
        #define CALCULATE_LDS_STORE_IDX_X(ti, gtid, esp, eic) CALCULATE_LDS_STORE_IDX_X__v2(ti, gtid, esp, eic)
        #define CALCULATE_LDS_LOAD_IDX_X(gtid, esp, ik)  CALCULATE_LDS_LOAD_IDX_X__v2(gtid, esp, ik)
    #endif

#elif (MAIN_CONFIG_NUMBER == 3) || (MAIN_CONFIG_NUMBER == 4)

        #define CALCULATE_LDS_STORE_IDX_W(ti, gtid, eoc, eic) CALCULATE_LDS_STORE_IDX_W__v4(ti, gtid, eoc, eic)
        #define CALCULATE_LDS_LOAD_IDX_W(gtid, eoc, ik)  CALCULATE_LDS_LOAD_IDX_W__v4(gtid, eoc, ik)
        #define CALCULATE_LDS_STORE_IDX_X(ti, gtid, esp, eic) CALCULATE_LDS_STORE_IDX_X__v4(ti, gtid, esp, eic)
        #define CALCULATE_LDS_LOAD_IDX_X(gtid, esp, ik)  CALCULATE_LDS_LOAD_IDX_X__v4(gtid, esp, ik)

#else

    #define CALCULATE_LDS_STORE_IDX_W(ti, gtid, eoc, eic) CALCULATE_LDS_STORE_IDX_W__v3(ti, gtid, eoc, eic)
    #define CALCULATE_LDS_LOAD_IDX_W(gtid, eoc, ik)  CALCULATE_LDS_LOAD_IDX_W__v3(gtid, eoc, ik)

    #define CALCULATE_LDS_STORE_IDX_X(ti, gtid, esp, eic) CALCULATE_LDS_STORE_IDX_X__v3(ti, gtid, esp, eic)
    #define CALCULATE_LDS_LOAD_IDX_X(gtid, esp, ik)  CALCULATE_LDS_LOAD_IDX_X__v3(gtid, esp, ik)


#endif //...#if defined(NB_THREADIDS_FROM_NUMTHREADS_XY) || defined(USE_CUSTOM_THREAD_NB_AND_v1_FOR_LDS_STORE)

#if defined(USE_BANK_CONFLICT_OFFSET) // to test when not using USE_ALTERNATE_LDS_ORG_*
    #define LDS_ACCESS_STORE_W(index) LDS_W[ LDS_BCAO(index) ]
    #define LDS_ACCESS_LOAD_W(index) LDS_W[ LDS_BCAO(index) ]
    #define LDS_ACCESS_STORE_X(index) LDS_X[ LDS_BCAO(index) ]
    #define LDS_ACCESS_LOAD_X(index) LDS_X[ LDS_BCAO(index) ]
#else
    #define LDS_ACCESS_STORE_W(index) LDS_W[index]
    #define LDS_ACCESS_LOAD_W(index) LDS_W[index]
    #define LDS_ACCESS_STORE_X(index) LDS_X[index]
    #define LDS_ACCESS_LOAD_X(index) LDS_X[index]
#endif


// ----------------------------------------------------------------------------------------------------------------
// ...LDS (SM/CU shared memory) declaration and access config
// ----------------------------------------------------------------------------------------------------------------








inline DATA_TYPE ApplyFusedActivation(DATA_TYPE v, float dbg = 0.0)
{
#if (DATA_TYPE_SPEC == DATA_TYPE_SPEC_NATIVE)

// #if GROUP_NB_OC_CHAN_DIVIDER == 1 && defined(NON_UNIFORM_CONVGROUP_PER_OC)
    // v *= 0.0f;
    // return v;
// #else
    //return max(v, _MinValue) * UnsizedLastMemberTest[0].x;
    //return max(v, _MinValue) * CBTestA.x;
    return max(v, _MinValue) * Scale;
    //return dbg;
    //return max(v, _MinValue) * CBTestArray[0].x;
//#endif
#else

    return LFLOATMUL_DATA(Scale, v);
#endif
}

[numthreads(NUMTHREADS_X, NUMTHREADS_Y, 1)]
void MainName(uint3 dispatchThreadID : SV_DispatchThreadID, uint3 groupThreadID : SV_GroupThreadID, uint threadIndex : SV_GroupIndex, uint3 groupID : SV_GroupID)
{
    uint3 gtid = groupThreadID;
    uint ti = threadIndex;
    // uint x = dispatchThreadID.x * ELEM_PER_THREAD_X; // depth*width*height
    // uint y = dispatchThreadID.y * ELEM_PER_THREAD_Y; // output_channels
    // uint tx = groupThreadID.x;
    // uint ty = groupThreadID.y;
    // uint bx = (NUMTHREADS_X * groupID.x) * ELEM_PER_THREAD_X;
    // uint by = (NUMTHREADS_Y * groupID.y) * ELEM_PER_THREAD_Y;
    uint outTGSpatialBaseOffset = (NUMTHREADS_SPATIALS * SELECT_FINAL_OUT_SPATIALS_THREADGROUP_ID(groupID)) * SPATIALS_ELEM_PER_THREAD;
    uint outTGOutchanBaseOffset = (NUMTHREADS_OUTCHAN * SELECT_FINAL_OUT_OUTCHAN_THREADGROUP_ID(groupID)) * OUTCHAN_ELEM_PER_THREAD;
    // uint ti = threadIndex;
    // uint xx =  bx + groupThreadID.x;
    // uint yy =  by + groupThreadID.y;
    // uint xx =  outTGSpatialBaseOffset + groupThreadID.x;
    // uint yy =  outTGOutchanBaseOffset + groupThreadID.y;
    uint outSpatialsThreadBaseOffset =  outTGSpatialBaseOffset + SELECT_FINAL_OUT_SPATIALS_THREADIDS(ti, gtid);
    uint outOutchanThreadBaseOffset =  outTGOutchanBaseOffset + SELECT_FINAL_OUT_OUTCHAN_THREADIDS(ti, gtid);

    #if defined(CONV3D)
    uint d = O_depth;
    uint h = O_height;
    uint w = O_width;
    uint channels = X_channels;
    uint features = O_channels;
    uint depthX = X_depth;
    uint heightX = X_height;
    uint widthX = X_width;
    uint strideX = depthX * heightX * widthX;
    uint strideO = d * h * w;
    uint strideK = K_depth * K_height * K_width;
    #elif defined(CONV2D)
    uint h = O_height;
    uint w = O_width;
    uint channels = X_channels;
    uint features = O_channels;
    uint heightX = X_height;
    uint widthX = X_width;
    uint strideX = heightX * widthX;
    uint strideO = h * w;
    uint strideK = K_height * K_width;
    #elif defined(CONV1D)
    uint w = O_width;
    uint channels = X_channels;
    uint features = O_channels;
    uint widthX = X_width;
    uint strideX = widthX;
    uint strideO = w;
    uint strideK = K_width;
    #endif

    #if !defined(STFT_DOUBLEOUT)
    uint nbFeaturesToCompute = features;
    #else
    uint nbFeaturesToCompute = NbUniqueDFTFreqTimes2;
    #endif

#if defined(CONV_TRANSPOSE)
    // In CONV_TRANSPOSE, for input channel strides in the kernel if there are groups,
    // we can't use the total number of features we output (ie "features") as the axis size:
    #if defined(GROUPS_ENABLED)
    uint kFeaturesAxisSize = O_channelsPerGroup;
    #else
    uint kFeaturesAxisSize = features;
    #endif
#endif

#if defined(GROUPS_ENABLED)
    uint numInnerChannels = X_channelsPerGroup;
#else
    uint numInnerChannels = X_channels;
#endif

    uint kInnerSliceSize = numInnerChannels * strideK;
    uint maxBIndex = (features - 1);

#if !defined(SHADER_API_D3D11)
    uint maxXIndex = O_batch * X_channels * strideX - 1;
#if !defined(CONV_TRANSPOSE)
    uint maxKIndex = O_channels * numInnerChannels * strideK - 1;
#else
    uint maxKIndex = X_channels * kFeaturesAxisSize * strideK - 1;
#endif
#endif // #if !defined(SHADER_API_D3D11)



#if !defined(CONV_TRANSPOSE)
// Important:
//
// First note "channels" (input channels number) and "features" (output channels number)
// are taken from the input tensor X and the output tensor O, NOT the kernel.
// This allows us to ignore for these the swap in semantics on which axes we use in the kernel
// for the inner product part of the convolution, depending on if we have conv transpose or not.
// ie the semantics of "channels" and "features" stay the same whether we have conv transpose
// or not.
//
// When NOT conv transpose, the kernel inner slice size - that is the inner (input) channels and spatial dims
// on which we do an inner product with the input data X -
// and output channel element-to-element strides are the same, obviously because these axes are all adjacent,
// but the kernel is kept identical if to be used in a conv transpose, but the semantics of the
// output/input channel axes are swapped (eg to allow reusing the same kernel tensors in forward and
// backward passes since the conv transpose is effectively the gradient of the conv with the same kernel
// wrt to its inputs given the outputs that were generated).
// Thus in conv transpose, we sum on the outermost axis (axis 0) of the kernel for "input" channels
// and the spatial axes, so the "inner slice" (also called "kicsp" in this code, that we cache in a mixed
// fashion when CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) is effectively split and not contiguous.

    //uint kStrideOC = X_channels * strideK;
    uint kStrideOC = numInnerChannels * strideK;
    // Note here we assume that when not CONV_TRANSPOSE, numInnerChannels == X_channelsPerGroup == kernelShape.axis[1] == X_channels / NumGroups
    // IMPORTANT: Note also we dont use directly kInnerSliceSize, as the later can be reduced if groups are enabled,
    // but here we want the kernel oc *stride* regardless of grouping
#else
// When conv transpose, switching output channels in the kernel means switching elements on axis 1
// (ie + spatial dims total size, strideK)
#define kStrideOC strideK

    // (Note kFeaturesAxisSize is defined earlier because we need it for maxXIndex)
    // 
    // In CONV_TRANSPOSE, for input channel strides in the kernel if there are groups,
    // we can't use the total number of features we output (ie "features") as the axis size:
    // #if defined(GROUPS_ENABLED)
    // uint kFeaturesAxisSize = O_channelsPerGroup;
    // #else
    // uint kFeaturesAxisSize = features;
    // #endif
#endif


    uint batchReadOffset = dispatchThreadID.z * channels * strideX;
    uint batchWriteOffset = dispatchThreadID.z * features * strideO;


    // For groups, we split the input channel number offset between when our TG deals with a single
    // convGroupId, vs when they can be different depending on the output channel selected,
    // and this is indicated by NON_UNIFORM_CONVGROUP_PER_OC.
    // When the later is enabled, inputChannelOffsetFromGroupByOC is used.
    uint inputChannelOffsetFromGroup = 0;

#if defined(GROUPS_ENABLED)
#if !defined(NON_UNIFORM_CONVGROUP_PER_OC)
    // Important: !defined(NON_UNIFORM_CONVGROUP_PER_OC) 
    // means NB_OUTCHAN_PER_TG <= GROUP_NB_OC_CHAN_DIVIDER == O_channelsPerGroup / k  (where k is a positive int >= 1)
    // and since
    //
    //      SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) + eoc * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE < NB_OUTCHAN_PER_TG,
    //
    // (where eoc := explicit output channel index)
    // we can thus assume that (SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) + eoc)/ O_channelsPerGroup == 0
    // 
    // We also have
    // outChannelId = (by + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) + eoc * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE);
    //
    // Now the division above is intdiv so it is no longer distributive, except if it divides evenly 1 term of the sum,
    // which is the case if !defined(NON_UNIFORM_CONVGROUP_PER_OC) for NB_OUTCHAN_PER_TG and "by".
    // We saw that one term will be 0, thus we have:
    //
    // groupId = outChannelId / O_channelsPerGroup
    //         = (by + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) + eoc * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) / O_channelsPerGroup
    //         = by / O_channelsPerGroup;
    uint convGroupId = outTGOutchanBaseOffset / O_channelsPerGroup;
    inputChannelOffsetFromGroup = convGroupId * X_channelsPerGroup;
#else
    // TODOTODO check if worth it to precomp those instead of calculating on the fly from outputChannelNum
    //
    // IMPORTANT: inputChannelOffsetFromGroupByOC is used in conv transpose for W_LDS_STORE part,
    // but if !defined(NB_THREADIDS_FROM_NUMTHREADS_XY), threads are repurposed
    // and thus SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE works, but for the access of the input data (Xptr)
    // in the inner sum it won't work because in that case, groupThreadID.y is the thread id for
    // output channels. We thus need to recompute another set of inputChannelOffsetFromGroupByOC!
    //
    // Only if 
    // #if !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && defined(CONV_TRANSPOSE) && defined(NON_UNIFORM_CONVGROUP_PER_OC)
    // would we actually need the version for W_LDS_STORE separate from the groupThreadID.y version.
    //
    uint inputChannelOffsetFromGroupByOC[NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE];
    {
        uint outputChannelNum = (outTGOutchanBaseOffset + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid));
        [unroll]
        for (uint eoc = 0; eoc < NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE; eoc++)
        {
            uint convGroupId = outputChannelNum / O_channelsPerGroup;
            inputChannelOffsetFromGroupByOC[eoc] = convGroupId * X_channelsPerGroup;
            outputChannelNum += NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE;
        }
    }
#endif // !defined(NON_UNIFORM_CONVGROUP_PER_OC)
#endif // defined(GROUPS_ENABLED)


    // kernel read index:
    //
    uint kInputChannelOffsetFromGroup = inputChannelOffsetFromGroup;


    // Update: because we need to compare to "features" (ouput axis 1) for oob for all threads in all TG, dont use modulo first,
    // doesn't save anything, still need to do it every loop turn anyway
    uint baseOutputChannelFromThreadId = (outTGOutchanBaseOffset + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid));

#if defined(CONV_TRANSPOSE) && defined(GROUPS_ENABLED)
    // If there is conv transpose and conv groups, even when the threads all deal with one convGroupId
    // (ie !defined(NON_UNIFORM_CONVGROUP_PER_OC)), since the axis 1 of the kernel is now the output channel axis,
    // and the size of that axis is only as large as O_channelsPerGroup, we need to make sure
    // that the contribution to the kernel read final index from the output channel doesn't go beyond O_channelsPerGroup.
    uint baseOutputChannelAxisElementFromThreadId = baseOutputChannelFromThreadId % O_channelsPerGroup;
#else
    uint baseOutputChannelAxisElementFromThreadId = baseOutputChannelFromThreadId;
#endif


#if !defined(CONV_TRANSPOSE) || (!defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) && !defined(NON_UNIFORM_CONVGROUP_PER_OC))
#if !defined(CONV_TRANSPOSE)
    kInputChannelOffsetFromGroup = 0;
    // First if there's no conv transpose, groups dont matter as there is only one input channel group in the
    // kernel input channel axis (axis 1 when no conv transpose) and groups have no effect on output channel
    // influence on kernel final index.

    // If there is conv transpose but NON_UNIFORM_CONVGROUP_PER_OC, we can't just use a single kInputChannelOffsetFromGroup offset
    // as it depends on the output channel selected (from some thread id and explicitly for NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE values, ie eoc in loops).
    // (And in that later case here we want kInputChannelOffsetFromGroup = 0 too but this is already handled by having
    // inputChannelOffsetFromGroup correctly set to 0 above.)
    // In those NON_UNIFORM_CONVGROUP_PER_OC cases, we deal with this elsewhere, by adding the "input channel offset from group"
    // explicitly where needed.

    // For #if defined(CONV_TRANSPOSE) && defined(GROUPS_ENABLED), see above with baseOutputChannelAxisElementFromThreadId.
#endif

    // For CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE:
    // If we're not doing conv transpose, the inner kernel slice has channels adjacent (and outermore)
    // to the innermore spatial dimensions, so we can use a stride to add to the reading index.
    // When doing conv transpose, we can only use such a stride if we're only caching weights for different inner channels
    // and NOT mixed with different spatial dims too (what I called a mixed kicsp slice) because then the non adjacent
    // axes for kernel input channel and spatial coords require a translation (split in 2 coords).

    uint readK = baseOutputChannelAxisElementFromThreadId * kStrideOC 
        + (kInputChannelOffsetFromGroup + SELECT_FROMTHREADIDS_AS_INNER_FOR_W_LDS_STORE(ti, gtid)) * IF_ONLY_INCHAN_ELSE(IF_CONV_TRANSPOSE_ELSE(kFeaturesAxisSize * strideK, strideK), 1);
    // IF_ONLY_INCHAN_ELSE : else 1 since in that case we dig in a flattened output channel slice, ie kernel weights can correspond to different input channel indices
    // but also kernel spatial offsets indices too.
    // IF_CONV_TRANSPOSE_ELSE: in conv transpose, the semantics of the kernel axes are (in order outermost to inner) input_channels_to_sum, output_channels aka features, spatial dims
    // (axes 0 and 1 semantics are swapped while keeping the exact same kernel tensor as used in a normal conv).
    // So the stride of inner channels ("inner" == those on which we sum) is the axis-0 element stride, not axis-1 as in normal convolution.

    // Also, if CONV_TRANSPOSE and groups are present, we only precalculate any part of the kernel read index if
    // we only deal with a single convGroupId for all our output channels (see NON_UNIFORM_CONVGROUP_PER_OC comment above)
    //
    //     [For the output channel, we need to apply a modulo on every final value of output channel (as in conv transpose
    //     we have "num of groups"-times more final output channels then the number of values on the output channel axis
    //     (in conv transpose, this is axis 1 of the kernel)
    //     also, for the input channel, we need an offset that depends on convGroupId which, if NON_UNIFORM_CONVGROUP_PER_OC,
    //     could also vary per output channel.]

#else
    // Here we have:
    //
    //      defined(CONV_TRANSPOSE) && (defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) || defined(NON_UNIFORM_CONVGROUP_PER_OC)):
    //
    #if !(defined(CONV_TRANSPOSE) && (defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) || defined(NON_UNIFORM_CONVGROUP_PER_OC)))
    #error Unexpected conv transpose config
    #endif

    // Conv transpose path with CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE and or NON_UNIFORM_CONVGROUP_PER_OC
    //
    // CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE case:
    //
    //      No inner slice (kicsp) selection with threadids is made outside loops in conv transpose when caching both for
    //      different kernel input (inner) channels and spatial (kicsp slice), (ie when CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
    //      as a translation (split in 2 coords) is needed
    //      from inner slice element index to 2 coords: axis 0 element 
    //      (kernel "input channels" on which we sum - again note these are normally the output channels for same kernel used as normal conv kernel)
    //      and linear spatial (flattened spatial dims of the kernel) element.


    // Also, in this conv transpose context, if groups are present but the convGroupId stays the same for all our output channels
    // (ie !defined(NON_UNIFORM_CONVGROUP_PER_OC))
    // it means we don't have to "wrap around" our output channel number using a % (modulo) O_channelsPerGroup
    // during later code for all threads / explicit output channel.
    // We sill have to deal with a modulo for baseOutputChannelAxisElementFromThreadId since it includes the global output channel offset
    // for our thread group (see by index).
    // (Remember in that case kernel axis 1 - the output channel axis in conv transpose - only has O_channelsPerGroup elements and
    // further output channels just switch convolution groups).
    // uint readK = baseOutputChannelAxisElementFromThreadId * kStrideOC;

#endif // #if !defined(CONV_TRANSPOSE) || (!defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) && !defined(NON_UNIFORM_CONVGROUP_PER_OC))




    // We will initialize accumulators with output channel bias which is the same for all input/output spatial
    // coordinates (as conceptually part of the kernels for each output channel, which process the same input)
    // so we also replicate these values for all spatial coordinates handled.
    //
    // We need the following amount of accumulators:
    //          (ELEM_PER_THREAD_X ie spatial elements per thread) x (ELEM_PER_THREAD_Y ie output channel elements per thread)
    // 
    // We use indexing for registers to simplify the code but this is all statically indexed so should not change
    // any compiler output vs using non-indexing.
    //

    DATA_TYPE dstA[NB_ACCU_REG_PER_THREAD];
    #define OC_AREG_STRIDE SPATIALS_ELEM_PER_THREAD  // output channel accumulator reg stride

    uint eoc; // (per thread) explicit output channels indices
    uint esp; // (per thread) explicit spatial indices
    [unroll]
    for (eoc = 0; eoc < OUTCHAN_ELEM_PER_THREAD; eoc++)
    {
    #if defined(USEBIAS) && !defined(BIAS_AFTER)
        // debug
        // #if defined(NON_UNIFORM_CONVGROUP_PER_OC)
        // dstA[eoc * OC_AREG_STRIDE] = 0;
        // #else
        dstA[eoc * OC_AREG_STRIDE] = READ_Bptr(MIN_WHEN_NON_D3D(outOutchanThreadBaseOffset + eoc * NUMTHREADS_OUTCHAN, maxBIndex));
        // #endif
    #else
        dstA[eoc * OC_AREG_STRIDE] = DATA_TYPE_ADDITIVE_ZERO;
    #endif
    }

    // Init / Replication of accumulators from bias:
    [unroll]
    for (eoc = 0; eoc < OUTCHAN_ELEM_PER_THREAD; eoc++)
        [unroll]
        for (esp = 1; esp < SPATIALS_ELEM_PER_THREAD; esp++)
        {
        #if defined(USEBIAS) && !defined(BIAS_AFTER)
            dstA[eoc * OC_AREG_STRIDE + esp] = dstA[eoc * OC_AREG_STRIDE];
        #else
            dstA[eoc * OC_AREG_STRIDE + esp] = DATA_TYPE_ADDITIVE_ZERO;
        #endif
        }


    // Spatial output coordinates starting point (where kernel window offset is added to get where kernel window overlaps input),
    // for each final spatial output position per thread:

#if !(   defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && (NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE != SPATIALS_ELEM_PER_THREAD)  )

    // The clause "A" inside the negated !(A) condition above, ie
    //      defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && (NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE != SPATIALS_ELEM_PER_THREAD)
    // means that because of NON_UNIFORM_CONVGROUP_PER_OC, we will need to stream directly from global memory the X input data,
    // and thus require the top*Padded vars, but if !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && (NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE != SPATIALS_ELEM_PER_THREAD),
    // it means we have threadids repurposed when reloading from LDS and these will need to be calculated differently:
    //
    // ie if [ defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && (NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE != SPATIALS_ELEM_PER_THREAD) ]
    // we can't precalculate those here, we will do it in the later phase as we stream and dont cache X input data

    uint topDPadded[NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE];
    uint topYPadded[NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE];
    uint topXPadded[NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE];
    [unroll]
    for (esp = 0; esp < NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE; esp++)
    {
        // Important: Note the stride of NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE
        // NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE == NB_SPATIALS_PER_TG
        //uint centroidId = bx + SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid) + esp * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE;
        uint centroidId = outTGSpatialBaseOffset + SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid) + esp * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE;

        #if !defined(CONV_TRANSPOSE)
        uint4 strideParam = _StrideParam;
        #else
        uint4 strideParam = uint4(1,1,1,1);
        #endif

        #if defined(CONV3D)
        topDPadded[esp] = (((centroidId / w) / h)) * strideParam.x - _Pad.x;
        topYPadded[esp] = ((centroidId / w) % h) * strideParam.y - _Pad.y;
        topXPadded[esp] = (centroidId % w) * strideParam.z - _Pad.z;

        #elif defined(CONV2D)
        topYPadded[esp] = ((centroidId / w)) * strideParam.x - _Pad.x;
        topXPadded[esp] = (centroidId % w) * strideParam.y - _Pad.y;

        #elif defined(CONV1D)
        topXPadded[esp] = centroidId * strideParam.x - _Pad.x;

        #endif
    }


    // uint readX = batchReadOffset + strideX * SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) + linear offset;
    // Final linear offset, eg for 2D = (topY - _Pad.x) * widthX + (topX - _Pad.y),
    // will be in kernelOffsetX.
    #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
    //uint readX = batchReadOffset + strideX * SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid);
    #endif
#endif


#if defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)

    for (uint i = 0; i < kInnerSliceSize; i += CACHE_DEPTH)

#else // #if CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE

    // Here we only cache (for the inner product) for different input channels,
    // not for different kernel spatial coords

    #if defined(CONV_TRANSPOSE)
    uint weightOffsetK = (strideK - 1);
    #else
    uint weightOffsetK = 0;
    #endif


#if !defined(K1x1)
    #if defined(CONV3D)
        for (uint dd = 0; dd < K_depth; dd++)
        for (uint dy = 0; dy < K_height; dy++)
        for (uint dx = 0; dx < K_width; dx++)
    #elif defined(CONV2D)
        for (uint dy = 0; dy < K_height; dy++)
        for (uint dx = 0; dx < K_width; dx++)
    #elif defined(CONV1D)
        for (uint dx = 0; dx < K_width; dx++)
    #endif
#endif // !defined(K1x1)
#endif // #else // #if CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE
    {
        #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)

        #if !(   defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && (NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE != SPATIALS_ELEM_PER_THREAD)  )
        // if [ defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && (NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE != SPATIALS_ELEM_PER_THREAD) ]
        // we can't precalculate those here, we will do it in the later phase as we stream and dont cache X input data
            uint kernelOffsetX[NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE];
            bool maskX[NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE];

            [unroll]
            for (esp = 0; esp < NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE; esp++)
            {
                #if defined(CONV_TRANSPOSE)
                uint4 strideParamIfConvTrans = _StrideParam;
                #else
                uint4 strideParamIfConvTrans = uint4(1,1,1,1);
                #endif

                // Note: because of the define above, if we're not using conv transpose,
                // all / strideParamIfConvTrans subexpressions and all mask modulo ( % ) tests
                // will all be optimized out by the compiler.
                // This is just to avoid even more #ifdef blocks below.

                // We do the same for K1x1 vs dilations:
                #if defined(K1x1)
                uint4 dilationParam = uint4(0,0,0,0);
                uint dd = 0;
                uint dy = 0;
                uint dx = 0;
                #else
                uint4 dilationParam = _Dilation;
                #endif

                #ifdef CONV3D
                kernelOffsetX[esp] = (topDPadded[esp] + dilationParam.x * dd)  / strideParamIfConvTrans[0] * heightX * widthX + (topYPadded[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1] * widthX 
                    + (topXPadded[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2];
                maskX[esp] = ((topDPadded[esp] + dilationParam.x * dd) / strideParamIfConvTrans[0]) < depthX &&  ((topDPadded[esp] + dilationParam.x * dd) % strideParamIfConvTrans[0] == 0) &&
                             ((topYPadded[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1]) < heightX && ((topYPadded[esp] + dilationParam.y * dy) % strideParamIfConvTrans[1] == 0) &&
                             ((topXPadded[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2]) < widthX &&  ((topXPadded[esp] + dilationParam.z * dx) % strideParamIfConvTrans[2] == 0);
                #elif defined(CONV2D)
                kernelOffsetX[esp] = (topYPadded[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0] * widthX + (topXPadded[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1];
                maskX[esp] = ((topYPadded[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0]) < heightX && ((topYPadded[esp] + dilationParam.x * dy) % strideParamIfConvTrans[0] == 0) &&
                             ((topXPadded[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1]) < widthX &&  ((topXPadded[esp] + dilationParam.y * dx) % strideParamIfConvTrans[1] == 0);
                #elif defined(CONV1D)
                kernelOffsetX[esp] = (topXPadded[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0];
                maskX[esp] = ((topXPadded[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0]) < widthX && ((topXPadded[esp] + dilationParam.x * dx) % strideParamIfConvTrans[0] == 0);
                #endif
            }
        #endif // #if !(   defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && (NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE != SPATIALS_ELEM_PER_THREAD)  )

        for (uint i = 0; i < numInnerChannels; i += CACHE_DEPTH)
        #endif // #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
        {
            //
            // store to LDS : First load in registers (srcW and srcX) from global memory:
            //
            {
                // #if NB_EXPLICIT_INNER_VAL_NEEDED_W_LDS_STORE * NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE == 2 * 8
                // #error ok
                // #endif
                DATA_TYPE srcW[NB_EXPLICIT_INNER_VAL_NEEDED_W_LDS_STORE * NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE];
                DATA_TYPE srcX[NB_EXPLICIT_INNER_VAL_NEEDED_X_LDS_STORE * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE];

                bool innerElemInBound = false;

                // Note: These line (IF_THREAD_DOING_*) can create a huge difference in perf in a case where we have more threadids left for the
                // inner slice indices after allocating those for output channel indices, then we need for the total CACHE_DEPTH)
                // since otherwise (eg when not using NB_THREADIDS_FROM_NUMTHREADS_XY) a group of totalthreads_in_tg/(ELEM_PER_THREAD_Y*NUMTHREADS_Y) threads 
                // all try to load/store the same location (on store, which one is undefined but since all carry the same value it doesn't matter)
                // but more importantly those requests could come from different waves so even for loads probably no hw broadcast possible
                // (and for store, can't just pick a single thread for the undefined spec).
                IF_THREAD_DOING_W_LDS_STORE(ti, gtid)
                {
                    const uint innerThreadIdsStride = NB_THREADIDS_FOR_INNER_W_LDS_STORE; // threadids used for inchan stride

                    // uint readK = (by + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid)) * kStrideOC + SELECT_FROMTHREADIDS_AS_INNER_FOR_W_LDS_STORE(ti, gtid) * IF_ONLY_INCHAN_ELSE(strideK,1);
                    // bool outChanMaskK = (by + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid)) < features;


                #if !defined(CONV_TRANSPOSE) || (!defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) && !defined(NON_UNIFORM_CONVGROUP_PER_OC))
                    // If we're not doing conv transpose, the inner kernel slice has channels adjacent (and outermore)
                    // to the innermore spatial dimensions, so we can use a stride to add to the reading index.

                    // When doing conv transpose, we can only use such a stride if we're only caching weights for different inner channels
                    // and NOT mixed with different spatial dims too (what I called a mixed kicsp slice).
                    // (Otherwise a translation of the inner index into 2 coords is needed).
                    //
                    // Also with conv transpose, with groups, if defined(NON_UNIFORM_CONVGROUP_PER_OC), the convGroupId (and thus offset due to it)
                    // depends on the final output channel value and so on the "eoc" loop index below.
                    // See -> Check #if defined(CONV_TRANSPOSE) && !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) && defined(NON_UNIFORM_CONVGROUP_PER_OC)
                    // 
                    const uint innerIdxStride = IF_ONLY_INCHAN_ELSE(IF_CONV_TRANSPOSE_ELSE(kFeaturesAxisSize * strideK, strideK), 1) * innerThreadIdsStride;

                    // In conv transpose, the semantics of the kernel axes are (in order outermost to inner) input_channels_to_sum, output_channels aka features, spatial dims
                    // (axes 0 and 1 semantics are swapped while keeping the exact same kernel tensor as used in a normal conv).

                    //uint readK = (by + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid)) (in fact baseOutputChannelAxisElementFromThreadId possibly with modulo O_channelsPerGroup) * kStrideOC 
                    //    + (kInputChannelOffsetFromGroup + SELECT_FROMTHREADIDS_AS_INNER_FOR_W_LDS_STORE(ti, gtid)) * IF_ONLY_INCHAN_ELSE(IF_CONV_TRANSPOSE_ELSE(features * strideK, strideK), 1);

                    uint baseIdxInnerPartStart = readK + IF_ONLY_INCHAN_ELSE(IF_CONV_TRANSPOSE_ELSE(kFeaturesAxisSize * strideK, strideK), 1) * i + IF_ONLY_INCHAN_ELSE(weightOffsetK, 0);

                #else
                    //      defined(CONV_TRANSPOSE) && (defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) || defined(NON_UNIFORM_CONVGROUP_PER_OC)):

                    //
                    // Conv transpose path with:
                    // CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE, with or without groups,
                    // or with NON_UNIFORM_CONVGROUP_PER_OC, with or without CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE.
                    //

                    // Conv tranpose + CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE:
                    //
                    //      No inner slice (kicsp) selection with threadids is made outside loops in conv transpose when caching both for
                    //      different kernel input (inner) channels and spatial (kicsp slice), (ie when CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                    //      as a translation (split in 2 coords) is needed
                    //      from inner slice element index to 2 coords: axis 0 element 
                    //      (kernel "input channels" on which we sum - again note these are normally the output channels for same kernel used as normal conv kernel)
                    //      and linear spatial (flattened spatial dims of the kernel) element.
                    //
                    //      uint baseOutputChannelAxisElementFromThreadId = (by + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid)) possibly with modulo O_channelsPerGroup;

                    // Conv tranpose + NON_UNIFORM_CONVGROUP_PER_OC:
                    //
                    //      Whether we have CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE or not,
                    //      we will not deal with the inner part contributions to the index outside the
                    //      unrolled loops below as with NON_UNIFORM_CONVGROUP_PER_OC,
                    //      the inputChannelOffsetFromGroupByOC depends on the explicit output channel.

                    uint baseIdxInnerPartStart = baseOutputChannelAxisElementFromThreadId * kStrideOC + IF_ONLY_INCHAN_ELSE(weightOffsetK, 0);

                #endif // #if !defined(CONV_TRANSPOSE) || (!defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) && !defined(NON_UNIFORM_CONVGROUP_PER_OC))


                    uint outputChannelNum = baseOutputChannelFromThreadId;
                    bool curOutChanMaskK = outputChannelNum < nbFeaturesToCompute;

                    // TODOTODO: #if defined(CONV_TRANSPOSE) && defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                    // flip the loop order
                    [unroll]
                    for (uint eoc = 0; eoc < NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE; eoc++)
                    {
                        uint baseIdx = baseIdxInnerPartStart;
                        uint baseInnerElement = SELECT_FROMTHREADIDS_AS_INNER_FOR_W_LDS_STORE(ti, gtid) + i;
                        // ...important! reset baseIdx and baseInnerElement for the flattened/combined kStrideOC-sized or input channels-sized slice

                    // -> Check
                    #if defined(CONV_TRANSPOSE) && !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) && defined(NON_UNIFORM_CONVGROUP_PER_OC)
                        // baseInnerElement is just input channel here as NO CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE
                        // also we have an output_channel-dependent offset to add to it for convGroupId:
                        baseIdx = baseIdxInnerPartStart + (inputChannelOffsetFromGroupByOC[eoc] + baseInnerElement) * (kFeaturesAxisSize * strideK);
                        const uint innerIdxStride = IF_ONLY_INCHAN_ELSE(IF_CONV_TRANSPOSE_ELSE(kFeaturesAxisSize * strideK, strideK), 1) * innerThreadIdsStride;
                    #endif


                        [unroll]
                        for (uint eic = 0; eic < NB_EXPLICIT_INNER_VAL_NEEDED_W_LDS_STORE; eic++)
                        {
                        #if defined(CONV_TRANSPOSE) && defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                            // Here, output channel part of the kernel read index already handled above with baseIdx = baseIdxInnerPartStart.
                            // We also handle all conv group modes: with/without groups, and uniform and non-uniform groups.

                            uint kInnerFlatSpatialElement = baseInnerElement % strideK;
                            uint kInnerInChannelElement = baseInnerElement / strideK;
                            uint kInnerInChannelElementWGroupOffset = kInnerInChannelElement + IF_HAVE_GROUPS_ELSE(IF_NON_UNIFORM_GROUPS_ELSE(inputChannelOffsetFromGroupByOC[eoc], inputChannelOffsetFromGroup), 0);
                            baseIdx = baseIdxInnerPartStart + kInnerInChannelElementWGroupOffset * (kFeaturesAxisSize * strideK)
                                + (strideK - 1) - kInnerFlatSpatialElement;
                            // (strideK - 1) - kInnerFlatSpatialElement: kernel is flipped along its spatial axes and the new "top/left/element 0" ie start of the window
                            // which is aligned on each start of input element (input is conceptually augmented with zero padding and "exploded with interleaved zeroes" by the stride param)
                            // is the "end" of the "original" (ie that would be used in normal conv) kernel and moving forward in the kernel for conv transpose
                            // means backward in the "original" kernel.
                        #endif

                            //innerElemInBound = SELECT_FROMTHREADIDS_AS_INNER_FOR_W_LDS_STORE(ti, gtid) + (i + eic * innerThreadIdsStride) < IF_ONLY_INCHAN_ELSE(numInnerChannels, kInnerSliceSize);
                            innerElemInBound = baseInnerElement < IF_ONLY_INCHAN_ELSE(numInnerChannels, kInnerSliceSize);
                            DATA_TYPE val = DATA_TYPE_ADDITIVE_ZERO;
                            if (curOutChanMaskK && innerElemInBound)
                                //val = Kptr[readK + IF_ONLY_INCHAN_ELSE(strideK, 1) * (i + eic * innerThreadIdsStride) + IF_ONLY_INCHAN_ELSE(weightOffsetK, 0)];
                                //debug
                                //val = O_channels;//features;//baseIdx; // debug
                                //debug
                                //val = baseIdx;
                                val = READ_Kptr(MIN_PATCHBUG(baseIdx, maxKIndex));

                        #if !defined(CONV_TRANSPOSE) || !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                            // Note: if defined(NON_UNIFORM_CONVGROUP_PER_OC), we can still use innerIdxStride because
                            // we're in the eic inner loop, eoc doesn't change and thus neither the convGroupId (although it can still be different implicitly per threadID).
                            baseIdx += innerIdxStride;
                        #endif
                            baseInnerElement += innerThreadIdsStride;

                            //LDS_[W_OFFSET + (eic * NB_THREADIDS_FOR_INNER_AND_OUTCHAN_W_LDS_STORE) + ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE_IDX(ti, gtid)] = val;
                            // ...ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE_IDX(ti, gtid) is just (ti)
                            srcW[eoc * NB_EXPLICIT_INNER_VAL_NEEDED_W_LDS_STORE + eic] = val;
                        }

                        //
                        // switch to next group of explicit output channel (1 group == NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE indices)
                        //
                        outputChannelNum += NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE;
                        curOutChanMaskK = outputChannelNum < nbFeaturesToCompute;
                    // -> Check
                    #if defined(CONV_TRANSPOSE) && defined(NON_UNIFORM_CONVGROUP_PER_OC)
                        baseIdxInnerPartStart = kStrideOC * (outputChannelNum % O_channelsPerGroup) + IF_ONLY_INCHAN_ELSE(weightOffsetK, 0);
                    #else
                        baseIdxInnerPartStart += kStrideOC * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE;
                    #endif
                    }

                }


                #if !defined(NON_UNIFORM_CONVGROUP_PER_OC) 
                // ...we dont cache input data if they can come from different groups per output channels
                IF_THREAD_DOING_X_LDS_STORE(ti, gtid)
                {

                    const uint innerThreadIdsStride = NB_THREADIDS_FOR_INNER_X_LDS_STORE; // threadids used for inchan stride
                #if 0
                    // TODOTODO  conv transpose path


                    // Switch LOOP order if NB_EXPLICIT_INNER_VAL_NEEDED_X_LDS_STORE < NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE
                    [unroll]
                    for (uint esp = 0; esp < NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE; esp++)
                    {
                        // cornerId from -> centroidId = bx + SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid);
                        // uint readX = strideX * SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) + cornerId + batchReadOffset;
                        // update: readX no longer has cornerId

                        #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                        //uint readX = batchReadOffset + strideX * SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid);
                        //uint baseIdx = readX + strideX * i + kernelOffsetX[esp]; // now kernelOffsetX includes cornerId !
                        uint baseIdx = batchReadOffset + strideX * (inputChannelOffsetFromGroup + i + SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid)) + kernelOffsetX[esp]; // now kernelOffsetX includes cornerId !
                        uint innerIdxStride = strideX * innerThreadIdsStride;
                        #else
                        // CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE case:
                        // No readX as it would only contain batchReadOffset outside loops, and no innerIdxStride usage possible
                        // as the precise index depends on a conversion between what kicsp index in that (kicsp) mixed inner chunk 
                        // is used for the weights, so converting from the kernel input channel and window coordinates for each 
                        // to the corresponding input data input channel (same) and spatial offset (from the kernel spatial window part).
                        #endif
                        uint baseInnerElement = SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) + i;
                        // ...important! reset baseInnerElement for the flattened/combined kStrideOC-sized or input channels-sized slice
                        // at each start of a new inner (input channel / kernel spatial window) chunk

                        // Also, the switch to next group of explicit spatial indices (esp) (1 group == NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE indices)
                        // is implicit and precalculated in the [esp] indexed arrays (topDXY, kernelOffsetX)

                        [unroll]
                        for (uint eic = 0; eic < NB_EXPLICIT_INNER_VAL_NEEDED_X_LDS_STORE; eic++)
                        {
                        #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)

                            //innerElemInBound = SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) + (i + eic * innerThreadIdsStride) < numInnerChannels;
                            innerElemInBound = baseInnerElement < numInnerChannels;

                            DATA_TYPE val = DATA_TYPE_ADDITIVE_ZERO;
                            if (maskX[esp] && innerElemInBound)
                                val = READ_Xptr(MIN_PATCHBUG(baseIdx, maxXIndex));
                                //val = Xptr[readX + strideX * (i + eic * innerThreadIdsStride) + kernelOffsetX];
                                //                   -------        ---   ----  => hence innerIdxStride to add to baseIdx is just (strideX * innerThreadIdsStride)
                                // and similarly baseInnerElement just doesn't include the strideX, it just expresses the inner element number not the full linear index
                                // so it is simply innerThreadIdsStride since we have NB_THREADIDS_FOR_INNER_X_LDS_STORE threadids chunk at each loop turn.

                            baseIdx += innerIdxStride;
                            baseInnerElement += innerThreadIdsStride;
                            //LDS_[X_OFFSET + (eic * NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE) + ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE_IDX(ti, gtid)] = val;
                            srcX[esp * NB_EXPLICIT_INNER_VAL_NEEDED_X_LDS_STORE + eic] = val;

                        #else
                            // inner index to match kernel CACHE_DEPTH-sized chunk
                            //uint ii = SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) + (i + eic * innerThreadIdsStride);
                            uint ii = baseInnerElement;
                            bool inBound = ii < kInnerSliceSize; // that way no need to check ic < channels (or < numInnerChannels actually) below


                            // Note: K1x1 cannot be defined here at the same time that CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE is,
                            // as in that K1x1 case, we can only be caching for different input channels indices,
                            // the spatial dimensions sizes of the kernel all multiply to 1, ie there's only one spatial coord in the kernel.
                            // #if defined(K1x1)
                            // #error "K1x1 shouldn't touch the path where CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE"
                            // #endif
                            // Update: SHADER_API_WEBGPU

                            #if defined(CONV_TRANSPOSE)
                            uint4 strideParamIfConvTrans = _StrideParam;
                            #else
                            uint4 strideParamIfConvTrans = uint4(1,1,1,1);
                            #endif
                            // Note: because of the define above, if we're not using conv transpose,
                            // all / strideParamIfConvTrans subexpressions and all mask modulo ( % ) tests
                            // will all be optimized out by the compiler.
                            // This is just to avoid even more #ifdef blocks below.

                            #if defined(K1x1)
                            uint4 dilationParam = uint4(0,0,0,0);
                            #else
                            uint4 dilationParam = _Dilation;
                            #endif

                            #if defined(CONV3D)
                            uint dd = ((ii / K_width) / K_height) % K_depth;
                            uint ic = ((ii / K_width) / K_height) / K_depth;
                            uint dy = (ii / K_width) % K_height;
                            uint dx = ii % K_width;
                            #elif defined(CONV2D)
                            uint dy = (ii / K_width) % K_height;
                            uint ic = (ii / K_width) / K_height;
                            uint dx = ii % K_width;
                            #elif defined(CONV1D)
                            uint dx = ii % K_width;
                            uint ic = ii / K_width;
                            #endif

                            ic = ic + inputChannelOffsetFromGroup;

                            #ifdef CONV3D
                            uint kernelOffsetX = (topDPadded[esp] + dilationParam.x * dd)  / strideParamIfConvTrans[0] * heightX * widthX + (topYPadded[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1] * widthX 
                                + (topXPadded[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2];
                            bool maskX = ((topDPadded[esp] + dilationParam.x * dd) / strideParamIfConvTrans[0]) < depthX &&  ((topDPadded[esp] + dilationParam.x * dd) % strideParamIfConvTrans[0] == 0) &&
                                         ((topYPadded[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1]) < heightX && ((topYPadded[esp] + dilationParam.y * dy) % strideParamIfConvTrans[1] == 0) &&
                                         ((topXPadded[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2]) < widthX &&  ((topXPadded[esp] + dilationParam.z * dx) % strideParamIfConvTrans[2] == 0);
                            #elif defined(CONV2D)
                            uint kernelOffsetX = (topYPadded[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0] * widthX + (topXPadded[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1];
                            bool maskX = ((topYPadded[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0]) < heightX && ((topYPadded[esp] + dilationParam.x * dy) % strideParamIfConvTrans[0] == 0) &&
                                         ((topXPadded[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1]) < widthX &&  ((topXPadded[esp] + dilationParam.y * dx) % strideParamIfConvTrans[1] == 0);
                            #elif defined(CONV1D)
                            uint kernelOffsetX = (topXPadded[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0];
                            bool maskX = ((topXPadded[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0]) < widthX && ((topXPadded[esp] + dilationParam.x * dx) % strideParamIfConvTrans[0] == 0);
                            #endif


                            DATA_TYPE val = DATA_TYPE_ADDITIVE_ZERO;
                            if (maskX && inBound)
                                val = READ_Xptr(MIN_PATCHBUG(batchReadOffset + strideX * ic + kernelOffsetX, maxXIndex));
                            baseInnerElement += innerThreadIdsStride;
                            //LDS_[X_OFFSET + (eic * NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE) + ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE_IDX(ti, gtid)] = val;
                            srcX[esp * NB_EXPLICIT_INNER_VAL_NEEDED_X_LDS_STORE + eic] = val;

                        #endif
                        } // for explicit inner input channel * maybe kernel spatial offset (if CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)


                        // CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE case:
                        // The switch to next group of explicit spatial indices (1 group == NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE indices)
                        // is implicit and precalculated in the [esp] indexed arrays (topDXY, kernelOffsetX)
                    }// for explicit spatial channels

                #else // if 0


                    // uint centroidId = bx + SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid) + esp * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE;
                    // -> top*Padded -> kernelOffsetX
                    // uint readX = strideX * SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) + batchReadOffset + kernelOffsetX which include the above

                    #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                    //uint readX = batchReadOffset + strideX * SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid);
                    //uint baseIdx = readX + strideX * i;
                    uint baseIdx = batchReadOffset + strideX * (inputChannelOffsetFromGroup + i + SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid));
                    uint innerIdxStride = strideX * innerThreadIdsStride;
                    #else
                    // CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE case:
                    // No readX as it would only contain batchReadOffset outside loops, and no innerIdxStride usage possible
                    // as the precise index depends on a conversion between what kicsp index in that (kicsp) mixed inner chunk 
                    // is used for the weights, so converting from the kernel input channel and window coordinates for each 
                    // to the corresponding input data input channel (same) and spatial offset (from the kernel spatial window part).
                    #endif
                    uint baseInnerElement = SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) + i;
                    // ...important! reset baseInnerElement for the flattened/combined kStrideOC-sized or input channels-sized slice
                    // at each start of a new inner (input channel / kernel spatial window) chunk

                    // Also, the switch to next group of explicit spatial indices (esp) (1 group == NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE indices)
                    // is implicit and precalculated in the [esp] indexed arrays (topDXY, kernelOffsetX)

                    [unroll]
                    for (uint eic = 0; eic < NB_EXPLICIT_INNER_VAL_NEEDED_X_LDS_STORE; eic++)
                    {
                    #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                        //innerElemInBound = SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) + (i + eic * innerThreadIdsStride) < channels;
                        innerElemInBound = baseInnerElement < numInnerChannels;
                    #else

                        // inner index to match kernel CACHE_DEPTH-sized chunk
                        //uint ii = SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) + (i + eic * innerThreadIdsStride);
                        uint ii = baseInnerElement;
                        bool inBound = ii < kInnerSliceSize; // that way no need to check ic < channels below

                        // Note: K1x1 cannot be defined here at the same time that CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE is,
                        // as in that K1x1 case, we can only be caching for different input channels indices,
                        // the spatial dimensions sizes of the kernel all multiply to 1, ie there's only one spatial coord in the kernel.
                        // Update: see SHADER_API_WEBGPU prob.
                        #if defined(CONV3D)
                        uint dd = ((ii / K_width) / K_height) % K_depth;
                        uint ic = ((ii / K_width) / K_height) / K_depth;
                        uint dy = (ii / K_width) % K_height;
                        uint dx = ii % K_width;
                        #elif defined(CONV2D)
                        uint dy = (ii / K_width) % K_height;
                        uint ic = (ii / K_width) / K_height;
                        uint dx = ii % K_width;
                        #elif defined(CONV1D)
                        uint dx = ii % K_width;
                        uint ic = ii / K_width;
                        #endif

                        ic = ic + inputChannelOffsetFromGroup;

                    #endif


                        [unroll]
                        for (uint esp = 0; esp < NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE; esp++)
                        {
                        #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                            //uint readX = batchReadOffset + strideX * SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid);
                            //uint baseIdx = readX + strideX * i + kernelOffsetX[esp]; // now kernelOffsetX includes cornerId !
                            DATA_TYPE val = DATA_TYPE_ADDITIVE_ZERO;
                            if (maskX[esp] && innerElemInBound)
                                val = READ_Xptr(MIN_PATCHBUG(baseIdx + kernelOffsetX[esp], maxXIndex)); // now kernelOffsetX includes cornerId !
                                //val = Xptr[readX + strideX * (i + eic * innerThreadIdsStride) + kernelOffsetX];
                                //                   -------        ---   ----  => hence innerIdxStride to add to baseIdx is just (strideX * innerThreadIdsStride)
                                // and similarly baseInnerElement just doesn't include the strideX, it just expresses the inner element number not the full linear index
                                // so it is simply innerThreadIdsStride since we have NB_THREADIDS_FOR_INNER_X_LDS_STORE threadids chunk at each loop turn.

                            //LDS_[X_OFFSET + (eic * NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE) + ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE_IDX(ti, gtid)] = val;
                            srcX[esp * NB_EXPLICIT_INNER_VAL_NEEDED_X_LDS_STORE + eic] = val;

                        #else

                            #if defined(CONV_TRANSPOSE)
                            uint4 strideParamIfConvTrans = _StrideParam;
                            #else
                            uint4 strideParamIfConvTrans = uint4(1,1,1,1);
                            #endif

                            // Note: because of the define above, if we're not using conv transpose,
                            // all / strideParamIfConvTrans subexpressions and all mask modulo ( % ) tests
                            // will all be optimized out by the compiler.
                            // This is just to avoid even more #ifdef blocks below.

                            // Note: K1x1 cannot be defined here at the same time that CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE is,
                            // as in that K1x1 case, we can only be caching for different input channels indices,
                            // the spatial dimensions sizes of the kernel all multiply to 1, ie there's only one spatial coord in the kernel.
                            // #if defined(K1x1)
                            // #error "K1x1 shouldn't touch the path where CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE"
                            // #endif
                            // Update: SHADER_API_WEBGPU

                            #if defined(K1x1)
                            uint4 dilationParam = uint4(0,0,0,0);
                            #else
                            uint4 dilationParam = _Dilation;
                            #endif


                            #ifdef CONV3D
                            uint kernelOffsetX = (topDPadded[esp] + dilationParam.x * dd)  / strideParamIfConvTrans[0] * heightX * widthX + (topYPadded[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1] * widthX 
                                + (topXPadded[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2];
                            bool maskX = ((topDPadded[esp] + dilationParam.x * dd) / strideParamIfConvTrans[0]) < depthX &&  ((topDPadded[esp] + dilationParam.x * dd) % strideParamIfConvTrans[0] == 0) &&
                                         ((topYPadded[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1]) < heightX && ((topYPadded[esp] + dilationParam.y * dy) % strideParamIfConvTrans[1] == 0) &&
                                         ((topXPadded[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2]) < widthX &&  ((topXPadded[esp] + dilationParam.z * dx) % strideParamIfConvTrans[2] == 0);
                            #elif defined(CONV2D)
                            uint kernelOffsetX = (topYPadded[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0] * widthX + (topXPadded[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1];
                            bool maskX = ((topYPadded[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0]) < heightX && ((topYPadded[esp] + dilationParam.x * dy) % strideParamIfConvTrans[0] == 0) &&
                                         ((topXPadded[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1]) < widthX &&  ((topXPadded[esp] + dilationParam.y * dx) % strideParamIfConvTrans[1] == 0);
                            #elif defined(CONV1D)
                            uint kernelOffsetX = (topXPadded[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0];
                            bool maskX = ((topXPadded[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0]) < widthX && ((topXPadded[esp] + dilationParam.x * dx) % strideParamIfConvTrans[0] == 0);
                            #endif

                            //LDS_[X_OFFSET + (eic << LOG2_NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE) + ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE_IDX(ti, gtid)] 
                            DATA_TYPE val = DATA_TYPE_ADDITIVE_ZERO;
                            if (maskX && inBound)
                                val = READ_Xptr(MIN_PATCHBUG(batchReadOffset + strideX * ic + kernelOffsetX, maxXIndex));

                            //LDS_[X_OFFSET + (eic * NB_THREADIDS_FOR_INNER_AND_SPATIALS_X_LDS_STORE) + ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE_IDX(ti, gtid)] = val;
                            srcX[esp * NB_EXPLICIT_INNER_VAL_NEEDED_X_LDS_STORE + eic] = val;

                        #endif
                        }// for explicit spatial channels


                        #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                        baseIdx += innerIdxStride;
                        #endif
                        baseInnerElement += innerThreadIdsStride;

                    }// for explicit inner input channel * maybe kernel spatial offset (if CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)

                #endif //0
                } // X_LDS_STORE
                #endif // #if !defined(NON_UNIFORM_CONVGROUP_PER_OC) // we dont cache input data if they can come from different groups per output channels


                GroupMemoryBarrierWithGroupSync();

                // store regs to LDS
                IF_THREAD_DOING_W_LDS_STORE(ti, gtid)
                {
                    [unroll]
                    for (uint eoc = 0; eoc < NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE; eoc++)
                    {
                        [unroll]
                        for (uint eic = 0; eic < NB_EXPLICIT_INNER_VAL_NEEDED_W_LDS_STORE; eic++)
                        {
                            uint index = CALCULATE_LDS_STORE_IDX_W(ti, gtid, eoc, eic);
                            LDS_ACCESS_STORE_W(index) = srcW[eoc * NB_EXPLICIT_INNER_VAL_NEEDED_W_LDS_STORE + eic];
                        }
                    }
                }

                #if !defined(NON_UNIFORM_CONVGROUP_PER_OC) 
                // ...we dont cache input data if they can come from different groups per output channels
                IF_THREAD_DOING_X_LDS_STORE(ti, gtid)
                {
                    [unroll]
                    for (uint eic = 0; eic < NB_EXPLICIT_INNER_VAL_NEEDED_X_LDS_STORE; eic++)
                    {
                        [unroll]
                        for (uint esp = 0; esp < NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE; esp++)
                        {
                            uint index = CALCULATE_LDS_STORE_IDX_X(ti, gtid, esp, eic);
                            LDS_ACCESS_STORE_X(index) = srcX[esp * NB_EXPLICIT_INNER_VAL_NEEDED_X_LDS_STORE + eic]; // srcX[eic];
                        }
                    }
                }
                #endif // #if !defined(NON_UNIFORM_CONVGROUP_PER_OC) 

            }// store to LDS


            GroupMemoryBarrierWithGroupSync();


            // -------------------------------------------------------------------------------------------------------------------
            // Computing (if necessary) those arrays:
            //
            // inputChannelOffsetFromGroupByOC_rel, topD,Y,X,Padded_rel and possibly kernelOffsetX_rel, maskX_rel
            // 
            // If we can't reload inputs (from tensor X) from LDS cache (because they are not in it, ie NON_UNIFORM_CONVGROUP_PER_OC),
            // we will get them from X directly. In that case, we might have to recompute certain values because the thread ids
            // semantics might have been repurposed between the LDS store and the reload / broadcast here,
            // so even if we could have calculated them earlier even in the NON_UNIFORM_CONVGROUP_PER_OC case, we might not have
            // been able to do so because of that repurpose:

            #if defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && (NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE != OUTCHAN_ELEM_PER_THREAD)
            // We need to recompute some of our eoc-accessed (explicit output channels) reg arrays: inputChannelOffsetFromGroupByOC
            //
            // Note we can't just do the extra needed elements (eg if NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE < ELEM_PER_THREAD_Y,
            // just do the (ELEM_PER_THREAD_Y - NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE) extra elements.
            // The reason is twofold: 
            // 1) that not all the same threadids hold those "first already done elements in existing inputChannelOffsetFromGroupByOC[] register array.
            // 2) the threadnumber stride from each explicit reg to the other (as coalesced groups of thread all have their reg[0], reg[1] etc.) is not the same.

            // In that case threadids used for output channels in the LDS store context are not necessarily the same
            // as those used in the final output phase, on and after reloading/broadcast from LDS:
            uint inputChannelOffsetFromGroupByOC_rel[OUTCHAN_ELEM_PER_THREAD];
            {
                uint outputChannelNum = outTGOutchanBaseOffset + SELECT_FINAL_OUT_OUTCHAN_THREADIDS(ti, gtid); // by + gtid.y;  cf with + eoc * NUMTHREADS_OUTCHAN;
                [unroll]
                for (eoc = 0; eoc < OUTCHAN_ELEM_PER_THREAD; eoc++)
                {
                    uint convGroupId = outputChannelNum / O_channelsPerGroup;
                    inputChannelOffsetFromGroupByOC_rel[eoc] = convGroupId * X_channelsPerGroup;
                    outputChannelNum += NUMTHREADS_OUTCHAN; // cf with in non-_rel (reload) arrays: NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE;
                }
            }
            #else
            #define inputChannelOffsetFromGroupByOC_rel inputChannelOffsetFromGroupByOC
            #endif // #if defined(CONV_TRANSPOSE) && defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY)


            #if defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && (NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE != SPATIALS_ELEM_PER_THREAD)
            //
            // Also, same thing for esp-accessed ones (see eoc-accessed just above): top?Padded[] arrays and possibly maskX[], kernelOffsetX[].
            //
                uint topDPadded_rel[SPATIALS_ELEM_PER_THREAD];
                uint topYPadded_rel[SPATIALS_ELEM_PER_THREAD];
                uint topXPadded_rel[SPATIALS_ELEM_PER_THREAD];
            #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                uint kernelOffsetX_rel[SPATIALS_ELEM_PER_THREAD];
                bool maskX_rel[SPATIALS_ELEM_PER_THREAD];
            #endif
            {
                uint centroidId = outTGSpatialBaseOffset + SELECT_FINAL_OUT_SPATIALS_THREADIDS(ti, gtid); // = bx + gtid.x;  cf + esp * NUMTHREADS_SPATIALS;

                #if !defined(CONV_TRANSPOSE)
                uint4 strideParam = _StrideParam;
                #else
                uint4 strideParam = uint4(1,1,1,1);
                #endif

                [unroll]
                for (esp = 0; esp < SPATIALS_ELEM_PER_THREAD; esp++)
                {
                    #if defined(CONV3D)
                    topDPadded_rel[esp] = (((centroidId / w) / h)) * strideParam.x - _Pad.x;
                    topYPadded_rel[esp] = ((centroidId / w) % h) * strideParam.y - _Pad.y;
                    topXPadded_rel[esp] = (centroidId % w) * strideParam.z - _Pad.z;

                    #elif defined(CONV2D)
                    topYPadded_rel[esp] = ((centroidId / w)) * strideParam.x - _Pad.x;
                    topXPadded_rel[esp] = (centroidId % w) * strideParam.y - _Pad.y;

                    #elif defined(CONV1D)
                    topXPadded_rel[esp] = centroidId * strideParam.x - _Pad.x;
                    #endif

                    centroidId += NUMTHREADS_SPATIALS;

                #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)

                    #if defined(CONV_TRANSPOSE)
                    uint4 strideParamIfConvTrans = _StrideParam;
                    #else
                    uint4 strideParamIfConvTrans = uint4(1,1,1,1);
                    #endif

                    // Note: because of the define above, if we're not using conv transpose,
                    // all / strideParamIfConvTrans subexpressions and all mask modulo ( % ) tests
                    // will all be optimized out by the compiler.
                    // This is just to avoid even more #ifdef blocks below.

                    // We do the same for K1x1 vs dilations:
                    #if defined(K1x1)
                    uint4 dilationParam = uint4(0,0,0,0);
                    uint dd = 0;
                    uint dy = 0;
                    uint dx = 0;
                    #else
                    uint4 dilationParam = _Dilation;
                    #endif

                    #ifdef CONV3D
                    kernelOffsetX_rel[esp] = (topDPadded_rel[esp] + dilationParam.x * dd)  / strideParamIfConvTrans[0] * heightX * widthX + (topYPadded_rel[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1] * widthX 
                        + (topXPadded_rel[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2];
                    maskX_rel[esp] = ((topDPadded_rel[esp] + dilationParam.x * dd) / strideParamIfConvTrans[0]) < depthX &&  ((topDPadded_rel[esp] + dilationParam.x * dd) % strideParamIfConvTrans[0] == 0) &&
                                 ((topYPadded_rel[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1]) < heightX && ((topYPadded_rel[esp] + dilationParam.y * dy) % strideParamIfConvTrans[1] == 0) &&
                                 ((topXPadded_rel[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2]) < widthX &&  ((topXPadded_rel[esp] + dilationParam.z * dx) % strideParamIfConvTrans[2] == 0);
                    #elif defined(CONV2D)
                    kernelOffsetX_rel[esp] = (topYPadded_rel[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0] * widthX + (topXPadded_rel[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1];
                    maskX_rel[esp] = ((topYPadded_rel[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0]) < heightX && ((topYPadded_rel[esp] + dilationParam.x * dy) % strideParamIfConvTrans[0] == 0) &&
                                 ((topXPadded_rel[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1]) < widthX &&  ((topXPadded_rel[esp] + dilationParam.y * dx) % strideParamIfConvTrans[1] == 0);
                    #elif defined(CONV1D)
                    kernelOffsetX_rel[esp] = (topXPadded_rel[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0];
                    maskX_rel[esp] = ((topXPadded_rel[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0]) < widthX && ((topXPadded_rel[esp] + dilationParam.x * dx) % strideParamIfConvTrans[0] == 0);
                    #endif
                #endif // #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                }
            }
            #else
                #define topDPadded_rel topDPadded
                #define topYPadded_rel topYPadded
                #define topXPadded_rel topXPadded
            #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                #define kernelOffsetX_rel kernelOffsetX
                #define maskX_rel maskX
            #endif
            #endif //#if defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && (NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE < SPATIALS_ELEM_PER_THREAD)

            //
            // ... end of computing (if necessary) those arrays:
            //
            // inputChannelOffsetFromGroupByOC_rel, topD,Y,X,Padded_rel and possibly kernelOffsetX_rel, maskX_rel
            // -------------------------------------------------------------------------------------------------------------------

            // Inner product:
            for (uint di = 0; di < CACHE_DEPTH; di++)
            {
                // Reload / broadcast on threads and accumulate:
                DATA_TYPE srcW[OUTCHAN_ELEM_PER_THREAD];
                DATA_TYPE srcX[SPATIALS_ELEM_PER_THREAD];
                // TODOTODO: if NON_UNIFORM_CONVGROUP_PER_OC check if caching to reg first using srcX[SPATIALS_ELEM_PER_THREAD * OUTCHAN_ELEM_PER_THREAD]
                // is faster.

                // Reload weights:
                [unroll]
                for (eoc = 0; eoc < OUTCHAN_ELEM_PER_THREAD; eoc++)
                {
                    uint index = CALCULATE_LDS_LOAD_IDX_W(gtid, eoc, di);
                    srcW[eoc] = LDS_ACCESS_LOAD_W(index);
                }

                // Reload inputs:
                #if !defined(NON_UNIFORM_CONVGROUP_PER_OC)
                [unroll]
                for (esp = 0; esp < SPATIALS_ELEM_PER_THREAD; esp++)
                {
                    uint index = CALCULATE_LDS_LOAD_IDX_X(gtid, esp, di);
                    srcX[esp] = LDS_ACCESS_LOAD_X(index);
                }
                #endif // #if !defined(NON_UNIFORM_CONVGROUP_PER_OC)


                //
                // Accumulate:
                //
                #if !defined(NON_UNIFORM_CONVGROUP_PER_OC)
                [unroll]
                for (eoc = 0; eoc < OUTCHAN_ELEM_PER_THREAD; eoc++)
                    [unroll]
                    for (esp = 0; esp < SPATIALS_ELEM_PER_THREAD; esp++)
                    {
                        //dstA[eoc * OC_AREG_STRIDE + esp] += srcX[esp] * srcW[eoc];
                        dstA[eoc * OC_AREG_STRIDE + esp] = ADD_DATA(dstA[eoc * OC_AREG_STRIDE + esp], MUL_DATA(srcX[esp], srcW[eoc]));
                    }
                #else
                //
                // we have NON_UNIFORM_CONVGROUP_PER_OC, load the input data on the fly in the accumulation loops
                //
                // TODOTODO: if NON_UNIFORM_CONVGROUP_PER_OC check if caching to reg first using srcX[SPATIALS_ELEM_PER_THREAD * OUTCHAN_ELEM_PER_THREAD]
                // before the accumulate loop is faster.
                uint baseInnerElement = di + i;// cf vs SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) + i; : we are NOT storing to LDS here!
                [unroll]
                for (eoc = 0; eoc < OUTCHAN_ELEM_PER_THREAD; eoc++)
                    [unroll]
                    for (esp = 0; esp < SPATIALS_ELEM_PER_THREAD; esp++)
                    {
                        DATA_TYPE inputValFromXptr = DATA_TYPE_ADDITIVE_ZERO;

                        #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                        {
                            //uint readX = batchReadOffset + strideX * SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid);
                            //uint baseIdx = readX + strideX * i + kernelOffsetX[esp]; // now kernelOffsetX includes cornerId !
                            uint baseIdx = batchReadOffset + strideX * (inputChannelOffsetFromGroupByOC_rel[eoc] + baseInnerElement) + kernelOffsetX_rel[esp];
                            bool innerElemInBound = baseInnerElement < numInnerChannels;

                            if (maskX_rel[esp] && innerElemInBound)
                                inputValFromXptr = READ_Xptr(MIN_PATCHBUG(baseIdx, maxXIndex));
                        }
                        #else
                        // CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE case:
                        {
                            // inner index to match kernel CACHE_DEPTH-sized chunk
                            //uint ii = SELECT_FROMTHREADIDS_AS_INNER_FOR_X_LDS_STORE(ti, gtid) + (i + eic * innerThreadIdsStride);
                            uint ii = baseInnerElement;
                            bool inBound = ii < kInnerSliceSize; // that way no need to check ic < channels (or < numInnerChannels actually) below


                            // Note: K1x1 cannot be defined here at the same time that CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE is,
                            // as in that K1x1 case, we can only be caching for different input channels indices,
                            // the spatial dimensions sizes of the kernel all multiply to 1, ie there's only one spatial coord in the kernel.
                            // #if defined(K1x1)
                            // #error "K1x1 shouldn't touch the path where CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE"
                            // #endif
                            // Update: SHADER_API_WEBGPU

                            #if defined(CONV_TRANSPOSE)
                            uint4 strideParamIfConvTrans = _StrideParam;
                            #else
                            uint4 strideParamIfConvTrans = uint4(1,1,1,1);
                            #endif
                            // Note: because of the define above, if we're not using conv transpose,
                            // all / strideParamIfConvTrans subexpressions and all mask modulo ( % ) tests
                            // will all be optimized out by the compiler.
                            // This is just to avoid even more #ifdef blocks below.

                            #if defined(K1x1)
                            uint4 dilationParam = uint4(0,0,0,0);
                            #else
                            uint4 dilationParam = _Dilation;
                            #endif

                            #if defined(CONV3D)
                            uint dd = ((ii / K_width) / K_height) % K_depth;
                            uint ic = ((ii / K_width) / K_height) / K_depth;
                            uint dy = (ii / K_width) % K_height;
                            uint dx = ii % K_width;
                            #elif defined(CONV2D)
                            uint dy = (ii / K_width) % K_height;
                            uint ic = (ii / K_width) / K_height;
                            uint dx = ii % K_width;
                            #elif defined(CONV1D)
                            uint dx = ii % K_width;
                            uint ic = ii / K_width;
                            #endif

                            ic = ic + inputChannelOffsetFromGroupByOC_rel[eoc];

                            #ifdef CONV3D
                            uint kernelOffsetX = (topDPadded_rel[esp] + dilationParam.x * dd)  / strideParamIfConvTrans[0] * heightX * widthX + (topYPadded_rel[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1] * widthX 
                                + (topXPadded_rel[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2];
                            bool maskX = ((topDPadded_rel[esp] + dilationParam.x * dd) / strideParamIfConvTrans[0]) < depthX &&  ((topDPadded_rel[esp] + dilationParam.x * dd) % strideParamIfConvTrans[0] == 0) &&
                                         ((topYPadded_rel[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1]) < heightX && ((topYPadded_rel[esp] + dilationParam.y * dy) % strideParamIfConvTrans[1] == 0) &&
                                         ((topXPadded_rel[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2]) < widthX &&  ((topXPadded_rel[esp] + dilationParam.z * dx) % strideParamIfConvTrans[2] == 0);
                            #elif defined(CONV2D)
                            uint kernelOffsetX = (topYPadded_rel[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0] * widthX + (topXPadded_rel[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1];
                            bool maskX = ((topYPadded_rel[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0]) < heightX && ((topYPadded_rel[esp] + dilationParam.x * dy) % strideParamIfConvTrans[0] == 0) &&
                                         ((topXPadded_rel[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1]) < widthX &&  ((topXPadded_rel[esp] + dilationParam.y * dx) % strideParamIfConvTrans[1] == 0);
                            #elif defined(CONV1D)
                            uint kernelOffsetX = (topXPadded_rel[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0];
                            bool maskX = ((topXPadded_rel[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0]) < widthX && ((topXPadded_rel[esp] + dilationParam.x * dx) % strideParamIfConvTrans[0] == 0);
                            #endif

                            if (maskX && inBound)
                                inputValFromXptr = READ_Xptr(MIN_PATCHBUG(batchReadOffset + strideX * ic + kernelOffsetX, maxXIndex));
                        }
                        #endif

                        // Finally, accumulate:
                        dstA[eoc * OC_AREG_STRIDE + esp] = ADD_DATA(dstA[eoc * OC_AREG_STRIDE + esp], MUL_DATA(inputValFromXptr, srcW[eoc]));
                        //debug
                        //dstA[eoc * OC_AREG_STRIDE + esp] += di == 1 ? srcW[eoc] : 0;
                        //debug
                        //dstA[eoc * OC_AREG_STRIDE + esp] += srcW[eoc];
                    }
                #endif // #if !defined(NON_UNIFORM_CONVGROUP_PER_OC)
            }

            //if (i + CACHE_DEPTH < numInnerChannels)
            //GroupMemoryBarrierWithGroupSync();
        } // reload/broadcast to threads weights/inputs from LDS and MAD/accumulate

        #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
        #if defined(CONV_TRANSPOSE)
        weightOffsetK--;
        #else
        weightOffsetK++;
        #endif
        #endif
    }


#if defined(USEBIAS) && defined(BIAS_AFTER)
    DATA_TYPE biases[OUTCHAN_ELEM_PER_THREAD];
    [unroll]
    for (eoc = 0; eoc < OUTCHAN_ELEM_PER_THREAD; eoc++)
        biases[eoc] = READ_Bptr(MIN_WHEN_NON_D3D(outOutchanThreadBaseOffset + eoc * NUMTHREADS_OUTCHAN, maxBIndex));

#if defined(TRANSPOSE_OUTPUT)

    // Note: tidx tidy eix eiy haven't been renamed but here in TRANSPOSE_OUTPUT, they still mean
    // "thread of spatials" "thread for outchan" etc. respectively even though the threadids actually used might be .y and .x
    // if OUT_OUTCHAN_USES_LOBITS.
    #define OUTPUT_EXPLICIT_XY_ITEM(tidx, tidy, eix, eiy)                                                                                                                     \
        if (((tidx + eix * NUMTHREADS_SPATIALS) < strideO) && ((tidy + eiy * NUMTHREADS_OUTCHAN) < nbFeaturesToCompute))                                                                              \
            WRITE_OUTPUT((tidx + eix * NUMTHREADS_SPATIALS)*features + (tidy + eiy * NUMTHREADS_OUTCHAN) + batchWriteOffset, ApplyFusedActivation(dstA[eiy * OC_AREG_STRIDE + eix] + biases[eiy]));

#else // #if defined(TRANSPOSE_OUTPUT)

    #define OUTPUT_EXPLICIT_XY_ITEM(tidx, tidy, eix, eiy)                                                                                                                     \
        if (((tidx + eix * NUMTHREADS_SPATIALS) < strideO) && ((tidy + eiy * NUMTHREADS_OUTCHAN) < nbFeaturesToCompute))                                                                              \
            WRITE_OUTPUT((tidx + eix * NUMTHREADS_SPATIALS) + (tidy + eiy * NUMTHREADS_OUTCHAN)*strideO + batchWriteOffset, ApplyFusedActivation(dstA[eiy * OC_AREG_STRIDE + eix] + biases[eiy]));

#endif // #if defined(TRANSPOSE_OUTPUT)

#else // #if defined(USEBIAS) && defined(BIAS_AFTER)

#if defined(TRANSPOSE_OUTPUT)

    // Note: tidx tidy eix eiy haven't been renamed but here in TRANSPOSE_OUTPUT, they still mean
    // tidx = "threadids of spatials", 
    // tidy = "threadids for outchan", 
    // eix = "explicit spatial element"
    // eiy = "explicit outchan element" 
    // even though the threadids actually used might be eg tidx => groupThreadID.y and tidy => groupThreadID.x,
    // as is the case with OUT_OUTCHAN_USES_LOBITS.
    #define OUTPUT_EXPLICIT_XY_ITEM(tidx, tidy, eix, eiy)                                                                                                       \
        if (((tidx + eix * NUMTHREADS_SPATIALS) < strideO) && ((tidy + eiy * NUMTHREADS_OUTCHAN) < nbFeaturesToCompute))                                                                \
            WRITE_OUTPUT((tidx + eix * NUMTHREADS_SPATIALS)*features + (tidy + eiy * NUMTHREADS_OUTCHAN) + batchWriteOffset, ApplyFusedActivation(dstA[eiy * OC_AREG_STRIDE + eix]));

#else // #if defined(TRANSPOSE_OUTPUT)

    #define OUTPUT_EXPLICIT_XY_ITEM(tidx, tidy, eix, eiy)                                                                                                       \
        if (((tidx + eix * NUMTHREADS_SPATIALS) < strideO) && ((tidy + eiy * NUMTHREADS_OUTCHAN) < nbFeaturesToCompute))                                                                \
            WRITE_OUTPUT((tidx + eix * NUMTHREADS_SPATIALS) + (tidy + eiy * NUMTHREADS_OUTCHAN)*strideO + batchWriteOffset, ApplyFusedActivation(dstA[eiy * OC_AREG_STRIDE + eix]));

#endif // #if defined(TRANSPOSE_OUTPUT)


#endif // #if defined(USEBIAS) && defined(BIAS_AFTER)




#if 1
    // The unroll is what allows the full static analysis and collapse of all the branches in the
    // macro while at the same time simplifying the output code for all possible permutations
    // of data arrangements.

#if defined(TRANSPOSE_OUTPUT) && defined(OUT_OUTCHAN_USES_LOBITS)
    // See also above "Note: tidx tidy eix eiy"
    // Here we only swap the explicit output order with OUT_OUTCHAN_USES_LOBITS
    // but this shouldn't change much since it is the memory coherence of low bits
    // of threadids that matters, so as long as defined(TRANSPOSE_OUTPUT) == defined(OUT_OUTCHAN_USES_LOBITS),
    // we should be ok
    [unroll]
    for (esp = 0; esp < SPATIALS_ELEM_PER_THREAD; esp++)
        [unroll]
        for (eoc = 0; eoc < OUTCHAN_ELEM_PER_THREAD; eoc++)
        {
            //debug
            //#if DATA_TYPE_SPEC != DATA_TYPE_SPEC_COMPLEX
            //if (outOutchanThreadBaseOffset > 1)
            //    dstA[eoc * OC_AREG_STRIDE + esp] = (K_width * 2) - (outOutchanThreadBaseOffset & ~(1u)) + (outOutchanThreadBaseOffset & 1);
            //#endif
            OUTPUT_EXPLICIT_XY_ITEM(outSpatialsThreadBaseOffset, outOutchanThreadBaseOffset, esp, eoc)
        }


    #if defined(STFT_DOUBLE_OUTPUT)
    {
        // In these variants, we're dispatching enough threads for the unique DFT freq but the required output is
        // prescribed to be double sided still.
        // We generate a 2nd output sequence for that.
        // We must be careful to avoid repeating outputs for 2 frequencies: the DC (freq 0) and the exact Nyquist one,
        // which is present only when the framelength K_width is even.
        //
        // Note that in the later case, with a complex signal, this would be "NbUniqueDFTFreq" is == K_width / 2  +  1,
        // but for a real signal, we set this to NbUniqueDFTFreqTimes2 == (K_width / 2  +  1) * 2.
        //
        // We will avoid anyway the Nyquist frequency (if present) with the >= NbUniqueDFTFreqTimes2 boundcheck (see below).

        // If we have a real signal, the dispatch is organized such that we have twice the number of unique dft freq
        // items to work with as the real and imaginary parts are split

        // Another important point: the addressing of the mirror frequency boils down to
        // nbDFTFreq - originalIdx
        // For the complex signal, we would have simply K_width - originalIdx
        // but for a real signal, we must be careful: we conjugate-mirror **pairs of rows**,
        // not "raw" rows!:
        //
        //      newIdx = (K_width - (originalIdx / 2))*2 + originalIdx %2
        //             = (K_width * 2) - (originalIdx & ~(1u)) + (originalIdx & 1)
        // newOutOutchanThreadBaseOffset = (K_width * 2) - (outOutchanThreadBaseOffset & ~(1u)) + (outOutchanThreadBaseOffset & 1));
        int newOutOutchanPairThreadBaseOffset = ((int)(K_width * 2)) - ((int)(outOutchanThreadBaseOffset & ~(1u)));
        int newOutOutchanThreadBaseOffset = newOutOutchanPairThreadBaseOffset + ((int)outOutchanThreadBaseOffset & 1);

        #if !( (NUMTHREADS_OUTCHAN == 2) \
            || (NUMTHREADS_OUTCHAN == 4) \
            || (NUMTHREADS_OUTCHAN == 8) \
            || (NUMTHREADS_OUTCHAN == 16) \
            || (NUMTHREADS_OUTCHAN == 32) \
            || (NUMTHREADS_OUTCHAN == 64) )
            #error "STFT_DOUBLE_OUTPUT requires NUMTHREADS_OUTCHAN to be at least 2 as output channel values (dft freq) are treated as pairs"
        #endif

        #define STFT_DOUBLE_OUTPUT_DIRECT


        #if defined(STFT_DOUBLE_OUTPUT_DIRECT)
        {
            [unroll]
            for (esp = 0; esp < SPATIALS_ELEM_PER_THREAD; esp++)
            {
                [unroll]
                for (eoc = 0; eoc < OUTCHAN_ELEM_PER_THREAD; eoc++)
                {
                    if ((eoc > 0 || outOutchanThreadBaseOffset > 1)) // avoid the DC freq, this can only be in the very first output channel pair
                    {
                        // Note we need a custom output instead of those usual macros, see below
                        // #define OUTPUT_EXPLICIT_XY_ITEM(tidx, tidy, eix, eiy)
                        // OUTPUT_EXPLICIT_XY_ITEM(outSpatialsThreadBaseOffset, outOutchanThreadBaseOffset, esp, eoc)
                        // dstA[eoc * OC_AREG_STRIDE + esp]


                        // The second frequency lobe output is the conjugate of the first lobe
                        //      IMAG_PART(dstA[eoc * OC_AREG_STRIDE + esp]) = -1.0 * IMAG_PART(dstA[eoc * OC_AREG_STRIDE + esp]);
                        // but our data type here isn't complex so we do:
                        if (outOutchanThreadBaseOffset & 1) // the odd indices have the sin (imaginary) parts
                            dstA[eoc * OC_AREG_STRIDE + esp] = -1.0 * dstA[eoc * OC_AREG_STRIDE + esp];

                        // Note: Doing it like this on the threadid level shouldn't be bad: the reason is that the addresses
                        // generated are still part of the same memory segment just flipped vs the threadids.
                        // If that is a problem, you could also do the mirror on the base address of the block of data the threadgroup is in charge of,
                        // but then, at the threadid level, you would use an intrinsic to "butterfly" the data between them.
                        // Otherwise, we can use shared memory.
                        // But the reason this shouldn't be bad per se is because modern GPUs should be able to still coalesce
                        // the memory transaction regardless of how threads scramble their access as long as they end up as if
                        // in contiguous sectors. eg this capability was introduced very early in nvidia's CUDA compute capability 1.2.

                        // Note the -eoc and >= NbUniqueDFTFreqTimes2 here:
                        if (((outSpatialsThreadBaseOffset + esp * NUMTHREADS_SPATIALS) < strideO) && ((newOutOutchanThreadBaseOffset - (int)eoc * NUMTHREADS_OUTCHAN) >= (int)NbUniqueDFTFreqTimes2))
                            WRITE_OUTPUT((outSpatialsThreadBaseOffset + esp * NUMTHREADS_SPATIALS)*features + (newOutOutchanThreadBaseOffset - eoc * NUMTHREADS_OUTCHAN) + batchWriteOffset, (dstA[eoc * OC_AREG_STRIDE + esp]));
                    }
                }
            }
        }
        #else // STFT_DOUBLE_OUTPUT_DIRECT
        {
            // This shoulnd't be necessary but see note on compute capability 1.2 above in the STFT_DOUBLE_OUTPUT_DIRECT version,
            // if ever needed on some platforms.

            // In this version, we use 
            // In this mode largest cache is CACHE_DEPTH * NB_OUTCHAN_PER_TG = CACHE_DEPTH * (OUTCHAN_ELEM_PER_THREAD * NUMTHREADS_OUTCHAN)
            // eg 16 * (4 * 16) = 1024 entries,
            // when reordering the elements of the threads, we'd like to be able to do the whole threadgroup
            // at the same time, so we need NUMTHREADS_OUTCHAN * NUMTHREADS_SPATIALS, we could also do for all OUTCHAN_ELEM_PER_THREAD
            // before the output burst:
            #if ((CACHE_DEPTH * NB_OUTCHAN_PER_TG) < (NUMTHREADS_OUTCHAN * NUMTHREADS_SPATIALS * OUTCHAN_ELEM_PER_THREAD))
                #error "STFT_DOUBLE_OUTPUT via shared mem: modify output code or variant config, not enough shared mem"
            #endif

            int threadIdOutchan = SELECT_FINAL_OUT_OUTCHAN_THREADIDS(ti, gtid);
            int threadIdSpatial = SELECT_FINAL_OUT_SPATIALS_THREADIDS(ti, gtid);
            
            // (NbUniqueDFTFreqTimes2 - 1) is the absolute last valid index in the first lobe
            // and if the TG would do the whole DFT frame, we would do
            // int ldsOutchanPairThreadOffset = ((int)(NbUniqueDFTFreqTimes2 - 2)) - ((int)(threadIdOutchan & ~(1u)));
            // but we might have multiple TG in the grid for one DFT frame so we must use the values our TG handles:
            //      int ldsOutchanPairThreadOffset = int(OUTCHAN_ELEM_PER_THREAD * NUMTHREADS_OUTCHAN - 1) - ((int)(threadIdOutchan & ~(1u)));
            // also we will do - eoc * NUMTHREADS_OUTCHAN at each iteration

            //  int ldsOutchanPairLastOffset = int(OUTCHAN_ELEM_PER_THREAD * NUMTHREADS_OUTCHAN - 2)
            //  ...this would be for one spatial index 
            // not really needed
            int ldsOutchanPairThreadOffset = int(NUMTHREADS_OUTCHAN - 2) - ((int)(threadIdOutchan & ~(1u)));
            int ldsOutchanThreadOffset = ldsOutchanPairThreadOffset + ((int)threadIdOutchan & 1);
            // so ldsOutchanThreadOffset maps from eg if 8 threadids for oc, tid.x = 0 1 2 3 4 5 6 7 to tid.x = 6 7 4 5 2 3 0 1

            //int ldsReservedLastPairOffset = (OUTCHAN_ELEM_PER_THREAD * NUMTHREADS_SPATIALS * NUMTHREADS_OUTCHAN) - 2;
            // not really needed

            [unroll]
            for (esp = 0; esp < SPATIALS_ELEM_PER_THREAD; esp++)
            {
                [unroll]
                for (eoc = 0; eoc < OUTCHAN_ELEM_PER_THREAD; eoc++)
                {
                    //if (outOutchanThreadBaseOffset & 1) // the odd indices have the sin (imaginary) parts
                    if (threadIdOutchan & 1) // the odd indices have the sin (imaginary) parts
                                dstA[eoc * OC_AREG_STRIDE + esp] = -1.0 * dstA[eoc * OC_AREG_STRIDE + esp];

                    // uint outOutchanThreadBaseOffset =  outTGOutchanBaseOffset + SELECT_FINAL_OUT_OUTCHAN_THREADIDS(ti, gtid);
                    // int newOutOutchanPairThreadBaseOffset = ((int)(K_width * 2)) - ((int)(outOutchanThreadBaseOffset & ~(1u)));
                    // int newOutOutchanThreadBaseOffset = newOutOutchanPairThreadBaseOffset + ((int)outOutchanThreadBaseOffset & 1);
                    //
                    // In shared memory, since we want to store the chunk of OUTCHAN_ELEM_PER_THREAD * NUMTHREADS_OUTCHAN in the order
                    // it is going to be output, we reserve that (we don't need potentially 2 or 4 of them but this simplifies the code)
                    // and write in reverse order so that the TG can then reload in order before the final output:
                    //
                    // int ldsOutchanPairThreadOffset = ((int)(NbUniqueDFTFreqTimes2)) - ((int)(threadIdOutchan & ~(1u)));
                    // int ldsOutchanThreadOffset = ldsOutchanPairThreadOffset + ((int)threadIdOutchan & 1);

                    //int ldsOutChanIdx = (ldsOutchanThreadOffset - (int)(eoc * NUMTHREADS_OUTCHAN));
                    // ...but in the lds we will pack for different spatial (frames) before a larger jump for each explicit oc (eoc)
                    // of each group of threads:

                    //if (ldsOutChanIdx >= 0)
                    LDS_W[(eoc) * (NUMTHREADS_OUTCHAN * NUMTHREADS_SPATIALS) + (threadIdSpatial * NUMTHREADS_OUTCHAN) + ldsOutchanThreadOffset] = dstA[eoc * OC_AREG_STRIDE + esp];
                }

                // Now output to global mem
                [unroll]
                for (int eoc_i = (OUTCHAN_ELEM_PER_THREAD - 1); eoc_i >= 0; eoc_i--)
                {
                    
                    // Note that now the threadids access is in order: note usage of the simple threadIdOutchan.
                    // Also note that the loop eoc var goes in reverse as the last (valid) NUMTHREADS_OUTCHAN-sized block of  values we flipped
                    // and stored to lds is assumed the first block of values of the conjugate DFT lobe, again provided those values are valid / inbound.
                    //
                    // We keep the loop constant and simple and leave it to the bound check to select the first valid block / threads of that block,
                    // instead of doing some modulo calculations to start the eoc loop at the NUMTHREADS_OUTCHAN-sized that contains the start of the
                    // valid data. That start index is NbUniqueDFTFreqTimes2, which is the index right after the non-conjugate lobe:
                    //
                    // In LDS, from low to high memory we have something like eg 2 SPATIALS_ELEM_PER_THREAD and 2 OUTCHAN_ELEM_PER_THREAD):
                    //
                    // [NUMTHREADS_OUTCHAN data for esp0][NUMTHREADS_OUTCHAN data for esp1][NUMTHREADS_OUTCHAN data for esp0][NUMTHREADS_OUTCHAN data for esp1]
                    //                             ^^^^^^^^^^^^                                                         ^^^^^^^^^^^^
                    //                      ...all this for eoc = 0                                                 all this for eoc = 1
                    //
                    // Note how the esp and NUMTHREADS_OUTCHAN-sized blocks are all in order, it's only the NUMTHREADS_OUTCHAN-data itself that has been
                    // flipped, but obviously the eoc blocks in shared mem are going to be output starting with eoc = (OUTCHAN_ELEM_PER_THREAD - 1), here eoc = 1
                    // as the one with lowest addresses in global memory, then the eoc-- block after that etc.
                    
                    // uint outTGOutchanBaseOffset = (NUMTHREADS_OUTCHAN * SELECT_FINAL_OUT_OUTCHAN_THREADGROUP_ID(groupID)) * OUTCHAN_ELEM_PER_THREAD;
                    // uint outOutchanThreadBaseOffset =  outTGOutchanBaseOffset + SELECT_FINAL_OUT_OUTCHAN_THREADIDS(ti, gtid);
                    //
                    // int finalOutChanIdx = (one past the last valid address that we take care of) - (the number of NUMTHREADS_OUTCHAN-sized blocks we have to do * NUMTHREADS_OUTCHAN)
                    //                       + tid
                    //
                    // int finalOutChanIdx = ((int)(K_width * 2)) - ((int)outTGOutchanBaseOffset) - ((int)(eoc_i+1) * NUMTHREADS_OUTCHAN) + threadIdOutchan
                    //
                    // Note that because NUMTHREADS_OUTCHAN is supposed to be at elast a multiple of 2 (pow2 actually), all address calculations up to threadIdOutchan
                    // are even, and since now threadids for outchans output in order of threadids, we don't have to deal with the pairThreadBaseOffset like we
                    // did in the reversing pass above.
                    //
                    // There is one last complication: since the eoc = 0 block has the first 2 threads handling the DC component which we loaded into LDS
                    // and we count everything relative to that, that DC component is actually aligned to (K_width * 2) + 2 except we don't want to output it,
                    // it isn't part of the conjugate lobe.
                    // So we add + 2 to the finalOutChanIdx expression above AND if handling the eoc_i == 0 block, check that our TG doesn't actually land
                    // on that address (note also the loop is unrolled, this should not be tested at each iteration, only on the eoc_i == 0 block).
                    // This complication can be handled either in the LDS loading phase or here, we chose here.
                    int finalOutChanIdx = ((int)(K_width * 2)) - ((int)outTGOutchanBaseOffset) - ((int)(eoc_i+1) * NUMTHREADS_OUTCHAN) + threadIdOutchan + 2;

                    if ( (finalOutChanIdx >= (int)(NbUniqueDFTFreqTimes2)) && (eoc_i > 0 || finalOutChanIdx < ((int)(K_width * 2))) )
                    {
                        float val = LDS_W[(eoc_i) * (NUMTHREADS_OUTCHAN * NUMTHREADS_SPATIALS) + (threadIdSpatial * NUMTHREADS_OUTCHAN) + threadIdOutchan];

                        if ((outSpatialsThreadBaseOffset + esp * NUMTHREADS_SPATIALS) < strideO)
                                WRITE_OUTPUT((outSpatialsThreadBaseOffset + esp * NUMTHREADS_SPATIALS)*features + (finalOutChanIdx) + batchWriteOffset, val);
                    }
                }
            }
        }
        #endif // STFT_DOUBLE_OUTPUT_DIRECT
    }
    #endif // #if defined(STFT_DOUBLE_OUTPUT)


#else
    [unroll]
    for (eoc = 0; eoc < OUTCHAN_ELEM_PER_THREAD; eoc++)
        [unroll]
        for (esp = 0; esp < SPATIALS_ELEM_PER_THREAD; esp++)
            OUTPUT_EXPLICIT_XY_ITEM(outSpatialsThreadBaseOffset, outOutchanThreadBaseOffset, esp, eoc)
#endif // transpose_output

#else // #if 1
//debug

    if (gtid.y > 0)
        return;

    //if (gtid.x > 5)
    //    return;

    if (groupID.x == 0)
    {
            //for (int ttt; ttt < NB_ACCU_REG_PER_THREAD; ttt++)
            //    Optr[gtid.x * NB_ACCU_REG_PER_THREAD + ttt] = dstA[ttt];
            for (int ttt; ttt < NB_ACCU_REG_PER_THREAD; ttt++)
                //Optr[gtid.x + ttt * NUMTHREADS_X] = NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE; //dstA[ttt];
                //Optr[gtid.x + ttt * NUMTHREADS_X] = NUMTHREADS_SPATIALS * SPATIALS_ELEM_PER_THREAD; //dstA[ttt];
                //Optr[gtid.x] = dstA[0];
                WRITE_OUTPUT(gtid.x + ttt * NUMTHREADS_X, dstA[ttt]);
    }


    /*
    [unroll]
    for (eoc = 0; eoc < 1; eoc++) // OUTCHAN_ELEM_PER_THREAD
        [unroll]
        for (esp = 0; esp < 1; esp++) // SPATIALS_ELEM_PER_THREAD
            OUTPUT_EXPLICIT_XY_ITEM(outSpatialsThreadBaseOffset, outOutchanThreadBaseOffset, esp, eoc)
    */

#endif


}