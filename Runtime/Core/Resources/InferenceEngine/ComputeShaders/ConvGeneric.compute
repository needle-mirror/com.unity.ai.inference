#pragma kernel Conv3D_Generic MainName=Conv3D_Generic CONV3D NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16
#pragma kernel Conv2D_Generic MainName=Conv2D_Generic CONV2D NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16
#pragma kernel Conv1D_Generic MainName=Conv1D_Generic CONV1D NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16

#pragma kernel Conv3D_1x1_Generic MainName=Conv3D_1x1_Generic CONV3D K1x1 NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16
#pragma kernel Conv2D_1x1_Generic MainName=Conv2D_1x1_Generic CONV2D K1x1 NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16
#pragma kernel Conv1D_1x1_Generic MainName=Conv1D_1x1_Generic CONV1D K1x1 NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16

#pragma kernel ConvTranspose3D_Generic MainName=ConvTranspose3D_Generic CONV_TRANSPOSE CONV3D NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16
#pragma kernel ConvTranspose2D_Generic MainName=ConvTranspose2D_Generic CONV_TRANSPOSE CONV2D NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16
#pragma kernel ConvTranspose1D_Generic MainName=ConvTranspose1D_Generic CONV_TRANSPOSE CONV1D NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16

//Not tested yet:
#pragma kernel ConvTranspose3D_1x1_Generic MainName=ConvTranspose3D_1x1_Generic CONV_TRANSPOSE CONV3D K1x1 NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16
#pragma kernel ConvTranspose2D_1x1_Generic MainName=ConvTranspose2D_1x1_Generic CONV_TRANSPOSE CONV2D K1x1 NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16
#pragma kernel ConvTranspose1D_1x1_Generic MainName=ConvTranspose1D_1x1_Generic CONV_TRANSPOSE CONV1D K1x1 NUMTHREADS_X=16 NUMTHREADS_Y=8 ELEM_PER_THREAD_X=4 ELEM_PER_THREAD_Y=8 CACHE_DEPTH=16


#pragma multi_compile_local _ USEBIAS
#pragma multi_compile_local _ UNIT_STRIDES

// Grouped conv:
#pragma multi_compile_local _ GROUP_OC_PER_GROUP_LT_ANYVARIANTS GROUP_OC_PER_GROUP_MINDIV_2 GROUP_OC_PER_GROUP_MINDIV_4 GROUP_OC_PER_GROUP_MINDIV_8 GROUP_OC_PER_GROUP_MINDIV_16 GROUP_OC_PER_GROUP_MINDIV_32 GROUP_OC_PER_GROUP_MINDIV_64
// for now we just need those 2 anyway:
#pragma multi_compile_local _ GROUP_OC_PER_GROUP_LT_ANYVARIANTS GROUP_OC_PER_GROUP_MINDIV_64
// Explanation:
//
// You need to enable the variants corresponding to each config having NB_OUTCHAN_PER_TG evenly dividing the number output channels per convolution group
// that the variant refers to.
// This is because only one matters vs each config:
// eg if NB_OUTCHAN_PER_TG == 64 for a particular compiled kernel, only GROUP_OC_PER_GROUP_MINDIV_64 is relevant.
// C# code should drive the keyword knowing NB_OUTCHAN_PER_TG == (ELEM_PER_THREAD_Y*NUMTHREADS_Y) of the particular kernel it selects.
//
// Also, if none of those keywords are enabled, then it is assumed there's no convolution group (# groups == 1) involved.

// #pragma skip_variants GROUP_OC_PER_GROUP_LT_ANYVARIANTS
// #pragma skip_variants GROUP_OC_PER_GROUP_MINDIV_2
// #pragma skip_variants GROUP_OC_PER_GROUP_MINDIV_4
// #pragma skip_variants GROUP_OC_PER_GROUP_MINDIV_8
// #pragma skip_variants GROUP_OC_PER_GROUP_MINDIV_16
// #pragma skip_variants GROUP_OC_PER_GROUP_MINDIV_32
// #pragma skip_variants GROUP_OC_PER_GROUP_MINDIV_64 // all our current variants use NUMTHREADS_Y=8 ELEM_PER_THREAD_Y=8, so NB_OUTCHAN_PER_TG == 64

#if defined(GROUP_OC_PER_GROUP_LT_ANYVARIANTS) || defined(GROUP_OC_PER_GROUP_MINDIV_2) || defined(GROUP_OC_PER_GROUP_MINDIV_4) || defined(GROUP_OC_PER_GROUP_MINDIV_8) || defined(GROUP_OC_PER_GROUP_MINDIV_16) || defined(GROUP_OC_PER_GROUP_MINDIV_32) || defined(GROUP_OC_PER_GROUP_MINDIV_64)

#define GROUPS_ENABLED

#if defined(GROUP_OC_PER_GROUP_LT_ANYVARIANTS)
    #define GROUP_NB_OC_CHAN_DIVIDER 1
#elif defined(GROUP_OC_PER_GROUP_MINDIV_2)
    #define GROUP_NB_OC_CHAN_DIVIDER 2
#elif defined(GROUP_OC_PER_GROUP_MINDIV_4)
    #define GROUP_NB_OC_CHAN_DIVIDER 4
#elif defined(GROUP_OC_PER_GROUP_MINDIV_8)
    #define GROUP_NB_OC_CHAN_DIVIDER 8
#elif defined(GROUP_OC_PER_GROUP_MINDIV_16)
    #define GROUP_NB_OC_CHAN_DIVIDER 16
#elif defined(GROUP_OC_PER_GROUP_MINDIV_32)
    #define GROUP_NB_OC_CHAN_DIVIDER 32
#elif defined(GROUP_OC_PER_GROUP_MINDIV_64)
    #define GROUP_NB_OC_CHAN_DIVIDER 64
#endif

#endif


// Note: this is bogus (actual values will be taken from the pragma definitions above) but prevents the compiler IPC from crashing,
// ie this is to bypass a bug.
#if !defined(NUMTHREADS_X)
#define NUMTHREADS_X 8
#endif
#if !defined(NUMTHREADS_Y)
#define NUMTHREADS_Y 8
#endif
#if !defined(ELEM_PER_THREAD_Y)
#define ELEM_PER_THREAD_Y 1
#endif
#if !defined(ELEM_PER_THREAD_X)
#define ELEM_PER_THREAD_X 1
#endif
#if !defined(CACHE_DEPTH)
#define CACHE_DEPTH 8
#endif

StructuredBuffer<float> Xptr;
StructuredBuffer<float> Kptr;
StructuredBuffer<float> Bptr;
RWStructuredBuffer<float> Optr;

uint O_batch;
uint O_channels, O_depth, O_height, O_width;
uint X_channels, X_depth, X_height, X_width;
uint K_depth, K_height, K_width;

uint X_channelsPerGroup; // if conv with group > 1, # of input channels per group
uint O_channelsPerGroup; // if conv with group > 1, # of output channels per group

uint4 _Pad;
uint4 _Stride;
uint4 _Dilation;
float _MinValue;

#if defined(UNIT_STRIDES)
#define _StrideParam uint4(1,1,1,1)
#else
#define _StrideParam _Stride
#endif


// ---------------------------------------------------------------------------------------------------------------------------------------
// Main config:

// Check list when adding a variant:
//  -switch on/off NB_THREADIDS_FROM_NUMTHREADS_XY
//  -force always on/off CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE regardless of K1x1


//#define BIAS_AFTER
//..slightly faster before

// faster to define this:
#define NB_THREADIDS_FROM_NUMTHREADS_XY

// debug, to force compilation of path to check for compilation errors:
//#define GROUPS_ENABLED


#if !defined(K1x1)
#define CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE
#endif

// #if defined(K1x1)
// #undef K1x1
// #endif

#if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) && defined(SHADER_API_WEBGPU)
// For SHADER_API_WEBGPU even when K1x1 (ie no spatial window), see ConvTranspose.compute, we still enable the caching
// path as if there was a spatial window, as this removes the triple for loop for it (but needs in turn to recover the individual window coord from a linear index)
#define CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE
#endif


// ---------------------------------------------------------------------------------------------------------------------------------------

// To quickly test variants

//#define CACHE_DEPTH 16 // 32


// #if 0
// #define NUMTHREADS_X 64
// #define NUMTHREADS_Y 4
// #define ELEM_PER_THREAD_X 1
// #define ELEM_PER_THREAD_Y 1

// #elif !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)

// //#define BIAS_AFTER

// #define NUMTHREADS_X 16
// #define NUMTHREADS_Y 8
// #define ELEM_PER_THREAD_X 4
// #define ELEM_PER_THREAD_Y 8

// #else
// //#define BIAS_AFTER

// #define NUMTHREADS_X 16
// #define NUMTHREADS_Y 8
// #define ELEM_PER_THREAD_X 4
// #define ELEM_PER_THREAD_Y 8
// #endif


// ------------------------------------------------------------------
// Main threadgroup config
#define NB_THREADS_PER_TG (NUMTHREADS_X*NUMTHREADS_Y)
#define LOG2_NB_THREADS_PER_TG uint(log2(NB_THREADS_PER_TG))

// Output channels and spatial elements per TG
#define NB_OUTCHAN_PER_TG (ELEM_PER_THREAD_Y*NUMTHREADS_Y)
#define LOG2_NB_OUTCHAN_PER_TG uint(log2(NB_OUTCHAN_PER_TG))

#define NB_SPATIALS_PER_TG (ELEM_PER_THREAD_X*NUMTHREADS_X)
#define LOG2_NB_SPATIALS_PER_TG uint(log2(NB_SPATIALS_PER_TG))
// ------------------------------------------------------------------

// Bug somewhere in compiler pipeline: simply this
//#if (NB_OUTCHAN_PER_TG) / GROUP_NB_OC_CHAN_DIVIDER == 0
// will cause this:
//Shader error in 'ConvGeneric.compute': Internal error communicating with the shader compiler process.  Please report a bug including this shader and the editor log. Error code 0x80000004 (Not connected).
// Valid arithmetic expressions with a minus ("-") sign will give an "Invalid conditional expression" error...
//#endif
// The following can at least fix what seems to be a division by 0 crash:
//#ifndef GROUP_NB_OC_CHAN_DIVIDER
//#define GROUP_NB_OC_CHAN_DIVIDER 1
//#endif
//
// There's no modulo in the preprocessor ops anyway so for now,
// just add a equality check, each config can be validated manually if needed
#if defined(GROUPS_ENABLED)
#if NB_OUTCHAN_PER_TG != GROUP_NB_OC_CHAN_DIVIDER
    #define NON_UNIFORM_CONVGROUP_PER_OC
#endif
#endif


#if !defined(SHADER_API_D3D11)
#define MIN_WHEN_NON_D3D(val, maxidx) (min((val), (maxidx)))
#else
#define MIN_WHEN_NON_D3D(val, maxidx) (val)
#endif

//#if defined(SHADER_API_METAL) || defined(UNITY_PLATFORM_OSX) || defined(UNITY_PLATFORM_IOS)
#if defined(UNITY_PLATFORM_IOS)
#define MIN_PATCHBUG(val, maxidx) (min((val), (maxidx)))
#else
#define MIN_PATCHBUG(val, maxidx) (val)
#endif


#if defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
#define IF_ONLY_INCHAN_ELSE(a, b) (b)
#else
#define IF_ONLY_INCHAN_ELSE(a, b) (a)
#endif

#if defined(CONV_TRANSPOSE)
#define IF_CONV_TRANSPOSE_ELSE(a, b) (a)
#else
#define IF_CONV_TRANSPOSE_ELSE(a, b) (b)
#endif

#if defined(GROUPS_ENABLED)
#define IF_HAVE_GROUPS_ELSE(a, b) (a)
#else
#define IF_HAVE_GROUPS_ELSE(a, b) (b)
#endif

#if defined(NON_UNIFORM_CONVGROUP_PER_OC)
#define IF_NON_UNIFORM_GROUPS_ELSE(a, b) (a)
#else
#define IF_NON_UNIFORM_GROUPS_ELSE(a, b) (b)
#endif


#if defined(GROUPS_ENABLED) && defined(CONV_TRANSPOSE)
#define CONV_TRANSPOSE_WITH_GROUPS
#define IF_CONV_TRANSPOSE_GROUPS(a, b) (a)
#else
#define IF_CONV_TRANSPOSE_GROUPS(a, b) (b)
#endif


#define NB_BANKS 32
#define LOG2_NB_BANKS 5


// Main derived config that makes the rest of the code generic:
//
// NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE
// NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE
//
// SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE
// SELECT_FROMTHREADIDS_AS_INCHAN_FOR_W_LDS_STORE
//
// NB_EXPLICIT_INCHAN_VAL_NEEDED_W_LDS_STORE
//
// NB_THREADIDS_FOR_INCHAN_AND_OUTCHAN_W_LDS_STORE
// eg stride when doing multiple load/store in LDS   (using the counter )
//
// IF_THREAD_DOING_W_LDS_STORE
// ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE
// ->in the original conv scheme just "ti" because all threads index into LDS, high part are for the inner cache chunk,
// low part is for all the output channels
//
// NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE
// NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE
//
// SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE
// SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE
//
// NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE
//
// NB_THREADIDS_FOR_INCHAN_AND_SPATIALS_X_LDS_STORE
//
// IF_THREAD_DOING_X_LDS_STORE
// ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE
//
//
// need also (then not enough threadids left for inner chunk, used in inner product sum)
//    NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE
//    NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE


#define LDS_W_STRICT_SIZE (CACHE_DEPTH * NB_OUTCHAN_PER_TG)
#define LDS_X_STRICT_SIZE (CACHE_DEPTH * NB_SPATIALS_PER_TG)
#define LDS_W_FINAL_SIZE (LDS_W_STRICT_SIZE + (LDS_W_STRICT_SIZE >> LOG2_NB_BANKS))
#define LDS_X_FINAL_SIZE (LDS_X_STRICT_SIZE + (LDS_X_STRICT_SIZE >> LOG2_NB_BANKS))
#define LDS_BCAO(off) (off + (off >> LOG2_NB_BANKS)) // bank conflict avoidance offset


#if defined(NB_THREADIDS_FROM_NUMTHREADS_XY)
#define USE_ALTERNATE_LDS_ORG_X // TODO investigate (hunch: drvopt), saves 2ms / 40ms
#define USE_ALTERNATE_LDS_ORG_W // TODO investigate, saves 5-6 / 42ms
#endif

#ifdef USE_ALTERNATE_LDS_ORG_W

groupshared float LDS_W[NB_OUTCHAN_PER_TG * (CACHE_DEPTH+1)];
// ITE = inner (kernel inputchannel and possibly flattened spatial) index, threadid for output indices, explicit output channel
#define LDS_ACCESS_W_ITE(ik, ytid, eoc) LDS_W[(ik) * (NUMTHREADS_Y * ELEM_PER_THREAD_Y + 1) + (ytid) * ELEM_PER_THREAD_Y + (eoc)]

#else // #ifdef USE_ALTERNATE_LDS_ORG_W

groupshared float LDS_W[LDS_W_FINAL_SIZE];
#define LDS_ACCESS_W_BCA(offset) LDS_W[LDS_BCAO(offset)]
// slower?
// #define LDS_ACCESS_W(offset) LDS_ACCESS_W_BCA(offset) // LDS_W[offset]
#define LDS_ACCESS_W(offset) LDS_W[offset]

#endif // #ifdef USE_ALTERNATE_LDS_ORG_W



#if !defined(NON_UNIFORM_CONVGROUP_PER_OC)
// If defined(NON_UNIFORM_CONVGROUP_PER_OC), for now we don't cache for multiple different groups
// as they can't uniformly be broadcasted the same weights.
    #ifdef USE_ALTERNATE_LDS_ORG_X

    groupshared float LDS_X[NB_SPATIALS_PER_TG * (CACHE_DEPTH+0)];
    // ITE = inner (kernel inputchannel and possibly flattened spatial) index, threadid for output spatial indices, explicit spatial indices
    #define LDS_ACCESS_X_ITE(ik, xtid, esp) LDS_X[(ik) * (NUMTHREADS_X * ELEM_PER_THREAD_X) + (xtid) * ELEM_PER_THREAD_X + (esp)]

    #else // #ifdef USE_ALTERNATE_LDS_ORG_X

    groupshared float LDS_X[LDS_X_FINAL_SIZE];
    #define LDS_ACCESS_X_BCA(offset) LDS_X[LDS_BCAO(offset)]
    // slower?
    // #define LDS_ACCESS_X(offset) LDS_ACCESS_X_BCA(offset) // LDS_X[offset]
    #define LDS_ACCESS_X(offset) LDS_X[offset]

    #endif // #ifdef USE_ALTERNATE_LDS_ORG_X
#endif // #if !defined(NON_UNIFORM_CONVGROUP_PER_OC)


// ------------------------------------------------------------------
// Main threadgroup config
// #define NB_THREADS_PER_TG (NUMTHREADS_X*NUMTHREADS_Y)
// #define LOG2_NB_THREADS_PER_TG uint(log2(NB_THREADS_PER_TG))

// // Output channels and spatial elements per TG
// #define NB_OUTCHAN_PER_TG (ELEM_PER_THREAD_Y*NUMTHREADS_Y)
// #define LOG2_NB_OUTCHAN_PER_TG uint(log2(NB_OUTCHAN_PER_TG))

// #define NB_SPATIALS_PER_TG (ELEM_PER_THREAD_X*NUMTHREADS_X)
// #define LOG2_NB_SPATIALS_PER_TG uint(log2(NB_SPATIALS_PER_TG))
// ------------------------------------------------------------------

// ------------------------------------------------------------------
// Weight LDS loads

// assert NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE <= NB_THREADS_PER_TG
// as we dont do any multiple explicit loads for output channels loads in LDS:

// #if NB_OUTCHAN_PER_TG > NB_THREADS_PER_TG
    // #error Having NB_OUTCHAN_PER_TG > NB_THREADS_PER_TG needs additional loops in LDS W/X load/stores, and probably not useful.
// #endif

#if defined(NB_THREADIDS_FROM_NUMTHREADS_XY)
    #define NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE NUMTHREADS_Y
#else
    #if NB_OUTCHAN_PER_TG > NB_THREADS_PER_TG
        #define NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE NB_THREADS_PER_TG
    #else
        #define NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE NB_OUTCHAN_PER_TG
    #endif
#endif

#if !( ((NB_OUTCHAN_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 1) \
    || ((NB_OUTCHAN_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 2) \
    || ((NB_OUTCHAN_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 4) \
    || ((NB_OUTCHAN_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 8) \
    || ((NB_OUTCHAN_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 16) \
    || ((NB_OUTCHAN_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == 32) )
#error NB_OUTCHAN_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE should be a power of 2
#endif


#define NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE (NB_OUTCHAN_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE)


#define LOG2_NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE uint(log2(NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE))

#define NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE (NB_THREADS_PER_TG/NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE)


#if defined(NB_THREADIDS_FROM_NUMTHREADS_XY)
    #define SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) (gtid.y)
    #define SELECT_FROMTHREADIDS_AS_INCHAN_FOR_W_LDS_STORE(ti, gtid) (gtid.x)
#else
    #define SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) (ti & (NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE-1))
    #define SELECT_FROMTHREADIDS_AS_INCHAN_FOR_W_LDS_STORE(ti, gtid) (ti >> LOG2_NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE)
#endif

#if CACHE_DEPTH >= NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE

    #define NB_EXPLICIT_INCHAN_VAL_NEEDED_W_LDS_STORE (CACHE_DEPTH / NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE)
    // ...again CACHE_DEPTH should be pow2, and eg we have 16 / (256/64) = 16/4 = 4
    // see assert above and also, assert CACHE_DEPTH >= NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE
    // or test and if <, need also an additional test in IF_THREAD_DOING_W_LDS_STORE as we have more threads
    // than required to load the input channels even!

    // #define SEL_HIGHTHREADS_AS_INCHAN_FOR_WEIGHTS_LDS_STORE(ti, gtid) (ti & (NB_THREADS_PER_TG-NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE))
    // ...this is because *remaining* high threadids are used straight for input channels
    // and the number of threads to be used for output channels are the amount we need to output,
    // the **MASK** for the later is (NB_THREADS_PER_TG-NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE): we dont use NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE
    // since this is not the mask, we actually want to leave the bits in the same position!

    // So yeah right now in this scheme we (another assert) assume # output channels is always loaded all using threadids,
    // never explicitly.

    // #define ADD_WEIGHTS_LDS_STORE_IDX_FROM_THREAD(ti, gtid) (SEL_HIGHTHREADS_AS_INCHAN_FOR_WEIGHTS_LDS_STORE(ti, gtid) + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid))
    // ...but in that case just use ti directly!!
    // ie SEL_HIGHTHREADS_AS_INCHAN_FOR_WEIGHTS_LDS_STORE(ti, gtid) + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) == ti !!


    #define NB_THREADIDS_FOR_INCHAN_AND_OUTCHAN_W_LDS_STORE (NB_THREADS_PER_TG)
    // assert (NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) == NB_THREADS_PER_TG
    #if (NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) != NB_THREADS_PER_TG
    #error (NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) should be == NB_THREADS_PER_TG
    #endif

    // TODO: will no longer be needed once we actually not hardcode ti & 63 for SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE !!
    // ACTUALLY NO case still useful: CACHE_DEPTH is not as high as the number of leftover threads so still need to keep excessive
    // threads out of the loading code block!
    #define IF_THREAD_DOING_W_LDS_STORE(ti, gtid) // here passthrough, ie if(true), as CACHE_DEPTH >= NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE

    #if defined(NB_THREADIDS_FROM_NUMTHREADS_XY)
        #define ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE(ti, gtid) (gtid.y)  // TODOTODO CUSTOM! combine x and y
    #else
        #define ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE(ti, gtid) (ti)
    #endif

#else // #if CACHE_DEPTH >= NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE

    // Here CACHE_DEPTH < NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE
    // so we have more threads than needed to load weights in the LDS:

    #define NB_EXPLICIT_INCHAN_VAL_NEEDED_W_LDS_STORE 1
    //#define SEL_HIGHTHREADS_AS_INCHAN_FOR_WEIGHTS_LDS_STORE(ti, gtid) (ti & (NB_THREADS_PER_TG-NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE))
    // This would be to work as a mask like the original (ti & 0x1C0)


    #define NB_THREADIDS_FOR_INCHAN_AND_OUTCHAN_W_LDS_STORE (CACHE_DEPTH * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE)

    #if defined(NB_THREADIDS_FROM_NUMTHREADS_XY)
        #define IF_THREAD_DOING_W_LDS_STORE(ti, gtid) if (gtid.x < CACHE_DEPTH) // x threadids are used for inchan
        #define ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE(ti, gtid) (gtid.y)  // TODOTODO CUSTOM! combine x and y
    #else
        #define IF_THREAD_DOING_W_LDS_STORE(ti, gtid) if (ti < NB_THREADIDS_FOR_INCHAN_AND_OUTCHAN_W_LDS_STORE) // high threads are used for inchan, hence the ti < test
        // Guarded version not really needed if using the IF_THREAD_DOING_W_LDS_STORE :
        // #define ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE(ti, gtid) (ti & (NB_THREADIDS_FOR_INCHAN_AND_OUTCHAN_W_LDS_STORE - 1))
        #define ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE(ti, gtid) (ti)
    #endif

#endif // #if CACHE_DEPTH >= NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE

#define LOG2_NB_THREADIDS_FOR_INCHAN_AND_OUTCHAN_W_LDS_STORE uint(log2(NB_THREADIDS_FOR_INCHAN_AND_OUTCHAN_W_LDS_STORE))

// ------------------------------------------------------------------

// ------------------------------------------------------------------
// X Input LDS loads

// assert NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE <= NB_THREADS_PER_TG
// as we dont do any multiple explicit loads for spatial dimensions elements (from the input X) loads in LDS:
//#define NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE min(NB_THREADS_PER_TG, NB_SPATIALS_PER_TG)
// #if NB_SPATIALS_PER_TG > NB_THREADS_PER_TG
    // #error Having NB_SPATIALS_PER_TG > NB_THREADS_PER_TG needs additional loops in LDS W/X load/stores, and probably not useful.
// #endif



#if defined(NB_THREADIDS_FROM_NUMTHREADS_XY)
    #define NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE NUMTHREADS_X
#else
    #if NB_SPATIALS_PER_TG > NB_THREADS_PER_TG
        #define NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE NB_THREADS_PER_TG
    #else
        #define NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE NB_SPATIALS_PER_TG
    #endif
#endif

#if !( ((NB_SPATIALS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 1) \
    || ((NB_SPATIALS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 2) \
    || ((NB_SPATIALS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 4) \
    || ((NB_SPATIALS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 8) \
    || ((NB_SPATIALS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 16) \
    || ((NB_SPATIALS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == 32) )
#error NB_SPATIALS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE should be a power of 2
#endif

#define NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE (NB_SPATIALS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE)

#define LOG2_NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE uint(log2(NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE))

#define NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE (NB_THREADS_PER_TG/NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE)


#if defined(NB_THREADIDS_FROM_NUMTHREADS_XY)
    #define SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid) (gtid.x)
    #define SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid) (gtid.y)
#else
    #define SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid) (ti & (NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE-1))
    #define SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid)(ti >> LOG2_NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE)
#endif


#if CACHE_DEPTH >= NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE

    #define NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE (CACHE_DEPTH / NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE)

    #define NB_THREADIDS_FOR_INCHAN_AND_SPATIALS_X_LDS_STORE (NB_THREADS_PER_TG)
    // assert (NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == NB_THREADS_PER_TG
    #if (NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) != NB_THREADS_PER_TG
    #error (NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) should be == NB_THREADS_PER_TG
    #endif

    #define IF_THREAD_DOING_X_LDS_STORE(ti, gtid) // here passthrough, ie if(true), as CACHE_DEPTH >= NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE


    #if defined(NB_THREADIDS_FROM_NUMTHREADS_XY)
        #define ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE(ti, gtid) (gtid.x)  // TODOTODO CUSTOM! combine x and y
    #else
        #define ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE(ti, gtid) (ti)
    #endif

#else // #if CACHE_DEPTH >= NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE

    // Here CACHE_DEPTH < NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE
    // so we have more threads than needed to load all needed input values from X in the LDS:

    #define NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE 1

    #define NB_THREADIDS_FOR_INCHAN_AND_SPATIALS_X_LDS_STORE (CACHE_DEPTH * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE)

    #if defined(NB_THREADIDS_FROM_NUMTHREADS_XY)
        #define IF_THREAD_DOING_X_LDS_STORE(ti, gtid) if (gtid.y < CACHE_DEPTH) // y threadids are used for inchan
        #define ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE(ti, gtid) (gtid.x)  // TODOTODO CUSTOM! combine x and y
    #else
        #define IF_THREAD_DOING_X_LDS_STORE(ti, gtid) if (ti < NB_THREADIDS_FOR_INCHAN_AND_SPATIALS_X_LDS_STORE)
        // Guarded version not really needed if using the IF_THREAD_DOING_W_LDS_STORE :
        // #define ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE(ti, gtid) (ti & (NB_THREADIDS_FOR_INCHAN_AND_SPATIALS_X_LDS_STORE - 1))
        #define ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE(ti, gtid) (ti)
    #endif

#endif // #if CACHE_DEPTH >= NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE

#define LOG2_NB_THREADIDS_FOR_INCHAN_AND_SPATIALS_X_LDS_STORE uint(log2(NB_THREADIDS_FOR_INCHAN_AND_SPATIALS_X_LDS_STORE))

// ------------------------------------------------------------------
// LDS rebroadcast and MAD to final output writing threads
// ELEM_PER_THREAD_X ELEM_PER_THREAD_Y

#define NB_ACCU_REG_PER_THREAD (ELEM_PER_THREAD_X * ELEM_PER_THREAD_Y)

// ------------------------------------------------------------------


// kicsp = kernel inchan and possible spatial (ie inside a single output channel slice of the kernel)
// eoc = extra output channel per oc index
// oc = output channel
//#define LDS_W(kicsp, eoc, oc)
//#define LDS_ACCESS_W3(kicsp, eoc, oc)
//#define LDS_ACCESS_X3(kicsp, esp, sp)
// or with also gid, tid, etc. ?


inline float ApplyFusedActivation(float v)
{
// #if GROUP_NB_OC_CHAN_DIVIDER == 1 && defined(NON_UNIFORM_CONVGROUP_PER_OC)
    // v *= 0.0f;
    // return v;
// #else
    return max(v, _MinValue);
//#endif
}

[numthreads(NUMTHREADS_X, NUMTHREADS_Y, 1)]
void MainName(uint3 dispatchThreadID : SV_DispatchThreadID, uint3 groupThreadID : SV_GroupThreadID, uint threadIndex : SV_GroupIndex, uint3 groupID : SV_GroupID)
{
    uint3 gtid = groupThreadID;
    uint x = dispatchThreadID.x * ELEM_PER_THREAD_X; // depth*width*height
    uint y = dispatchThreadID.y * ELEM_PER_THREAD_Y; // output_channels
    uint tx = groupThreadID.x;
    uint ty = groupThreadID.y;
    uint bx = (NUMTHREADS_X * groupID.x) * ELEM_PER_THREAD_X;
    uint by = (NUMTHREADS_Y * groupID.y) * ELEM_PER_THREAD_Y;
    uint ti = threadIndex;
    uint xx =  bx + tx;
    uint yy =  by + ty;

    #if defined(CONV3D)
    uint d = O_depth;
    uint h = O_height;
    uint w = O_width;
    uint channels = X_channels;
    uint features = O_channels;
    uint depthX = X_depth;
    uint heightX = X_height;
    uint widthX = X_width;
    uint strideX = depthX * heightX * widthX;
    uint strideO = d * h * w;
    uint strideK = K_depth * K_height * K_width;
    #elif defined(CONV2D)
    uint h = O_height;
    uint w = O_width;
    uint channels = X_channels;
    uint features = O_channels;
    uint heightX = X_height;
    uint widthX = X_width;
    uint strideX = heightX * widthX;
    uint strideO = h * w;
    uint strideK = K_height * K_width;
    #elif defined(CONV1D)
    uint w = O_width;
    uint channels = X_channels;
    uint features = O_channels;
    uint widthX = X_width;
    uint strideX = widthX;
    uint strideO = w;
    uint strideK = K_width;
    #endif

#if defined(GROUPS_ENABLED)
    uint numInnerChannels = X_channelsPerGroup;
#else
    uint numInnerChannels = X_channels;
#endif

    uint kInnerSliceSize = numInnerChannels * strideK;
    // debug:
    //uint kInnerSliceSize = 2;
    uint maxBIndex = (features - 1);

#if !defined(SHADER_API_D3D11)
    uint maxXIndex = O_batch * X_channels * strideX - 1;
#if !defined(CONV_TRANSPOSE)
    uint maxKIndex = O_channels * numInnerChannels * strideK - 1;
#else
    uint maxKIndex = X_channels * O_channelsPerGroup * strideK - 1;
#endif
#endif // #if !defined(SHADER_API_D3D11)



#if !defined(CONV_TRANSPOSE)
// Important:
//
// First note "channels" (input channels number) and "features" (output channels number)
// are taken from the input tensor X and the output tensor O, NOT the kernel.
// This allows us to ignore for these the swap in semantics on which axes we use in the kernel
// for the inner product part of the convolution, depending on if we have conv transpose or not.
// ie the semantics of "channels" and "features" stay the same whether we have conv transpose
// or not.
//
// When NOT conv transpose, the kernel inner slice size - that is the inner (input) channels and spatial dims
// on which we do an inner product with the input data X -
// and output channel element-to-element strides are the same, obviously because these axes are all adjacent,
// but the kernel is kept identical if to be used in a conv transpose, but the semantics of the
// output/input channel axes are swapped (eg to allow reusing the same kernel tensors in forward and
// backward passes since the conv transpose is effectively the gradient of the conv with the same kernel
// wrt to its inputs given the outputs that were generated).
// Thus in conv transpose, we sum on the outermost axis (axis 0) of the kernel for "input" channels
// and the spatial axes, so the "inner slice" (also called "kicsp" in this code, that we cache in a mixed
// fashion when CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) is effectively split and not contiguous.

    //uint kStrideOC = X_channels * strideK;
    uint kStrideOC = numInnerChannels * strideK;
    // Note here we assume that when not CONV_TRANSPOSE, numInnerChannels == X_channelsPerGroup == kernelShape.axis[1] == X_channels / NumGroups
    // IMPORTANT: Note also we dont use directly kInnerSliceSize, as the later can be reduced if groups are enabled,
    // but here we want the kernel oc *stride* regardless of grouping
#else
// When conv transpose, switching output channels in the kernel means switching elements on axis 1
// (ie + spatial dims total size, strideK)
#define kStrideOC strideK

    // Also, for input channel strides in the kernel if there are groups, we can't use the total number of features we output (ie "features")
    // as the axis size:
    #if defined(GROUPS_ENABLED)
    uint kFeaturesAxisSize = O_channelsPerGroup;
    #else
    uint kFeaturesAxisSize = features;
    #endif
#endif


    uint batchReadOffset = dispatchThreadID.z * channels * strideX;
    uint batchWriteOffset = dispatchThreadID.z * features * strideO;


    // For groups, we split the input channel number offset between when our TG deals with a single
    // convGroupId, vs when they can be different depending on the output channel selected,
    // and this is indicated by NON_UNIFORM_CONVGROUP_PER_OC.
    // When the later is enabled, inputChannelOffsetFromGroupByOC is used.
    uint inputChannelOffsetFromGroup = 0;

#if defined(GROUPS_ENABLED)
#if !defined(NON_UNIFORM_CONVGROUP_PER_OC)
    // Important: !defined(NON_UNIFORM_CONVGROUP_PER_OC)
    // means NB_OUTCHAN_PER_TG <= GROUP_NB_OC_CHAN_DIVIDER == O_channelsPerGroup / k  (where k is a positive int >= 1)
    // and since
    //
    //      SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) + eoc * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE < NB_OUTCHAN_PER_TG,
    //
    // (where eoc := explicit output channel index)
    // we can thus assume that (SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) + eoc)/ O_channelsPerGroup == 0
    //
    // We also have
    // outChannelId = (by + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) + eoc * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE);
    //
    // Now the division above is intdiv so it is no longer distributive, except if it divides evenly 1 term of the sum,
    // which is the case if !defined(NON_UNIFORM_CONVGROUP_PER_OC) for NB_OUTCHAN_PER_TG and "by".
    // We saw that one term will be 0, thus we have:
    //
    // groupId = outChannelId / O_channelsPerGroup
    //         = (by + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) + eoc * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE) / O_channelsPerGroup
    //         = by / O_channelsPerGroup;
    uint convGroupId = by / O_channelsPerGroup;
    inputChannelOffsetFromGroup = convGroupId * X_channelsPerGroup;
#else
    // TODOTODO check if worth it to precomp those instead of calculating on the fly from outputChannelNum
    //
    // IMPORTANT: inputChannelOffsetFromGroupByOC is used in conv transpose for W_LDS_STORE part,
    // but if !defined(NB_THREADIDS_FROM_NUMTHREADS_XY), threads are repurposed
    // and thus SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE works, but for the access of the input data (Xptr)
    // in the inner sum it won't work because in that case, groupThreadID.y is the thread id for
    // output channels. We thus need to recompute another set of inputChannelOffsetFromGroupByOC!
    //
    // Only if
    // #if !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && defined(CONV_TRANSPOSE) && defined(NON_UNIFORM_CONVGROUP_PER_OC)
    // would we actually need the version for W_LDS_STORE separate from the groupThreadID.y version.
    //
    float inputChannelOffsetFromGroupByOC[NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE];
    {
        uint outputChannelNum = (by + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid));
        [unroll]
        for (uint eoc = 0; eoc < NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE; eoc++)
        {
            uint convGroupId = outputChannelNum / O_channelsPerGroup;
            inputChannelOffsetFromGroupByOC[eoc] = convGroupId * X_channelsPerGroup;
            outputChannelNum += NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE;
        }
    }
#endif // !defined(NON_UNIFORM_CONVGROUP_PER_OC)
#endif // defined(GROUPS_ENABLED)


    // kernel read index:
    //
    uint kInputChannelOffsetFromGroup = inputChannelOffsetFromGroup;


    // Update: because we need to compare to "features" (ouput axis 1) for oob for all threads in all TG, dont use modulo first,
    // doesn't save anything, still need to do it every loop turn anyway
    uint baseOutputChannelFromThreadId = (by + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid));

#if defined(CONV_TRANSPOSE) && defined(GROUPS_ENABLED)
    // If there is conv transpose and conv groups, even when the threads all deal with one convGroupId
    // (ie !defined(NON_UNIFORM_CONVGROUP_PER_OC)), since the axis 1 of the kernel is now the output channel axis,
    // and the size of that axis is only as large as O_channelsPerGroup, we need to make sure
    // that the contribution to the kernel read final index from the output channel doesn't go beyond O_channelsPerGroup.
    uint baseOutputChannelAxisElementFromThreadId = baseOutputChannelFromThreadId % O_channelsPerGroup;
#else
    uint baseOutputChannelAxisElementFromThreadId = baseOutputChannelFromThreadId;
#endif


#if !defined(CONV_TRANSPOSE) || (!defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) && !defined(NON_UNIFORM_CONVGROUP_PER_OC))
#if !defined(CONV_TRANSPOSE)
    kInputChannelOffsetFromGroup = 0;
    // First if there's no conv transpose, groups dont matter as there is only one input channel group in the
    // kernel input channel axis (axis 1 when no conv transpose) and groups have no effect on output channel
    // influence on kernel final index.

    // If there is conv transpose but NON_UNIFORM_CONVGROUP_PER_OC, we can't just use a single kInputChannelOffsetFromGroup offset
    // as it depends on the output channel selected (from some thread id and explicitly for NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE values, ie eoc in loops).
    // (And in that later case here we want kInputChannelOffsetFromGroup = 0 too but this is already handled by having
    // inputChannelOffsetFromGroup correctly set to 0 above.)
    // In those NON_UNIFORM_CONVGROUP_PER_OC cases, we deal with this elsewhere, by adding the "input channel offset from group"
    // explicitly where needed.

    // For #if defined(CONV_TRANSPOSE) && defined(GROUPS_ENABLED), see above with baseOutputChannelAxisElementFromThreadId.
#endif

    // For CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE:
    // If we're not doing conv transpose, the inner kernel slice has channels adjacent (and outermore)
    // to the innermore spatial dimensions, so we can use a stride to add to the reading index.
    // When doing conv transpose, we can only use such a stride if we're only caching weights for different inner channels
    // and NOT mixed with different spatial dims too (what I called a mixed kicsp slice) because then the non adjacent
    // axes for kernel input channel and spatial coords require a translation (split in 2 coords).

    uint readK = baseOutputChannelAxisElementFromThreadId * kStrideOC
        + (kInputChannelOffsetFromGroup + SELECT_FROMTHREADIDS_AS_INCHAN_FOR_W_LDS_STORE(ti, gtid)) * IF_ONLY_INCHAN_ELSE(IF_CONV_TRANSPOSE_ELSE(kFeaturesAxisSize * strideK, strideK), 1);
    // IF_ONLY_INCHAN_ELSE : else 1 since in that case we dig in a flattened output channel slice, ie kernel weights can correspond to different input channel indices
    // but also kernel spatial offsets indices too.
    // IF_CONV_TRANSPOSE_ELSE: in conv transpose, the semantics of the kernel axes are (in order outermost to inner) input_channels_to_sum, output_channels aka features, spatial dims
    // (axes 0 and 1 semantics are swapped while keeping the exact same kernel tensor as used in a normal conv).
    // So the stride of inner channels ("inner" == those on which we sum) is the axis-0 element stride, not axis-1 as in normal convolution.

    // Also, if CONV_TRANSPOSE and groups are present, we only precalculate any part of the kernel read index if
    // we only deal with a single convGroupId for all our output channels (see NON_UNIFORM_CONVGROUP_PER_OC comment above)
    //
    //     [For the output channel, we need to apply a modulo on every final value of output channel (as in conv transpose
    //     we have "num of groups"-times more final output channels then the number of values on the output channel axis
    //     (in conv transpose, this is axis 1 of the kernel)
    //     also, for the input channel, we need an offset that depends on convGroupId which, if NON_UNIFORM_CONVGROUP_PER_OC,
    //     could also vary per output channel.]

#else
    // Here we have:
    //
    //      defined(CONV_TRANSPOSE) && (defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) || defined(NON_UNIFORM_CONVGROUP_PER_OC)):
    //
    #if !(defined(CONV_TRANSPOSE) && (defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) || defined(NON_UNIFORM_CONVGROUP_PER_OC)))
    #error Unexpected conv transpose config
    #endif

    // Conv transpose path with CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE and or NON_UNIFORM_CONVGROUP_PER_OC
    //
    // CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE case:
    //
    //      No inner slice (kicsp) selection with threadids is made outside loops in conv transpose when caching both for
    //      different kernel input (inner) channels and spatial (kicsp slice), (ie when CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
    //      as a translation (split in 2 coords) is needed
    //      from inner slice element index to 2 coords: axis 0 element
    //      (kernel "input channels" on which we sum - again note these are normally the output channels for same kernel used as normal conv kernel)
    //      and linear spatial (flattened spatial dims of the kernel) element.


    // Also, in this conv transpose context, if groups are present but the convGroupId stays the same for all our output channels
    // (ie !defined(NON_UNIFORM_CONVGROUP_PER_OC))
    // it means we don't have to "wrap around" our output channel number using a % (modulo) O_channelsPerGroup
    // during later code for all threads / explicit output channel.
    // We sill have to deal with a modulo for baseOutputChannelAxisElementFromThreadId since it includes the global output channel offset
    // for our thread group (see by index).
    // (Remember in that case kernel axis 1 - the output channel axis in conv transpose - only has O_channelsPerGroup elements and
    // further output channels just switch convolution groups).
    // uint readK = baseOutputChannelAxisElementFromThreadId * kStrideOC;

#endif // #if !defined(CONV_TRANSPOSE) || (!defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) && !defined(NON_UNIFORM_CONVGROUP_PER_OC))




    // We will initialize accumulators with output channel bias which is the same for all input/output spatial
    // coordinates (as conceptually part of the kernels for each output channel, which process the same input)
    // so we also replicate these values for all spatial coordinates handled.
    //
    // We need the following amount of accumulators:
    //          (ELEM_PER_THREAD_X ie spatial elements per thread) x (ELEM_PER_THREAD_Y ie output channel elements per thread)
    //
    // We use indexing for registers to simplify the code but this is all statically indexed so should not change
    // any compiler output vs using non-indexing.
    //

    float dstA[NB_ACCU_REG_PER_THREAD];
    #define OC_AREG_STRIDE ELEM_PER_THREAD_X  // output channel accumulator reg stride

    uint eoc; // (per thread) explicit output channels indices
    uint esp; // (per thread) explicit spatial indices
    [unroll]
    for (eoc = 0; eoc < ELEM_PER_THREAD_Y; eoc++)
    {
    #if defined(USEBIAS) && !defined(BIAS_AFTER)
        // debug
        // #if defined(NON_UNIFORM_CONVGROUP_PER_OC)
        // dstA[eoc * OC_AREG_STRIDE] = 0;
        // #else
        dstA[eoc * OC_AREG_STRIDE] = Bptr[MIN_WHEN_NON_D3D(yy + eoc * NUMTHREADS_Y, maxBIndex)];
        // #endif
    #else
        dstA[eoc * OC_AREG_STRIDE] = 0;
    #endif
    }

    // Init / Replication of accumulators from bias:
    [unroll]
    for (eoc = 0; eoc < ELEM_PER_THREAD_Y; eoc++)
        [unroll]
        for (esp = 1; esp < ELEM_PER_THREAD_X; esp++)
        {
        #if defined(USEBIAS) && !defined(BIAS_AFTER)
            dstA[eoc * OC_AREG_STRIDE + esp] = dstA[eoc * OC_AREG_STRIDE];
        #else
            dstA[eoc * OC_AREG_STRIDE + esp] = 0;
        #endif
        }


    // Spatial output coordinates starting point (where kernel window offset is added to get where kernel window overlaps input),
    // for each final spatial output position per thread:

#if !(   defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && (NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE != ELEM_PER_THREAD_X)  )
    uint topDPadded[NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE];
    uint topYPadded[NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE];
    uint topXPadded[NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE];
    [unroll]
    for (esp = 0; esp < NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE; esp++)
    {
        // Important: Note the stride of NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE
        // NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE == NB_SPATIALS_PER_TG
        uint centroidId = bx + SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid) + esp * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE;

        #if !defined(CONV_TRANSPOSE)
        uint4 strideParam = _StrideParam;
        #else
        uint4 strideParam = uint4(1,1,1,1);
        #endif

        #if defined(CONV3D)
        topDPadded[esp] = (((centroidId / w) / h)) * strideParam.x - _Pad.x;
        topYPadded[esp] = ((centroidId / w) % h) * strideParam.y - _Pad.y;
        topXPadded[esp] = (centroidId % w) * strideParam.z - _Pad.z;

        #elif defined(CONV2D)
        topYPadded[esp] = ((centroidId / w)) * strideParam.x - _Pad.x;
        topXPadded[esp] = (centroidId % w) * strideParam.y - _Pad.y;

        #elif defined(CONV1D)
        topXPadded[esp] = centroidId * strideParam.x - _Pad.x;

        #endif
    }


    // uint readX = batchReadOffset + strideX * SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid) + linear offset;
    // Final linear offset, eg for 2D = (topY - _Pad.x) * widthX + (topX - _Pad.y),
    // will be in kernelOffsetX.
    #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
    //uint readX = batchReadOffset + strideX * SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid);
    #endif
#endif


#if defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)

    for (uint i = 0; i < kInnerSliceSize; i += CACHE_DEPTH)

#else // #if CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE

    // Here we only cache (for the inner product) for different input channels,
    // not for different kernel spatial coords

    #if defined(CONV_TRANSPOSE)
    uint weightOffsetK = (strideK - 1);
    #else
    uint weightOffsetK = 0;
    #endif


#if !defined(K1x1)
    #if defined(CONV3D)
        for (uint dd = 0; dd < K_depth; dd++)
        for (uint dy = 0; dy < K_height; dy++)
        for (uint dx = 0; dx < K_width; dx++)
    #elif defined(CONV2D)
        for (uint dy = 0; dy < K_height; dy++)
        for (uint dx = 0; dx < K_width; dx++)
    #elif defined(CONV1D)
        for (uint dx = 0; dx < K_width; dx++)
    #endif
#endif // !defined(K1x1)
#endif // #else // #if CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE
    {
        #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)

        #if !(   defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && (NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE != ELEM_PER_THREAD_X)  )
            uint kernelOffsetX[NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE];
            bool maskX[NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE];

            [unroll]
            for (esp = 0; esp < NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE; esp++)
            {
                #if defined(CONV_TRANSPOSE)
                uint4 strideParamIfConvTrans = _StrideParam;
                #else
                uint4 strideParamIfConvTrans = uint4(1,1,1,1);
                #endif

                // Note: because of the define above, if we're not using conv transpose,
                // all / strideParamIfConvTrans subexpressions and all mask modulo ( % ) tests
                // will all be optimized out by the compiler.
                // This is just to avoid even more #ifdef blocks below.

                // We do the same for K1x1 vs dilations:
                #if defined(K1x1)
                uint4 dilationParam = uint4(0,0,0,0);
                uint dd = 0;
                uint dy = 0;
                uint dx = 0;
                #else
                uint4 dilationParam = _Dilation;
                #endif

                #ifdef CONV3D
                kernelOffsetX[esp] = (topDPadded[esp] + dilationParam.x * dd)  / strideParamIfConvTrans[0] * heightX * widthX + (topYPadded[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1] * widthX
                    + (topXPadded[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2];
                maskX[esp] = ((topDPadded[esp] + dilationParam.x * dd) / strideParamIfConvTrans[0]) < depthX &&  ((topDPadded[esp] + dilationParam.x * dd) % strideParamIfConvTrans[0] == 0) &&
                             ((topYPadded[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1]) < heightX && ((topYPadded[esp] + dilationParam.y * dy) % strideParamIfConvTrans[1] == 0) &&
                             ((topXPadded[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2]) < widthX &&  ((topXPadded[esp] + dilationParam.z * dx) % strideParamIfConvTrans[2] == 0);
                #elif defined(CONV2D)
                kernelOffsetX[esp] = (topYPadded[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0] * widthX + (topXPadded[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1];
                maskX[esp] = ((topYPadded[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0]) < heightX && ((topYPadded[esp] + dilationParam.x * dy) % strideParamIfConvTrans[0] == 0) &&
                             ((topXPadded[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1]) < widthX &&  ((topXPadded[esp] + dilationParam.y * dx) % strideParamIfConvTrans[1] == 0);
                #elif defined(CONV1D)
                kernelOffsetX[esp] = (topXPadded[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0];
                maskX[esp] = ((topXPadded[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0]) < widthX && ((topXPadded[esp] + dilationParam.x * dx) % strideParamIfConvTrans[0] == 0);
                #endif
            }
        #endif // #if !(   defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && (NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE != ELEM_PER_THREAD_X)  )

        for (uint i = 0; i < numInnerChannels; i += CACHE_DEPTH)
        #endif // #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
        {
            // store to LDS
            {
                // #if NB_EXPLICIT_INCHAN_VAL_NEEDED_W_LDS_STORE * NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE == 2 * 8
                // #error ok
                // #endif
                float srcW[NB_EXPLICIT_INCHAN_VAL_NEEDED_W_LDS_STORE * NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE];
                float srcX[NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE];

                bool innerElemInBound = false;

                // Note: These line (IF_THREAD_DOING_*) can create a huge difference in perf in a case where we have more threadids left for the
                // inner slice indices after allocating those for output channel indices, then we need for the total CACHE_DEPTH)
                // since otherwise (eg when not using NB_THREADIDS_FROM_NUMTHREADS_XY) a group of totalthreads_in_tg/(ELEM_PER_THREAD_Y*NUMTHREADS_Y) threads
                // all try to load/store the same location (on store, which one is undefined but since all carry the same value it doesn't matter)
                // but more importantly those requests could come from different waves so even for loads probably no hw broadcast possible
                // (and for store, can't just pick a single thread for the undefined spec).
                IF_THREAD_DOING_W_LDS_STORE(ti, gtid)
                {
                    const uint innerThreadIdsStride = NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE; // threadids used for inchan stride

                    // uint readK = (by + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid)) * kStrideOC + SELECT_FROMTHREADIDS_AS_INCHAN_FOR_W_LDS_STORE(ti, gtid) * IF_ONLY_INCHAN_ELSE(strideK,1);
                    // bool outChanMaskK = (by + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid)) < features;


                #if !defined(CONV_TRANSPOSE) || (!defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) && !defined(NON_UNIFORM_CONVGROUP_PER_OC))
                    // If we're not doing conv transpose, the inner kernel slice has channels adjacent (and outermore)
                    // to the innermore spatial dimensions, so we can use a stride to add to the reading index.

                    // When doing conv transpose, we can only use such a stride if we're only caching weights for different inner channels
                    // and NOT mixed with different spatial dims too (what I called a mixed kicsp slice).
                    // (Otherwise a translation of the inner index into 2 coords is needed).
                    //
                    // Also with conv transpose, with groups, if defined(NON_UNIFORM_CONVGROUP_PER_OC), the convGroupId (and thus offset due to it)
                    // depends on the final output channel value and so on the "eoc" loop index below.
                    // See -> Check #if defined(CONV_TRANSPOSE) && !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) && defined(NON_UNIFORM_CONVGROUP_PER_OC)
                    //
                    const uint innerIdxStride = IF_ONLY_INCHAN_ELSE(IF_CONV_TRANSPOSE_ELSE(kFeaturesAxisSize * strideK, strideK), 1) * innerThreadIdsStride;

                    // In conv transpose, the semantics of the kernel axes are (in order outermost to inner) input_channels_to_sum, output_channels aka features, spatial dims
                    // (axes 0 and 1 semantics are swapped while keeping the exact same kernel tensor as used in a normal conv).

                    //uint readK = (by + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid)) (in fact baseOutputChannelAxisElementFromThreadId possibly with modulo O_channelsPerGroup) * kStrideOC
                    //    + (kInputChannelOffsetFromGroup + SELECT_FROMTHREADIDS_AS_INCHAN_FOR_W_LDS_STORE(ti, gtid)) * IF_ONLY_INCHAN_ELSE(IF_CONV_TRANSPOSE_ELSE(features * strideK, strideK), 1);

                    uint baseIdxInnerPartStart = readK + IF_ONLY_INCHAN_ELSE(IF_CONV_TRANSPOSE_ELSE(kFeaturesAxisSize * strideK, strideK), 1) * i + IF_ONLY_INCHAN_ELSE(weightOffsetK, 0);

                #else
                    //      defined(CONV_TRANSPOSE) && (defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) || defined(NON_UNIFORM_CONVGROUP_PER_OC)):

                    //
                    // Conv transpose path with:
                    // CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE, with or without groups,
                    // or with NON_UNIFORM_CONVGROUP_PER_OC, with or without CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE.
                    //

                    // Conv tranpose + CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE:
                    //
                    //      No inner slice (kicsp) selection with threadids is made outside loops in conv transpose when caching both for
                    //      different kernel input (inner) channels and spatial (kicsp slice), (ie when CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                    //      as a translation (split in 2 coords) is needed
                    //      from inner slice element index to 2 coords: axis 0 element
                    //      (kernel "input channels" on which we sum - again note these are normally the output channels for same kernel used as normal conv kernel)
                    //      and linear spatial (flattened spatial dims of the kernel) element.
                    //
                    //      uint baseOutputChannelAxisElementFromThreadId = (by + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid)) possibly with modulo O_channelsPerGroup;

                    // Conv tranpose + NON_UNIFORM_CONVGROUP_PER_OC:
                    //
                    //      Whether we have CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE or not,
                    //      we will not deal with the inner part contributions to the index outside the
                    //      unrolled loops below as with NON_UNIFORM_CONVGROUP_PER_OC,
                    //      the inputChannelOffsetFromGroupByOC depends on the explicit output channel.

                    uint baseIdxInnerPartStart = baseOutputChannelAxisElementFromThreadId * kStrideOC + IF_ONLY_INCHAN_ELSE(weightOffsetK, 0);

                #endif // #if !defined(CONV_TRANSPOSE) || (!defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) && !defined(NON_UNIFORM_CONVGROUP_PER_OC))


                    uint outputChannelNum = baseOutputChannelFromThreadId;
                    bool curOutChanMaskK = outputChannelNum < features;

                    // TODOTODO: #if defined(CONV_TRANSPOSE) && defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                    // flip the loop order
                    [unroll]
                    for (uint eoc = 0; eoc < NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE; eoc++)
                    {
                        uint baseIdx = baseIdxInnerPartStart;
                        uint baseInnerElement = SELECT_FROMTHREADIDS_AS_INCHAN_FOR_W_LDS_STORE(ti, gtid) + i;
                        // ...important! reset baseIdx and baseInnerElement for the flattened/combined kStrideOC-sized or input channels-sized slice

                    // -> Check
                    #if defined(CONV_TRANSPOSE) && !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE) && defined(NON_UNIFORM_CONVGROUP_PER_OC)
                        // baseInnerElement is just input channel here as NO CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE
                        // also we have an output_channel-dependent offset to add to it for convGroupId:
                        baseIdx = baseIdxInnerPartStart + (inputChannelOffsetFromGroupByOC[eoc] + baseInnerElement) * (kFeaturesAxisSize * strideK);
                        const uint innerIdxStride = IF_ONLY_INCHAN_ELSE(IF_CONV_TRANSPOSE_ELSE(kFeaturesAxisSize * strideK, strideK), 1) * innerThreadIdsStride;
                    #endif


                        [unroll]
                        for (uint eic = 0; eic < NB_EXPLICIT_INCHAN_VAL_NEEDED_W_LDS_STORE; eic++)
                        {
                        #if defined(CONV_TRANSPOSE) && defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                            // Here, output channel part of the kernel read index already handled above with baseIdx = baseIdxInnerPartStart.
                            // We also handle all conv group modes: with/without groups, and uniform and non-uniform groups.

                            uint kInnerFlatSpatialElement = baseInnerElement % strideK;
                            uint kInnerInChannelElement = baseInnerElement / strideK;
                            uint kInnerInChannelElementWGroupOffset = kInnerInChannelElement + IF_HAVE_GROUPS_ELSE(IF_NON_UNIFORM_GROUPS_ELSE(inputChannelOffsetFromGroupByOC[eoc], inputChannelOffsetFromGroup), 0);
                            baseIdx = baseIdxInnerPartStart + kInnerInChannelElementWGroupOffset * (kFeaturesAxisSize * strideK)
                                + (strideK - 1) - kInnerFlatSpatialElement;
                            // (strideK - 1) - kInnerFlatSpatialElement: kernel is flipped along its spatial axes and the new "top/left/element 0" ie start of the window
                            // which is aligned on each start of input element (input is conceptually augmented with zero padding and "exploded with interleaved zeroes" by the stride param)
                            // is the "end" of the "original" (ie that would be used in normal conv) kernel and moving forward in the kernel for conv transpose
                            // means backward in the "original" kernel.
                        #endif

                            //innerElemInBound = SELECT_FROMTHREADIDS_AS_INCHAN_FOR_W_LDS_STORE(ti, gtid) + (i + eic * innerThreadIdsStride) < IF_ONLY_INCHAN_ELSE(numInnerChannels, kInnerSliceSize);
                            innerElemInBound = baseInnerElement < IF_ONLY_INCHAN_ELSE(numInnerChannels, kInnerSliceSize);
                            float val = 0.0f;
                            if (curOutChanMaskK && innerElemInBound)
                                //val = Kptr[readK + IF_ONLY_INCHAN_ELSE(strideK, 1) * (i + eic * innerThreadIdsStride) + IF_ONLY_INCHAN_ELSE(weightOffsetK, 0)];
                                //debug
                                //val = O_channels;//features;//baseIdx; // debug
                                //debug
                                //val = baseIdx;
                                val = Kptr[MIN_PATCHBUG(baseIdx, maxKIndex)];

                        #if !defined(CONV_TRANSPOSE) || !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                            // Note: if defined(NON_UNIFORM_CONVGROUP_PER_OC), we can still use innerIdxStride because
                            // we're in the eic inner loop, eoc doesn't change and thus neither the convGroupId (although it can still be different implicitly per threadID).
                            baseIdx += innerIdxStride;
                        #endif
                            baseInnerElement += innerThreadIdsStride;

                            //LDS_[W_OFFSET + (eic * NB_THREADIDS_FOR_INCHAN_AND_OUTCHAN_W_LDS_STORE) + ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE(ti, gtid)] = val;
                            srcW[eoc * NB_EXPLICIT_INCHAN_VAL_NEEDED_W_LDS_STORE + eic] = val;
                        }

                        //
                        // switch to next group of explicit output channel (1 group == NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE indices)
                        //
                        outputChannelNum += NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE;
                        curOutChanMaskK = outputChannelNum < features;
                    // -> Check
                    #if defined(CONV_TRANSPOSE) && defined(NON_UNIFORM_CONVGROUP_PER_OC)
                        baseIdxInnerPartStart = kStrideOC * (outputChannelNum % O_channelsPerGroup) + IF_ONLY_INCHAN_ELSE(weightOffsetK, 0);
                    #else
                        baseIdxInnerPartStart += kStrideOC * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE;
                    #endif
                    }

                }


                #if !defined(NON_UNIFORM_CONVGROUP_PER_OC)
                // ...we dont cache input data if they can come from different groups per output channels
                IF_THREAD_DOING_X_LDS_STORE(ti, gtid)
                {

                    const uint innerThreadIdsStride = NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE; // threadids used for inchan stride
                #if 0
                    // TODOTODO  conv transpose path


                    // Switch LOOP order if NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE < NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE
                    [unroll]
                    for (uint esp = 0; esp < NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE; esp++)
                    {
                        // cornerId from -> centroidId = bx + SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid);
                        // uint readX = strideX * SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid) + cornerId + batchReadOffset;
                        // update: readX no longer has cornerId

                        #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                        //uint readX = batchReadOffset + strideX * SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid);
                        //uint baseIdx = readX + strideX * i + kernelOffsetX[esp]; // now kernelOffsetX includes cornerId !
                        uint baseIdx = batchReadOffset + strideX * (inputChannelOffsetFromGroup + i + SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid)) + kernelOffsetX[esp]; // now kernelOffsetX includes cornerId !
                        uint innerIdxStride = strideX * innerThreadIdsStride;
                        #else
                        // CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE case:
                        // No readX as it would only contain batchReadOffset outside loops, and no innerIdxStride usage possible
                        // as the precise index depends on a conversion between what kicsp index in that (kicsp) mixed inner chunk
                        // is used for the weights, so converting from the kernel input channel and window coordinates for each
                        // to the corresponding input data input channel (same) and spatial offset (from the kernel spatial window part).
                        #endif
                        uint baseInnerElement = SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid) + i;
                        // ...important! reset baseInnerElement for the flattened/combined kStrideOC-sized or input channels-sized slice
                        // at each start of a new inner (input channel / kernel spatial window) chunk

                        // Also, the switch to next group of explicit spatial indices (esp) (1 group == NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE indices)
                        // is implicit and precalculated in the [esp] indexed arrays (topDXY, kernelOffsetX)

                        [unroll]
                        for (uint eic = 0; eic < NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE; eic++)
                        {
                        #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)

                            //innerElemInBound = SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid) + (i + eic * innerThreadIdsStride) < numInnerChannels;
                            innerElemInBound = baseInnerElement < numInnerChannels;

                            float val = 0.0f;
                            if (maskX[esp] && innerElemInBound)
                                val = Xptr[MIN_PATCHBUG(baseIdx, maxXIndex)];
                                //val = Xptr[readX + strideX * (i + eic * innerThreadIdsStride) + kernelOffsetX];
                                //                   -------        ---   ----  => hence innerIdxStride to add to baseIdx is just (strideX * innerThreadIdsStride)
                                // and similarly baseInnerElement just doesn't include the strideX, it just expresses the inner element number not the full linear index
                                // so it is simply innerThreadIdsStride since we have NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE threadids chunk at each loop turn.

                            baseIdx += innerIdxStride;
                            baseInnerElement += innerThreadIdsStride;
                            //LDS_[X_OFFSET + (eic * NB_THREADIDS_FOR_INCHAN_AND_SPATIALS_X_LDS_STORE) + ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE(ti, gtid)] = val;
                            srcX[esp * NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE + eic] = val;

                        #else
                            // inner index to match kernel CACHE_DEPTH-sized chunk
                            //uint ii = SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid) + (i + eic * innerThreadIdsStride);
                            uint ii = baseInnerElement;
                            bool inBound = ii < kInnerSliceSize; // that way no need to check ic < channels (or < numInnerChannels actually) below


                            // Note: K1x1 cannot be defined here at the same time that CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE is,
                            // as in that K1x1 case, we can only be caching for different input channels indices,
                            // the spatial dimensions sizes of the kernel all multiply to 1, ie there's only one spatial coord in the kernel.
                            // #if defined(K1x1)
                            // #error "K1x1 shouldn't touch the path where CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE"
                            // #endif
                            // Update: SHADER_API_WEBGPU

                            #if defined(CONV_TRANSPOSE)
                            uint4 strideParamIfConvTrans = _StrideParam;
                            #else
                            uint4 strideParamIfConvTrans = uint4(1,1,1,1);
                            #endif
                            // Note: because of the define above, if we're not using conv transpose,
                            // all / strideParamIfConvTrans subexpressions and all mask modulo ( % ) tests
                            // will all be optimized out by the compiler.
                            // This is just to avoid even more #ifdef blocks below.

                            #if defined(K1x1)
                            uint4 dilationParam = uint4(0,0,0,0);
                            #else
                            uint4 dilationParam = _Dilation;
                            #endif

                            #if defined(CONV3D)
                            uint dd = ((ii / K_width) / K_height) % K_depth;
                            uint ic = ((ii / K_width) / K_height) / K_depth;
                            uint dy = (ii / K_width) % K_height;
                            uint dx = ii % K_width;
                            #elif defined(CONV2D)
                            uint dy = (ii / K_width) % K_height;
                            uint ic = (ii / K_width) / K_height;
                            uint dx = ii % K_width;
                            #elif defined(CONV1D)
                            uint dx = ii % K_width;
                            uint ic = ii / K_width;
                            #endif

                            ic = ic + inputChannelOffsetFromGroup;

                            #ifdef CONV3D
                            uint kernelOffsetX = (topDPadded[esp] + dilationParam.x * dd)  / strideParamIfConvTrans[0] * heightX * widthX + (topYPadded[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1] * widthX
                                + (topXPadded[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2];
                            bool maskX = ((topDPadded[esp] + dilationParam.x * dd) / strideParamIfConvTrans[0]) < depthX &&  ((topDPadded[esp] + dilationParam.x * dd) % strideParamIfConvTrans[0] == 0) &&
                                         ((topYPadded[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1]) < heightX && ((topYPadded[esp] + dilationParam.y * dy) % strideParamIfConvTrans[1] == 0) &&
                                         ((topXPadded[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2]) < widthX &&  ((topXPadded[esp] + dilationParam.z * dx) % strideParamIfConvTrans[2] == 0);
                            #elif defined(CONV2D)
                            uint kernelOffsetX = (topYPadded[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0] * widthX + (topXPadded[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1];
                            bool maskX = ((topYPadded[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0]) < heightX && ((topYPadded[esp] + dilationParam.x * dy) % strideParamIfConvTrans[0] == 0) &&
                                         ((topXPadded[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1]) < widthX &&  ((topXPadded[esp] + dilationParam.y * dx) % strideParamIfConvTrans[1] == 0);
                            #elif defined(CONV1D)
                            uint kernelOffsetX = (topXPadded[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0];
                            bool maskX = ((topXPadded[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0]) < widthX && ((topXPadded[esp] + dilationParam.x * dx) % strideParamIfConvTrans[0] == 0);
                            #endif


                            float val = 0.0f;
                            if (maskX && inBound)
                                val = Xptr[MIN_PATCHBUG(batchReadOffset + strideX * ic + kernelOffsetX, maxXIndex)];
                            baseInnerElement += innerThreadIdsStride;
                            //LDS_[X_OFFSET + (eic * NB_THREADIDS_FOR_INCHAN_AND_SPATIALS_X_LDS_STORE) + ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE(ti, gtid)] = val;
                            srcX[esp * NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE + eic] = val;

                        #endif
                        } // for explicit inner input channel * maybe kernel spatial offset (if CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)


                        // CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE case:
                        // The switch to next group of explicit spatial indices (1 group == NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE indices)
                        // is implicit and precalculated in the [esp] indexed arrays (topDXY, kernelOffsetX)
                    }// for explicit spatial channels

                #else // if 0


                    // uint centroidId = bx + SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid) + esp * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE;
                    // -> top*Padded -> kernelOffsetX
                    // uint readX = strideX * SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid) + batchReadOffset + kernelOffsetX which include the above

                    #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                    //uint readX = batchReadOffset + strideX * SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid);
                    //uint baseIdx = readX + strideX * i;
                    uint baseIdx = batchReadOffset + strideX * (inputChannelOffsetFromGroup + i + SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid));
                    uint innerIdxStride = strideX * innerThreadIdsStride;
                    #else
                    // CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE case:
                    // No readX as it would only contain batchReadOffset outside loops, and no innerIdxStride usage possible
                    // as the precise index depends on a conversion between what kicsp index in that (kicsp) mixed inner chunk
                    // is used for the weights, so converting from the kernel input channel and window coordinates for each
                    // to the corresponding input data input channel (same) and spatial offset (from the kernel spatial window part).
                    #endif
                    uint baseInnerElement = SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid) + i;
                    // ...important! reset baseInnerElement for the flattened/combined kStrideOC-sized or input channels-sized slice
                    // at each start of a new inner (input channel / kernel spatial window) chunk

                    // Also, the switch to next group of explicit spatial indices (esp) (1 group == NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE indices)
                    // is implicit and precalculated in the [esp] indexed arrays (topDXY, kernelOffsetX)

                    [unroll]
                    for (uint eic = 0; eic < NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE; eic++)
                    {
                    #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                        //innerElemInBound = SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid) + (i + eic * innerThreadIdsStride) < channels;
                        innerElemInBound = baseInnerElement < numInnerChannels;
                    #else

                        // inner index to match kernel CACHE_DEPTH-sized chunk
                        //uint ii = SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid) + (i + eic * innerThreadIdsStride);
                        uint ii = baseInnerElement;
                        bool inBound = ii < kInnerSliceSize; // that way no need to check ic < channels below

                        // Note: K1x1 cannot be defined here at the same time that CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE is,
                        // as in that K1x1 case, we can only be caching for different input channels indices,
                        // the spatial dimensions sizes of the kernel all multiply to 1, ie there's only one spatial coord in the kernel.
                        // Update: see SHADER_API_WEBGPU prob.
                        #if defined(CONV3D)
                        uint dd = ((ii / K_width) / K_height) % K_depth;
                        uint ic = ((ii / K_width) / K_height) / K_depth;
                        uint dy = (ii / K_width) % K_height;
                        uint dx = ii % K_width;
                        #elif defined(CONV2D)
                        uint dy = (ii / K_width) % K_height;
                        uint ic = (ii / K_width) / K_height;
                        uint dx = ii % K_width;
                        #elif defined(CONV1D)
                        uint dx = ii % K_width;
                        uint ic = ii / K_width;
                        #endif

                        ic = ic + inputChannelOffsetFromGroup;

                    #endif


                        [unroll]
                        for (uint esp = 0; esp < NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE; esp++)
                        {
                        #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                            //uint readX = batchReadOffset + strideX * SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid);
                            //uint baseIdx = readX + strideX * i + kernelOffsetX[esp]; // now kernelOffsetX includes cornerId !
                            float val = 0.0f;
                            if (maskX[esp] && innerElemInBound)
                                val = Xptr[MIN_PATCHBUG(baseIdx + kernelOffsetX[esp], maxXIndex)]; // now kernelOffsetX includes cornerId !
                                //val = Xptr[readX + strideX * (i + eic * innerThreadIdsStride) + kernelOffsetX];
                                //                   -------        ---   ----  => hence innerIdxStride to add to baseIdx is just (strideX * innerThreadIdsStride)
                                // and similarly baseInnerElement just doesn't include the strideX, it just expresses the inner element number not the full linear index
                                // so it is simply innerThreadIdsStride since we have NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE threadids chunk at each loop turn.

                            //LDS_[X_OFFSET + (eic * NB_THREADIDS_FOR_INCHAN_AND_SPATIALS_X_LDS_STORE) + ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE(ti, gtid)] = val;
                            srcX[esp * NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE + eic] = val;

                        #else

                            #if defined(CONV_TRANSPOSE)
                            uint4 strideParamIfConvTrans = _StrideParam;
                            #else
                            uint4 strideParamIfConvTrans = uint4(1,1,1,1);
                            #endif

                            // Note: because of the define above, if we're not using conv transpose,
                            // all / strideParamIfConvTrans subexpressions and all mask modulo ( % ) tests
                            // will all be optimized out by the compiler.
                            // This is just to avoid even more #ifdef blocks below.

                            // Note: K1x1 cannot be defined here at the same time that CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE is,
                            // as in that K1x1 case, we can only be caching for different input channels indices,
                            // the spatial dimensions sizes of the kernel all multiply to 1, ie there's only one spatial coord in the kernel.
                            // #if defined(K1x1)
                            // #error "K1x1 shouldn't touch the path where CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE"
                            // #endif
                            // Update: SHADER_API_WEBGPU

                            #if defined(K1x1)
                            uint4 dilationParam = uint4(0,0,0,0);
                            #else
                            uint4 dilationParam = _Dilation;
                            #endif


                            #ifdef CONV3D
                            uint kernelOffsetX = (topDPadded[esp] + dilationParam.x * dd)  / strideParamIfConvTrans[0] * heightX * widthX + (topYPadded[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1] * widthX
                                + (topXPadded[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2];
                            bool maskX = ((topDPadded[esp] + dilationParam.x * dd) / strideParamIfConvTrans[0]) < depthX &&  ((topDPadded[esp] + dilationParam.x * dd) % strideParamIfConvTrans[0] == 0) &&
                                         ((topYPadded[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1]) < heightX && ((topYPadded[esp] + dilationParam.y * dy) % strideParamIfConvTrans[1] == 0) &&
                                         ((topXPadded[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2]) < widthX &&  ((topXPadded[esp] + dilationParam.z * dx) % strideParamIfConvTrans[2] == 0);
                            #elif defined(CONV2D)
                            uint kernelOffsetX = (topYPadded[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0] * widthX + (topXPadded[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1];
                            bool maskX = ((topYPadded[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0]) < heightX && ((topYPadded[esp] + dilationParam.x * dy) % strideParamIfConvTrans[0] == 0) &&
                                         ((topXPadded[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1]) < widthX &&  ((topXPadded[esp] + dilationParam.y * dx) % strideParamIfConvTrans[1] == 0);
                            #elif defined(CONV1D)
                            uint kernelOffsetX = (topXPadded[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0];
                            bool maskX = ((topXPadded[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0]) < widthX && ((topXPadded[esp] + dilationParam.x * dx) % strideParamIfConvTrans[0] == 0);
                            #endif

                            //LDS_[X_OFFSET + (eic << LOG2_NB_THREADIDS_FOR_INCHAN_AND_SPATIALS_X_LDS_STORE) + ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE(ti, gtid)]
                            float val = 0.0f;
                            if (maskX && inBound)
                                val = Xptr[MIN_PATCHBUG(batchReadOffset + strideX * ic + kernelOffsetX, maxXIndex)];

                            //LDS_[X_OFFSET + (eic * NB_THREADIDS_FOR_INCHAN_AND_SPATIALS_X_LDS_STORE) + ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE(ti, gtid)] = val;
                            srcX[esp * NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE + eic] = val;

                        #endif
                        }// for explicit spatial channels


                        #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                        baseIdx += innerIdxStride;
                        #endif
                        baseInnerElement += innerThreadIdsStride;

                    }// for explicit inner input channel * maybe kernel spatial offset (if CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)

                #endif //0
                } // X_LDS_STORE
                #endif // #if !defined(NON_UNIFORM_CONVGROUP_PER_OC) // we dont cache input data if they can come from different groups per output channels


                GroupMemoryBarrierWithGroupSync();

                // store regs to LDS
                IF_THREAD_DOING_W_LDS_STORE(ti, gtid)
                {
                    [unroll]
                    for (uint eoc = 0; eoc < NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE; eoc++)
                    {
                        [unroll]
                        for (uint eic = 0; eic < NB_EXPLICIT_INCHAN_VAL_NEEDED_W_LDS_STORE; eic++)
                        {
                        #if defined(NB_THREADIDS_FROM_NUMTHREADS_XY)
                            // Here for eg CACHE_DEPTH = 16, numthreads(16,8,1), we except NB_EXPLICIT_INCHAN_VAL_NEEDED_W_LDS_STORE == 1
                            // and with, ELEM_PER_THREAD_Y = 8, to have NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE == 8
                            // #if NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE == 8 && NB_EXPLICIT_INCHAN_VAL_NEEDED_W_LDS_STORE == 1 && NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE == CACHE_DEPTH && NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE == NUMTHREADS_Y
                            // #error all ok
                            // #endif
                            // #if NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE == 8 && NB_EXPLICIT_INCHAN_VAL_NEEDED_W_LDS_STORE == 2 && NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE == CACHE_DEPTH/2 && NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE == NUMTHREADS_Y
                            // #error all ok
                            // #endif
                            // #if NB_EXPLICIT_INCHAN_VAL_NEEDED_W_LDS_STORE * NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE == CACHE_DEPTH
                            // #error okok
                            // #endif

                            uint index = eoc * CACHE_DEPTH * NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE
                                        + SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid) * CACHE_DEPTH /* groupThreadID.y * CACHE_DEPTH */
                                        + (eic * NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE)
                                        + SELECT_FROMTHREADIDS_AS_INCHAN_FOR_W_LDS_STORE(ti, gtid); /* groupThreadID.x */
                        #else
                            uint index = eoc * NB_EXPLICIT_INCHAN_VAL_NEEDED_W_LDS_STORE * NB_THREADIDS_FOR_INCHAN_AND_OUTCHAN_W_LDS_STORE // CHECK should be CACHE_DEPTH * NB_OUTCHAN_PER_TG
                                        + (eic * NB_THREADIDS_FOR_INCHAN_AND_OUTCHAN_W_LDS_STORE) + ADD_IDX_FROM_THREADID_FOR_W_LDS_STORE(ti, gtid);
                        #endif

                            #ifdef USE_ALTERNATE_LDS_ORG_W
                            LDS_ACCESS_W_ITE(SELECT_FROMTHREADIDS_AS_INCHAN_FOR_W_LDS_STORE(ti, gtid) + (NB_THREADIDS_LEFT_FOR_INCHAN_W_LDS_STORE * eic), SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid), eoc)
                                = srcW[eoc * NB_EXPLICIT_INCHAN_VAL_NEEDED_W_LDS_STORE + eic];
                            #else
                            LDS_ACCESS_W(index) = srcW[eoc * NB_EXPLICIT_INCHAN_VAL_NEEDED_W_LDS_STORE + eic]; // srcW[eic];
                            #endif
                        }
                    }
                }

                #if !defined(NON_UNIFORM_CONVGROUP_PER_OC)
                // ...we dont cache input data if they can come from different groups per output channels
                IF_THREAD_DOING_X_LDS_STORE(ti, gtid)
                {
                    [unroll]
                    for (uint eic = 0; eic < NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE; eic++)
                    {
                        [unroll]
                        for (uint esp = 0; esp < NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE; esp++)
                        {
                        #if defined(NB_THREADIDS_FROM_NUMTHREADS_XY)
                            //#if NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE == 4 && NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE == 2 \
                            //    && NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE == NUMTHREADS_Y && NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE == NUMTHREADS_X
                            //#error all ok
                            //#endif

                            uint index = (eic * NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE)
                                        + SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid) * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE /* groupThreadID.y * ... */
                                        + esp * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE
                                        + SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid); /* groupThreadID.x */

                        #else
                            //#if NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE == 1
                            //#error okok
                            //#endif
                            #if NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE == 1
                            uint index = (eic * NB_THREADIDS_FOR_INCHAN_AND_SPATIALS_X_LDS_STORE) + ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE(ti, gtid);
                            #else

                            //#if NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE * NB_THREADIDS_FOR_INCHAN_AND_SPATIALS_X_LDS_STORE * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE == CACHE_DEPTH * NB_SPATIALS_PER_TG
                            //#error ok ok
                            //#endif

                            uint index = eic * NB_THREADIDS_FOR_INCHAN_AND_SPATIALS_X_LDS_STORE * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE
                                        // NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE * NB_THREADIDS_FOR_INCHAN_AND_SPATIALS_X_LDS_STORE * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE == CACHE_DEPTH * NB_SPATIALS_PER_TG
                                        + esp * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE
                                        + ADD_IDX_FROM_THREADID_FOR_X_LDS_STORE(ti, gtid);
                            #endif
                        #endif


                            #ifdef USE_ALTERNATE_LDS_ORG_X
                            LDS_ACCESS_X_ITE(SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid) + eic * NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE, SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid), esp)
                                = srcX[esp * NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE + eic];
                            #else
                            LDS_ACCESS_X(index) = srcX[esp * NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE + eic]; // srcX[eic];
                            #endif
                        }
                    }
                }
                #endif // #if !defined(NON_UNIFORM_CONVGROUP_PER_OC)

            }// store to LDS


            GroupMemoryBarrierWithGroupSync();


            // -------------------------------------------------------------------------------------------------------------------
            // Computing (if necessary) those arrays:
            //
            // inputChannelOffsetFromGroupByOC_rel, topD,Y,X,Padded_rel and possibly kernelOffsetX_rel, maskX_rel
            //
            // If we can't reload inputs (from tensor X) from LDS cache (because they are not in it, ie NON_UNIFORM_CONVGROUP_PER_OC),
            // we will get them from X directly. In that case, we might have to recompute certain values because the thread ids
            // semantics might have been repurposed between the LDS store and the reload / broadcast here,
            // so even if we could have calculated them earlier even in the NON_UNIFORM_CONVGROUP_PER_OC case, we might not have
            // been able to do so because of that repurpose:

            #if defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && (NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE != ELEM_PER_THREAD_Y)
            // We need to recompute some of our eoc-accessed (explicit output channels) reg arrays: inputChannelOffsetFromGroupByOC
            //
            // Note we can't just do the extra needed elements (eg if NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE < ELEM_PER_THREAD_Y,
            // just do the (ELEM_PER_THREAD_Y - NB_EXPLICIT_OUTCHAN_VAL_NEEDED_W_LDS_STORE) extra elements.
            // The reason is twofold:
            // 1) that not all the same threadids hold those "first already done elements in existing inputChannelOffsetFromGroupByOC[] register array.
            // 2) the threadnumber stride from each explicit reg to the other (as coalesced groups of thread all have their reg[0], reg[1] etc.) is not the same.

            // In that case threadids used for output channels in the LDS store context are not necessarily the same
            // as those used in the final output phase, on and after reloading/broadcast from LDS:
            float inputChannelOffsetFromGroupByOC_rel[ELEM_PER_THREAD_Y];
            {
                uint outputChannelNum = by + gtid.y;// + eoc * NUMTHREADS_Y;
                [unroll]
                for (eoc = 0; eoc < ELEM_PER_THREAD_Y; eoc++)
                {
                    uint convGroupId = outputChannelNum / O_channelsPerGroup;
                    inputChannelOffsetFromGroupByOC_rel[eoc] = convGroupId * X_channelsPerGroup;
                    outputChannelNum += NUMTHREADS_Y; // cf with in non-_rel (reload) arrays: NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE;
                }
            }
            #else
            #define inputChannelOffsetFromGroupByOC_rel inputChannelOffsetFromGroupByOC
            #endif // #if defined(CONV_TRANSPOSE) && defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY)


            #if defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && (NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE != ELEM_PER_THREAD_X)
            //
            // Also, same thing for esp-accessed ones (see eoc-accessed just above): top?Padded[] arrays and possibly maskX[], kernelOffsetX[].
            //
                uint topDPadded_rel[ELEM_PER_THREAD_X];
                uint topYPadded_rel[ELEM_PER_THREAD_X];
                uint topXPadded_rel[ELEM_PER_THREAD_X];
            #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                uint kernelOffsetX_rel[ELEM_PER_THREAD_X];
                bool maskX_rel[ELEM_PER_THREAD_X];
            #endif
            {
                uint centroidId = bx + gtid.x; //+ esp * NUMTHREADS_X;
                #if !defined(CONV_TRANSPOSE)
                uint4 strideParam = _StrideParam;
                #else
                uint4 strideParam = uint4(1,1,1,1);
                #endif

                [unroll]
                for (esp = 0; esp < ELEM_PER_THREAD_X; esp++)
                {
                    #if defined(CONV3D)
                    topDPadded_rel[esp] = (((centroidId / w) / h)) * strideParam.x - _Pad.x;
                    topYPadded_rel[esp] = ((centroidId / w) % h) * strideParam.y - _Pad.y;
                    topXPadded_rel[esp] = (centroidId % w) * strideParam.z - _Pad.z;

                    #elif defined(CONV2D)
                    topYPadded_rel[esp] = ((centroidId / w)) * strideParam.x - _Pad.x;
                    topXPadded_rel[esp] = (centroidId % w) * strideParam.y - _Pad.y;

                    #elif defined(CONV1D)
                    topXPadded_rel[esp] = centroidId * strideParam.x - _Pad.x;
                    #endif

                    centroidId += NUMTHREADS_X;

                #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)

                    #if defined(CONV_TRANSPOSE)
                    uint4 strideParamIfConvTrans = _StrideParam;
                    #else
                    uint4 strideParamIfConvTrans = uint4(1,1,1,1);
                    #endif

                    // Note: because of the define above, if we're not using conv transpose,
                    // all / strideParamIfConvTrans subexpressions and all mask modulo ( % ) tests
                    // will all be optimized out by the compiler.
                    // This is just to avoid even more #ifdef blocks below.

                    // We do the same for K1x1 vs dilations:
                    #if defined(K1x1)
                    uint4 dilationParam = uint4(0,0,0,0);
                    uint dd = 0;
                    uint dy = 0;
                    uint dx = 0;
                    #else
                    uint4 dilationParam = _Dilation;
                    #endif

                    #ifdef CONV3D
                    kernelOffsetX_rel[esp] = (topDPadded_rel[esp] + dilationParam.x * dd)  / strideParamIfConvTrans[0] * heightX * widthX + (topYPadded_rel[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1] * widthX
                        + (topXPadded_rel[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2];
                    maskX_rel[esp] = ((topDPadded_rel[esp] + dilationParam.x * dd) / strideParamIfConvTrans[0]) < depthX &&  ((topDPadded_rel[esp] + dilationParam.x * dd) % strideParamIfConvTrans[0] == 0) &&
                                 ((topYPadded_rel[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1]) < heightX && ((topYPadded_rel[esp] + dilationParam.y * dy) % strideParamIfConvTrans[1] == 0) &&
                                 ((topXPadded_rel[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2]) < widthX &&  ((topXPadded_rel[esp] + dilationParam.z * dx) % strideParamIfConvTrans[2] == 0);
                    #elif defined(CONV2D)
                    kernelOffsetX_rel[esp] = (topYPadded_rel[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0] * widthX + (topXPadded_rel[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1];
                    maskX_rel[esp] = ((topYPadded_rel[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0]) < heightX && ((topYPadded_rel[esp] + dilationParam.x * dy) % strideParamIfConvTrans[0] == 0) &&
                                 ((topXPadded_rel[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1]) < widthX &&  ((topXPadded_rel[esp] + dilationParam.y * dx) % strideParamIfConvTrans[1] == 0);
                    #elif defined(CONV1D)
                    kernelOffsetX_rel[esp] = (topXPadded_rel[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0];
                    maskX_rel[esp] = ((topXPadded_rel[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0]) < widthX && ((topXPadded_rel[esp] + dilationParam.x * dx) % strideParamIfConvTrans[0] == 0);
                    #endif
                #endif // #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                }
            }
            #else
                #define topDPadded_rel topDPadded
                #define topYPadded_rel topYPadded
                #define topXPadded_rel topXPadded
            #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                #define kernelOffsetX_rel kernelOffsetX
                #define maskX_rel maskX
            #endif
            #endif //#if defined(NON_UNIFORM_CONVGROUP_PER_OC) && !defined(NB_THREADIDS_FROM_NUMTHREADS_XY) && (NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE < ELEM_PER_THREAD_X)

            //
            // ... end of computing (if necessary) those arrays:
            //
            // inputChannelOffsetFromGroupByOC_rel, topD,Y,X,Padded_rel and possibly kernelOffsetX_rel, maskX_rel
            // -------------------------------------------------------------------------------------------------------------------

            // Inner product:
            for (uint di = 0; di < CACHE_DEPTH; di++)
            {
                // Reload / broadcast on threads and accumulate:
                float srcW[ELEM_PER_THREAD_Y];
                float srcX[ELEM_PER_THREAD_X];
                // TODOTODO: if NON_UNIFORM_CONVGROUP_PER_OC check if caching to reg first using srcX[ELEM_PER_THREAD_X * ELEM_PER_THREAD_Y]
                // is faster.

                // Reload weights:
                [unroll]
                for (eoc = 0; eoc < ELEM_PER_THREAD_Y; eoc++)
                {
                #if defined(NB_THREADIDS_FROM_NUMTHREADS_XY)
                    uint index = eoc * CACHE_DEPTH * NUMTHREADS_Y + ty * CACHE_DEPTH + di;
                    // Note: we dont use the eg SELECT_FROMTHREADIDS_AS_OUTCHAN_FOR_W_LDS_STORE(ti, gtid)
                    // macros here instead of ty (groupThreadID.y) and NB_THREADIDS_FOR_OUTCHAN_W_LDS_STORE instead of NUMTHREADS_Y etc.
                    // because whatever way we arrange the LDS cache, ty (groupThreadID.y) MUST select an output channel index base
                    // when threads load for a final time here before doing calculations and final output.
                    // ie it is required by the dispatch convention of having output channel dispatch (and ceil(oc nb / ELEM_PER_THREAD_Y)) flattened
                    // in gridY and similarly for all output spatial dimensions in gridX.

                    #ifdef USE_ALTERNATE_LDS_ORG_W
                    srcW[eoc] = LDS_ACCESS_W_ITE(di, groupThreadID.y, eoc);
                    #else
                    srcW[eoc] = LDS_ACCESS_W(index);
                    #endif
                #else
                    srcW[eoc] = LDS_ACCESS_W( ((di << LOG2_NB_OUTCHAN_PER_TG) | (eoc * NUMTHREADS_Y | ty)) );
                #endif
                }

                // Reload inputs:
                #if !defined(NON_UNIFORM_CONVGROUP_PER_OC)
                [unroll]
                for (esp = 0; esp < ELEM_PER_THREAD_X; esp++)
                {
                    //#if defined(NB_THREADIDS_FROM_NUMTHREADS_XY)
                    //uint index = (eic * NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE)
                    //            + SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid) * NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE /* groupThreadID.y * ... */
                    //            + esp * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE
                    //            + SELECT_FROMTHREADIDS_AS_SPATIALS_FOR_X_LDS_STORE(ti, gtid); /* groupThreadID.x */
                    // (NB_EXPLICIT_INCHAN_VAL_NEEDED_X_LDS_STORE * NB_THREADIDS_LEFT_FOR_INCHAN_X_LDS_STORE) == CACHE_DEPTH
                    // (NB_EXPLICIT_SPATIALS_VAL_NEEDED_X_LDS_STORE * NB_THREADIDS_FOR_SPATIALS_X_LDS_STORE) == (ELEM_PER_THREAD_X * NUMTHREADS_X)
                    //
                    // same either way:

                    #ifdef USE_ALTERNATE_LDS_ORG_X
                    srcX[esp] = LDS_ACCESS_X_ITE(di, groupThreadID.x, esp);
                    #else
                    srcX[esp] = LDS_ACCESS_X( ((di << LOG2_NB_SPATIALS_PER_TG) | (esp * NUMTHREADS_X | tx)) );
                    #endif
                }
                #endif // #if !defined(NON_UNIFORM_CONVGROUP_PER_OC)




                //
                // Accumulate:
                //
                #if !defined(NON_UNIFORM_CONVGROUP_PER_OC)
                [unroll]
                for (eoc = 0; eoc < ELEM_PER_THREAD_Y; eoc++)
                    [unroll]
                    for (esp = 0; esp < ELEM_PER_THREAD_X; esp++)
                    {
                        dstA[eoc * OC_AREG_STRIDE + esp] += srcX[esp] * srcW[eoc];
                    }
                #else
                //
                // we have NON_UNIFORM_CONVGROUP_PER_OC, load the input data on the fly in the accumulation loops
                //
                // TODOTODO: if NON_UNIFORM_CONVGROUP_PER_OC check if caching to reg first using srcX[ELEM_PER_THREAD_X * ELEM_PER_THREAD_Y]
                // before the accumulate loop is faster.
                uint baseInnerElement = di + i;// cf vs SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid) + i; : we are NOT storing to LDS here!
                [unroll]
                for (eoc = 0; eoc < ELEM_PER_THREAD_Y; eoc++)
                    [unroll]
                    for (esp = 0; esp < ELEM_PER_THREAD_X; esp++)
                    {
                        float inputValFromXptr = 0.0f;

                        #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
                        {
                            //uint readX = batchReadOffset + strideX * SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid);
                            //uint baseIdx = readX + strideX * i + kernelOffsetX[esp]; // now kernelOffsetX includes cornerId !
                            uint baseIdx = batchReadOffset + strideX * (inputChannelOffsetFromGroupByOC_rel[eoc] + baseInnerElement) + kernelOffsetX_rel[esp];
                            bool innerElemInBound = baseInnerElement < numInnerChannels;

                            if (maskX_rel[esp] && innerElemInBound)
                                inputValFromXptr = Xptr[MIN_PATCHBUG(baseIdx, maxXIndex)];
                        }
                        #else
                        // CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE case:
                        {
                            // inner index to match kernel CACHE_DEPTH-sized chunk
                            //uint ii = SELECT_FROMTHREADIDS_AS_INCHAN_FOR_X_LDS_STORE(ti, gtid) + (i + eic * innerThreadIdsStride);
                            uint ii = baseInnerElement;
                            bool inBound = ii < kInnerSliceSize; // that way no need to check ic < channels (or < numInnerChannels actually) below


                            // Note: K1x1 cannot be defined here at the same time that CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE is,
                            // as in that K1x1 case, we can only be caching for different input channels indices,
                            // the spatial dimensions sizes of the kernel all multiply to 1, ie there's only one spatial coord in the kernel.
                            // #if defined(K1x1)
                            // #error "K1x1 shouldn't touch the path where CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE"
                            // #endif
                            // Update: SHADER_API_WEBGPU

                            #if defined(CONV_TRANSPOSE)
                            uint4 strideParamIfConvTrans = _StrideParam;
                            #else
                            uint4 strideParamIfConvTrans = uint4(1,1,1,1);
                            #endif
                            // Note: because of the define above, if we're not using conv transpose,
                            // all / strideParamIfConvTrans subexpressions and all mask modulo ( % ) tests
                            // will all be optimized out by the compiler.
                            // This is just to avoid even more #ifdef blocks below.

                            #if defined(K1x1)
                            uint4 dilationParam = uint4(0,0,0,0);
                            #else
                            uint4 dilationParam = _Dilation;
                            #endif

                            #if defined(CONV3D)
                            uint dd = ((ii / K_width) / K_height) % K_depth;
                            uint ic = ((ii / K_width) / K_height) / K_depth;
                            uint dy = (ii / K_width) % K_height;
                            uint dx = ii % K_width;
                            #elif defined(CONV2D)
                            uint dy = (ii / K_width) % K_height;
                            uint ic = (ii / K_width) / K_height;
                            uint dx = ii % K_width;
                            #elif defined(CONV1D)
                            uint dx = ii % K_width;
                            uint ic = ii / K_width;
                            #endif

                            ic = ic + inputChannelOffsetFromGroupByOC_rel[eoc];

                            #ifdef CONV3D
                            uint kernelOffsetX = (topDPadded_rel[esp] + dilationParam.x * dd)  / strideParamIfConvTrans[0] * heightX * widthX + (topYPadded_rel[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1] * widthX
                                + (topXPadded_rel[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2];
                            bool maskX = ((topDPadded_rel[esp] + dilationParam.x * dd) / strideParamIfConvTrans[0]) < depthX &&  ((topDPadded_rel[esp] + dilationParam.x * dd) % strideParamIfConvTrans[0] == 0) &&
                                         ((topYPadded_rel[esp] + dilationParam.y * dy) / strideParamIfConvTrans[1]) < heightX && ((topYPadded_rel[esp] + dilationParam.y * dy) % strideParamIfConvTrans[1] == 0) &&
                                         ((topXPadded_rel[esp] + dilationParam.z * dx) / strideParamIfConvTrans[2]) < widthX &&  ((topXPadded_rel[esp] + dilationParam.z * dx) % strideParamIfConvTrans[2] == 0);
                            #elif defined(CONV2D)
                            uint kernelOffsetX = (topYPadded_rel[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0] * widthX + (topXPadded_rel[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1];
                            bool maskX = ((topYPadded_rel[esp] + dilationParam.x * dy) / strideParamIfConvTrans[0]) < heightX && ((topYPadded_rel[esp] + dilationParam.x * dy) % strideParamIfConvTrans[0] == 0) &&
                                         ((topXPadded_rel[esp] + dilationParam.y * dx) / strideParamIfConvTrans[1]) < widthX &&  ((topXPadded_rel[esp] + dilationParam.y * dx) % strideParamIfConvTrans[1] == 0);
                            #elif defined(CONV1D)
                            uint kernelOffsetX = (topXPadded_rel[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0];
                            bool maskX = ((topXPadded_rel[esp] + dilationParam.x * dx) / strideParamIfConvTrans[0]) < widthX && ((topXPadded_rel[esp] + dilationParam.x * dx) % strideParamIfConvTrans[0] == 0);
                            #endif

                            if (maskX && inBound)
                                inputValFromXptr = Xptr[MIN_PATCHBUG(batchReadOffset + strideX * ic + kernelOffsetX, maxXIndex)];
                        }
                        #endif

                        // Finally, accumulate:
                        dstA[eoc * OC_AREG_STRIDE + esp] += inputValFromXptr * srcW[eoc];
                        //debug
                        //dstA[eoc * OC_AREG_STRIDE + esp] += di == 1 ? srcW[eoc] : 0;
                        //debug
                        //dstA[eoc * OC_AREG_STRIDE + esp] += srcW[eoc];
                    }
                #endif // #if !defined(NON_UNIFORM_CONVGROUP_PER_OC)
            }

            //if (i + CACHE_DEPTH < numInnerChannels)
            //GroupMemoryBarrierWithGroupSync();
        } // reload/broadcast to threads weights/inputs from LDS and MAD/accumulate

        #if !defined(CACHE_MIXED_KERNEL_OUTPUTCHANNEL_SLICE)
        #if defined(CONV_TRANSPOSE)
        weightOffsetK--;
        #else
        weightOffsetK++;
        #endif
        #endif
    }


#if defined(USEBIAS) && defined(BIAS_AFTER)
    float biases[ELEM_PER_THREAD_Y];
    [unroll]
    for (eoc = 0; eoc < ELEM_PER_THREAD_Y; eoc++)
        biases[eoc] = Bptr[MIN_WHEN_NON_D3D(yy + eoc * NUMTHREADS_Y, maxBIndex)];

#define OUTPUT_EXPLICIT_XY_ITEM(tidx, tidy, eix, eiy)                                                                                                                     \
    if (((tidx + eix * NUMTHREADS_X) < strideO) && ((tidy + eiy * NUMTHREADS_Y) < features))                                                                              \
        Optr[(tidx + eix * NUMTHREADS_X) + (tidy + eiy * NUMTHREADS_Y)*strideO + batchWriteOffset] = ApplyFusedActivation(dstA[eiy * OC_AREG_STRIDE + eix] + biases[eiy]);\

#else // #if defined(USEBIAS) && defined(BIAS_AFTER)

#define OUTPUT_EXPLICIT_XY_ITEM(tidx, tidy, eix, eiy)                                                                                                       \
    if (((tidx + eix * NUMTHREADS_X) < strideO) && ((tidy + eiy * NUMTHREADS_Y) < features))                                                                \
        Optr[(tidx + eix * NUMTHREADS_X) + (tidy + eiy * NUMTHREADS_Y)*strideO + batchWriteOffset] = ApplyFusedActivation(dstA[eiy * OC_AREG_STRIDE + eix]);\

#endif // #if defined(USEBIAS) && defined(BIAS_AFTER)




//debug
#if 1
    // The unroll is what allows the full static analysis and collapse of all the branches in the
    // macro while at the same time simplifying the output code for all possible permutations
    // of data arrangements.
    [unroll]
    for (eoc = 0; eoc < ELEM_PER_THREAD_Y; eoc++)
        [unroll]
        for (esp = 0; esp < ELEM_PER_THREAD_X; esp++)
            OUTPUT_EXPLICIT_XY_ITEM(xx, yy, esp, eoc)

#else

    if (gtid.y > 0)
        return;

    if (gtid.x > 5)
        return;

    [unroll]
    for (eoc = 0; eoc < 1; eoc++) // ELEM_PER_THREAD_Y
        [unroll]
        for (esp = 0; esp < 1; esp++) // ELEM_PER_THREAD_X
            OUTPUT_EXPLICIT_XY_ITEM(xx, yy, esp, eoc)


#endif


}